{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962f2478-14fe-4698-ad93-a17adaecd624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import vllm\n",
    "\n",
    "import string\n",
    "import re\n",
    "import collections\n",
    "from sklearn.metrics import f1_score\n",
    "# import Levenshtein\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import datetime\n",
    "from functools import reduce\n",
    "# from huggingface_hub import login\n",
    "# from dataclasses import dataclass\n",
    "import pickle\n",
    "import sys\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af45a84a-c715-4bff-bf8a-14b82bf3c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    with open(path, 'r') as f:\n",
    "        a = f.readlines()\n",
    "        g = [json.loads(i) for i in a]\n",
    "    return g\n",
    "eval_output_path = \"./downstream_compare_outputs\"\n",
    "valfile = load_jsonl(\"./downstream_compare_outputs/val_downstream.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d8c25f-7ab8-4cf5-9e17-79a4a40178a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10256"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c2979bc-3253-4a62-b54a-73afb6a0529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# outputs = {}\n",
    "\n",
    "# for fname in tqdm(os.listdir(eval_output_path)):\n",
    "#     fpath = os.path.join(eval_output_path, fname)\n",
    "#     model_name = fname.split('_downstream_eval.pkl')[0]\n",
    "#     with open(fpath, 'rb') as f:\n",
    "#         outputs[model_name] = pickle.load(f)\n",
    "#     print(f\"{model_name:<20}\", \"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "874e848e-b649-4c5a-a3e0-2f6baf61c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = json.load(open('./downstream_compare_outputs/_downstream_eval.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc46d65e-a3c7-4fa3-a571-70ae6576c446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ner', 'pc', 'sf', 'ee', 're', 'sar', 'sc', 'qna', 'mcq'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['llamat3-chat'].keys()#['pc'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "768ea722",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'ner' # ['ner', 'pc', 'sf', 'ee', 're', 'sar', 'sc', 'qna', 'mcq']\n",
    "dataset_list = list(outputs['llamat2-chat'][task].keys())\n",
    "dataset_idx = 0\n",
    "idxx = 0\n",
    "idx = outputs['llamat2-chat'][task][dataset_list[dataset_idx]][idxx]['sample_idx']\n",
    "\n",
    "#['glass_non_glass'])\n",
    "# idx = outputs['llamat2-chat']['pc']['glass_non_glass'][0]['sample_idx']\n",
    "# valfile[idx]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "453b3d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f221855f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'matscholar'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "159e594a-93fd-4c4e-8caa-d40b919ce326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'You are a linguist and a material scientist. You need to identify the named entity for each of the keywords given after WORDS in the input. Answer to the question should be from one of the provided options. Do not output anything else other than the answer. You should output the word entity pair separated by \":\" in each line. Your options are: b-material, i-material, b-device, i-device, b-experiment, i-experiment, b-value, i-value. Answer for each word must be in a new line.',\n",
       " 'question': 'WORDS: demonstrate, BCFZY-ZnO, SOFC, 400, –, 500, °, C, .\\nSENTENCE: However , our findings reveal it is feasible to demonstrate BCFZY-ZnO electrolyte in SOFC with excellent ionic conductivity , high OCVs and power densities at low temperature range of 400 – 500 ° C .',\n",
       " 'answer': 'demonstrate : b-experiment\\nBCFZY-ZnO : b-material\\nSOFC : b-device\\n400 : b-value\\n– : i-value\\n500 : i-value\\n° : i-value\\nC : i-value\\n. : i-value',\n",
       " 'task': 'downstream',\n",
       " 'dataset': 'sofc_token'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keylist = [list(val.keys()) for val in valfile]\n",
    "datasetlist = [val['dataset'] for val in valfile]\n",
    "tasklist = [val['task'] for val in valfile]\n",
    "valfile[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe8242-f210-499b-a81d-12f03daf8cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2438384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335bfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hybrid fiber-reinforced thermoset plastic matrix composites are increasingly being used in automotive applications, owing to their high specific strength and modulus. In this study, hybrid composites were manufactured by means of hybrid reinforcement; that is, biaxial glass and woven flax fabrics in vinyl ester resin with various stackings were created via the vacuum assisted resin transferee molding (VARTM) process. The manufactured composites were tested for their mechanical properties (tensile strength, modulus, flexural strength, and impact strength) and verified by means of morphological analysis. The results demonstrated rather unexpected phenomena, such as the tensile strength of the pure woven glass fibers (85.16â\\x80¯MPa) being lower than that of the hybrid composites (143.21â\\x80¯MPa). The flexural tests indicated that the strength (305.46â\\x80¯MPa) is higher for composites with glass ply at the bottom surface, and composites with glass ply on both ends exhibit high impact strength (0.145â\\x80¯J/mm2) compared to others. A water absorption test and thermo-gravimetric analysis were also performed.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5596841",
   "metadata": {},
   "outputs": [],
   "source": [
    "glass_non_glass_data_with_questions = [x for x in valfile if x['dataset'] == 'glass_non_glass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c47453d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'You are a linguist and a material scientist. You need to classify if the paragraph below is related to inorganic glass research. Answer to the question should be from one of the provided options. Do not output anything else other than the answer. Your choices are: yes/no.',\n",
       " 'question': 'A novel erbium ion doped zinc fluoride glass was prepared. 2.7Î¼m emission and transmittance properties together with thermal ability were investigated. An enhanced 2.7Î¼m emission was observed by introducing ZnF2 in the ZrF4-based fluoride glass. Meanwhile, the J-O parameters and branching ratios (Î²) of Er3+-doped zinc fluoride glass were calculated and analyzed. The present Er3+-doped zinc fluoride glass with large emission cross-section (0.92Ã\\x9710â\\x88\\x9220 cm2) and long decay lifetime (2.05ms) at 2.7Î¼m indicates that it have very promising applications for solid state lasers around 3Î¼m.',\n",
       " 'answer': 'yes',\n",
       " 'task': 'downstream',\n",
       " 'dataset': 'glass_non_glass'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glass_non_glass_data_with_questions[63]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9e26dd4-b5f6-4012-9a6b-a1f045df7d3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sample_idx': 63,\n",
       "  'gold_answer': 'no',\n",
       "  'prediction': 'no',\n",
       "  'processed_gold': 'no',\n",
       "  'processed_prediction': 'no',\n",
       "  'system_prompt': 'You are a linguist and a material scientist. You need to classify if the paragraph below is related to inorganic glass research. Answer to the question should be from one of the provided options. Do not output anything else other than the answer. Your choices are: yes/no.'},\n",
       " {'sample_idx': 95,\n",
       "  'gold_answer': 'no',\n",
       "  'prediction': 'no',\n",
       "  'processed_gold': 'no',\n",
       "  'processed_prediction': 'no',\n",
       "  'system_prompt': 'You are a linguist and a material scientist. You need to classify if the paragraph below is related to inorganic glass research. Answer to the question should be from one of the provided options. Do not output anything else other than the answer. Your choices are: yes/no.'},\n",
       " {'sample_idx': 98,\n",
       "  'gold_answer': 'no',\n",
       "  'prediction': 'no',\n",
       "  'processed_gold': 'no',\n",
       "  'processed_prediction': 'no',\n",
       "  'system_prompt': 'You are a linguist and a material scientist. You need to classify if the paragraph below is related to inorganic glass research. Answer to the question should be from one of the provided options. Do not output anything else other than the answer. Your choices are: yes/no.'},\n",
       " {'sample_idx': 159,\n",
       "  'gold_answer': 'no',\n",
       "  'prediction': 'no',\n",
       "  'processed_gold': 'no',\n",
       "  'processed_prediction': 'no',\n",
       "  'system_prompt': 'You are a linguist and a material scientist. You need to classify if the paragraph below is related to inorganic glass research. Answer to the question should be from one of the provided options. Do not output anything else other than the answer. Your choices are: yes/no.'},\n",
       " {'sample_idx': 164,\n",
       "  'gold_answer': 'no',\n",
       "  'prediction': 'no',\n",
       "  'processed_gold': 'no',\n",
       "  'processed_prediction': 'no',\n",
       "  'system_prompt': 'You are a linguist and a material scientist. You need to classify if the paragraph below is related to inorganic glass research. Answer to the question should be from one of the provided options. Do not output anything else other than the answer. Your choices are: yes/no.'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['llamat3-chat']['pc']['glass_non_glass'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0319ee9f-8932-4c9f-bd72-8b67fc6e43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "our = 'llamat3-chat'\n",
    "def get_correct_indices(analysis_dict, task, dataset):\n",
    "    correct = []\n",
    "    for entry in analysis_dict[task][dataset]:\n",
    "        if entry[\"processed_gold\"] == entry[\"processed_prediction\"]:\n",
    "            correct.append(entry[\"sample_idx\"])\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57a74eee-d13c-463a-802a-aa9a17c37c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_llamas = ['LLaMA-3-chat', 'LLaMA-2',  'LLaMA-2-chat', 'LLaMA-3']\n",
    "all_others = ['Claude-3-Opus', 'GPT-4o', 'Gemini-1.5-Flash-8b', 'Claude-3-Haiku', 'LLaMA-3-chat', 'Gemini-1.5-flash', 'GPT-4',  'LLaMA-2',  'Gemini-1.5-pro', 'LLaMA-2-chat', 'Claude-3.5-Sonnet', 'LLaMA-3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70073737-6bcc-4291-90ab-8b7e67bd4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_across_all(analysis_dicts, our_name, other_name):\n",
    "    results = {}\n",
    "    for task in analysis_dicts[our_name].keys():\n",
    "        results[task] = {}\n",
    "        for dataset in analysis_dicts[our_name][task].keys():\n",
    "            ours_better = compare_models(\n",
    "                analysis_dicts[our_name], \n",
    "                analysis_dicts[other_name], \n",
    "                task, dataset\n",
    "            )\n",
    "            results[task][dataset] = ours_better\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4de1869-12b9-4e42-8bf4-7a62502b0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_wins(analysis_dicts, our_name, other_names, min_wrong=None):\n",
    "    results = []\n",
    "\n",
    "    for task in analysis_dicts[our_name]:\n",
    "        for dataset in analysis_dicts[our_name][task]:\n",
    "            for entry in analysis_dicts[our_name][task][dataset]:\n",
    "                idx = entry[\"sample_idx\"]\n",
    "                gold = entry[\"processed_gold\"]\n",
    "                ours_pred = entry[\"processed_prediction\"]\n",
    "\n",
    "                # skip if our model is wrong\n",
    "                if ours_pred != gold:\n",
    "                    continue\n",
    "\n",
    "                wrong_count = 0\n",
    "                compared = 0\n",
    "                for other in other_names:\n",
    "                    # check if this other model has that idx\n",
    "                    other_entries = [\n",
    "                        e for e in analysis_dicts[other][task][dataset]\n",
    "                        if e[\"sample_idx\"] == idx\n",
    "                    ]\n",
    "                    if not other_entries:  # skip if missing\n",
    "                        continue\n",
    "\n",
    "                    compared += 1\n",
    "                    other_entry = other_entries[0]\n",
    "                    if other_entry[\"processed_prediction\"] != gold:\n",
    "                        wrong_count += 1\n",
    "\n",
    "                if compared == 0:\n",
    "                    continue  # nobody to compare against\n",
    "\n",
    "                if min_wrong is None:\n",
    "                    if wrong_count == compared:  # all wrong\n",
    "                        results.append((task, dataset, idx))\n",
    "                else:\n",
    "                    if wrong_count >= min_wrong:\n",
    "                        results.append((task, dataset, idx))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23e3b25f-fc3d-4758-b0cf-deea516dbe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = find_unique_wins(outputs, 'llamat3-chat', all_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e465cd4-4d74-46ec-831f-5909491e460e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1733"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85fb1a07-c168-4bc7-9e31-a72360b727e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_wins_with_stats(analysis_dicts, our_name, other_names, min_wrong=None, return_full=False):\n",
    "    \"\"\"\n",
    "    Returns cases where our model is correct and others are wrong,\n",
    "    and also generates a dict structured like original analysis_dict,\n",
    "    along with statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    analysis_dicts : dict\n",
    "        Dictionary like your `analysis_dict` containing all models' outputs.\n",
    "    our_name : str\n",
    "        The key of our model in the dictionary.\n",
    "    other_names : list of str\n",
    "        Keys of other models to compare against.\n",
    "    min_wrong : int or None\n",
    "        Minimum number of other models that must be wrong. If None, all must be wrong.\n",
    "    return_full : bool\n",
    "        If True, returns full data including predictions and gold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    wins_list : list\n",
    "        List of unique wins (see previous function).\n",
    "    wins_dict : dict\n",
    "        Same structure as analysis_dict but only containing unique wins.\n",
    "    stats : dict\n",
    "        {task: {dataset: (unique_count, total_count, fraction)}} \n",
    "    \"\"\"\n",
    "    wins_list = []\n",
    "    wins_dict = defaultdict(lambda: defaultdict(list))\n",
    "    stats = defaultdict(lambda: defaultdict(lambda: (0, 0, 0.0)))\n",
    "\n",
    "    for task in analysis_dicts[our_name]:\n",
    "        for dataset in analysis_dicts[our_name][task]:\n",
    "            total = len(analysis_dicts[our_name][task][dataset])\n",
    "            unique_count = 0\n",
    "\n",
    "            for entry in analysis_dicts[our_name][task][dataset]:\n",
    "                idx = entry[\"sample_idx\"]\n",
    "                gold = entry[\"processed_gold\"]\n",
    "                ours_pred = entry[\"processed_prediction\"]\n",
    "\n",
    "                if ours_pred != gold:\n",
    "                    continue\n",
    "\n",
    "                wrong_count = 0\n",
    "                compared = 0\n",
    "                other_data = {}\n",
    "\n",
    "                for other in other_names:\n",
    "                    other_entries = [\n",
    "                        e for e in analysis_dicts[other][task][dataset]\n",
    "                        if e[\"sample_idx\"] == idx\n",
    "                    ]\n",
    "                    if not other_entries:\n",
    "                        continue\n",
    "\n",
    "                    compared += 1\n",
    "                    other_entry = other_entries[0]\n",
    "                    other_pred = other_entry[\"processed_prediction\"]\n",
    "\n",
    "                    other_data[other] = other_pred\n",
    "\n",
    "                    if other_pred != gold:\n",
    "                        wrong_count += 1\n",
    "\n",
    "                if compared == 0:\n",
    "                    continue  # nobody to compare against\n",
    "\n",
    "                condition = (\n",
    "                    (min_wrong is None and wrong_count == compared) or\n",
    "                    (min_wrong is not None and wrong_count >= min_wrong)\n",
    "                )\n",
    "\n",
    "                if condition:\n",
    "                    unique_count += 1\n",
    "                    if return_full:\n",
    "                        wins_list.append({\n",
    "                            \"task\": task,\n",
    "                            \"dataset\": dataset,\n",
    "                            \"sample_idx\": idx,\n",
    "                            \"gold\": gold,\n",
    "                            \"ours\": ours_pred,\n",
    "                            \"others\": other_data\n",
    "                        })\n",
    "                    else:\n",
    "                        wins_list.append((task, dataset, idx))\n",
    "\n",
    "                    # add to wins_dict in the same structure as analysis_dict\n",
    "                    wins_dict[task][dataset].append(entry)\n",
    "\n",
    "            fraction = unique_count / total if total > 0 else 0\n",
    "            stats[task][dataset] = (unique_count, total, fraction)\n",
    "\n",
    "    # Print stats nicely\n",
    "    print(\"=== Unique Wins Statistics ===\")\n",
    "    for task in stats:\n",
    "        print(f\"\\nTask: {task}\")\n",
    "        for dataset in stats[task]:\n",
    "            unique_count, total, fraction = stats[task][dataset]\n",
    "            print(f\"  Dataset: {dataset}, Unique Wins: {unique_count}/{total} ({fraction:.2%})\")\n",
    "\n",
    "    return wins_list, wins_dict, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3915ea4-6867-43c3-84aa-0eac111d73aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Unique Wins Statistics ===\n",
      "\n",
      "Task: ner\n",
      "  Dataset: matscholar, Unique Wins: 582/1070 (54.39%)\n",
      "  Dataset: sofc_token, Unique Wins: 25/175 (14.29%)\n",
      "  Dataset: sc_comics, Unique Wins: 400/945 (42.33%)\n",
      "\n",
      "Task: pc\n",
      "  Dataset: glass_non_glass, Unique Wins: 17/300 (5.67%)\n",
      "\n",
      "Task: sf\n",
      "  Dataset: sofc_token, Unique Wins: 41/175 (23.43%)\n",
      "\n",
      "Task: ee\n",
      "  Dataset: sc_comics, Unique Wins: 1/315 (0.32%)\n",
      "\n",
      "Task: re\n",
      "  Dataset: structured_re, Unique Wins: 0/1529 (0.00%)\n",
      "  Dataset: sc_comics, Unique Wins: 17/418 (4.07%)\n",
      "\n",
      "Task: sar\n",
      "  Dataset: synthesis_actions, Unique Wins: 123/566 (21.73%)\n",
      "\n",
      "Task: sc\n",
      "  Dataset: sofc_sent, Unique Wins: 437/1855 (23.56%)\n",
      "\n",
      "Task: qna\n",
      "  Dataset: squad, Unique Wins: 81/1081 (7.49%)\n",
      "\n",
      "Task: mcq\n",
      "  Dataset: story_cloze, Unique Wins: 0/324 (0.00%)\n",
      "  Dataset: hellaswag, Unique Wins: 7/976 (0.72%)\n",
      "  Dataset: boolqa, Unique Wins: 2/500 (0.40%)\n"
     ]
    }
   ],
   "source": [
    "results = find_unique_wins_with_stats(outputs, 'llamat3-chat', all_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066d1e9-96d6-460d-b6a6-cd65ea9fc0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
