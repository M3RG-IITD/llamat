{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6597d75-eca2-47c9-94ba-583775510255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af73821-571f-43bb-aeb2-fdfe62d3df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vllm\n",
    "import torch\n",
    "import string\n",
    "import re\n",
    "import collections\n",
    "from sklearn.metrics import f1_score\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b87f724b-d262-4e69-91b2-64fbc3b8779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                          macro-f1/em          micro-f1/f1         \n",
    "#                                 mean      std        mean      std\n",
    "# (ner, matscholar)           80.12533  0.04045    84.62633  0.23798\n",
    "# (ner, sofc_token)           78.55333  0.42465    89.54000  0.12165\n",
    "# (ner, sc_comics)            89.47433  0.45713    92.46833  0.19805\n",
    "# (pc, glass_non_glass)       90.29333  0.42272    91.77767  0.38509\n",
    "# (sf, sofc_token)            75.95667  0.87344    85.36800  0.81667\n",
    "# (ee, sc_comics)             90.79300  1.66148    93.44033  1.11710\n",
    "# (re, structured_re)        100.00000  0.00000   100.00000  0.00000\n",
    "# (re, sc_comics)             99.36867  0.10970    99.19300  0.14030\n",
    "# (sar, synthesis_actions)    95.95367  0.15938    96.83567  0.23199\n",
    "# (sc, sofc_sent)             76.74700  1.17814    93.37933  0.35192\n",
    "# (qna, squad)                42.15267  0.21362    54.40733  0.69172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a793b0b8-28ef-4f77-a0ad-4ec0b4f83179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/cse/btech/cs1200389/.conda/envs/matllama2.0/bin:/home/apps/anaconda3_2018/4.6.9/condabin:/home/soft/centOS/compilers/gcc/11.2/bin:/home/soft/cuda-11.0.2/bin:/home/soft/cuda-11.0.2/nvvm/bin:/usr/lib64/qt-3.3/bin:/usr/share/Modules/4.4.1/bin:/opt/pbs/default/bin:/opt/pbs/default/sbin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/pbs/2022.1.3/bin:/home/cse/btech/cs1200448/gurobi1003/linux64/bin:/home/cse/btech/cs1200448/bin: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0208bb4b-2382-4c72-ba62-34eadc03d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18762bbe-671e-498a-bf8f-a332605b2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--checkpoint', type=str, help=\"Path to the checkpoint to run inference on\")\n",
    "parser.add_argument('--valfile', type=str)\n",
    "parser.add_argument('--multi_seed', action='store_true', help=\"Whether to run inference on multiple seeds or not.\")\n",
    "parser.add_argument('--batch_size', type=int, default=16, help=\"Batch size for inference\")\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f5439e1-5580-42c1-8600-e7d30cc9d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.checkpoint = \"/scratch/cse/btech/cs1200448/MatLlama/ft-mthf/llama-original-squad\"\n",
    "args.valfile = '/scratch/cse/btech/cs1200448/MatLlama/ft_ds/val_ft.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "719a004f-d5b2-46eb-bf0c-cce32e046d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "valfile = []\n",
    "with open(args.valfile, 'r') as f:\n",
    "    valfile = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c440c6aa-3844-4d4d-9d68-b318e5c65a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(valfile)\n",
    "# with open(args.valfile, \"w\") as f:\n",
    "#     for document in valfile:\n",
    "#         f.write(json.dumps(document) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd3c12bb-53ff-4234-b944-bf93ccf7006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valinputs = [f\"<|im_start|>system\\n{i['system']}<|im_end|>\\n\"+f\"<|im_start|>question\\n{i['question']}<|im_end|>\\n\"+\"<|im_start|>answer\\n\" for i in valfile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ca580b7-d9da-4e5c-8f58-e1e4efa58c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a linguist and a material scientist. You need to mark the synthesis action for each of the keywords given after WORDS in the input. Answer to the question should be from one of the provided options. Do not output anything else other than the answer. You should output the word entity pair separated by \":\" in each line. Your options are: cooling, heating, mixing, non-altering, purification, reaction, shaping, starting. Answer for each word must be in a new line.<|im_end|>\n",
      "<|im_start|>question\n",
      "WORDS: calcined\n",
      "SENTENCE: The resulting solid was calcined at 500 ° C for 4 h in air with a heating rate of 10 ° C min-1 to obtain the final solid , which was denoted as CuCrOx .<|im_end|>\n",
      "<|im_start|>answer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(valinputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b17ae7c-3f97-4df4-9fd8-b2dc6a2b8fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-05 22:36:34 llm_engine.py:87] Initializing an LLM engine with config: model='/scratch/cse/btech/cs1200448/MatLlama/ft-mthf/llama-original-squad', tokenizer='/scratch/cse/btech/cs1200448/MatLlama/ft-mthf/llama-original-squad', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-05 22:37:36 llm_engine.py:357] # GPU blocks: 1961, # CPU blocks: 512\n",
      "INFO 04-05 22:37:38 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-05 22:37:38 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-05 22:37:47 model_runner.py:756] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "args.seed = 2\n",
    "\n",
    "kwargs = {\n",
    "    \"model\": args.checkpoint,\n",
    "    \"tokenizer\": args.checkpoint,\n",
    "    \"trust_remote_code\": True,\n",
    "    \"tensor_parallel_size\": 1,\n",
    "    \"seed\":args.seed,\n",
    "    \"gpu_memory_utilization\":0.9,\n",
    "}\n",
    "client = vllm.LLM(**kwargs, dtype='float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f49a13d9-69e6-440d-aeea-00ba8a8bcd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                                                                                                                                                                                              | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-05 22:38:56 scheduler.py:195] Input prompt (183001 tokens) is too long and exceeds limit of 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 268.56it/s]\n"
     ]
    }
   ],
   "source": [
    "response = client.generate(valinputs[0]*1000, sampling_params=vllm.SamplingParams(\n",
    "        best_of=1,\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=1.0,\n",
    "        top_k=50,\n",
    "        top_p=1.0,\n",
    "        temperature=0.75,\n",
    "        stop=[\"<|im_start|>\", \"<|im_end|>\"],\n",
    "        use_beam_search=False,\n",
    "        max_tokens=500,\n",
    "        logprobs=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "973c0950-e406-45b6-b882-8a151c17d8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CompletionOutput(index=0, text='', token_ids=[], cumulative_logprob=0.0, logprobs=[], finish_reason=length)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbff4ddc-a280-4804-b368-d19944291e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions += [\"\"] + [i.outputs[0].text for i in response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d745c09-d604-4c3d-b35b-84cade2f7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\"ner\",\"pc\",\"sf\",\"ee\",\"re\",\"sar\",\"sc\"]\n",
    "kw = {i : [] for i in tasks}\n",
    "kw[\"ner\"] = {i : [] for i in [\"matscholar\", \"sofc_token\", \"sc_comics\"]}\n",
    "kw[\"pc\"] = {i : [] for i in [\"glass_non_glass\"]}\n",
    "kw[\"sf\"] = {i : [] for i in [\"sofc_token\"]}\n",
    "kw[\"ee\"] = {i : [] for i in [\"sc_comics\"]}\n",
    "kw[\"re\"] = {i : [] for i in [\"structured_re\", \"sc_comics\"]}\n",
    "kw[\"sar\"] = {i : [] for i in [\"synthesis_actions\"]}\n",
    "kw[\"sc\"] = {i : [] for i in [\"sofc_sent\"]}\n",
    "\n",
    "kw['ner']['matscholar'] = ['b-mat','i-mat','b-pro','i-pro','b-dsc','i-dsc','b-spl','i-spl','b-apl','i-apl','b-smt','i-smt','b-cmt','i-cmt']\n",
    "kw['ner']['sofc_token'] = ['b-material', 'i-material', 'b-device', 'i-device', 'b-experiment', 'i-experiment', 'b-value', 'i-value']\n",
    "kw['ner']['sc_comics'] = ['material', 'doping', 'sc', 'value', 'process', 'characterization', 'element', 'property', 'main']\n",
    "kw['pc']['glass_non_glass'] = ['yes','no']\n",
    "kw['sf']['sofc_token'] = ['i-device', 'b-voltage', 'b-anode_material', 'b-cathode_material', 'b-time_of_operation', 'i-working_temperature', 'b-conductivity', 'i-fuel_used', 'i-interlayer_material', 'i-time_of_operation', 'i-anode_material', 'i-current_density', 'b-degradation_rate', 'i-resistance', 'i-conductivity', 'b-current_density', 'b-working_temperature', 'i-thickness', 'i-experiment_evoking_word', 'b-open_circuit_voltage', 'i-degradation_rate', 'b-electrolyte_material', 'i-open_circuit_voltage', 'i-electrolyte_material', 'b-fuel_used', 'b-power_density', 'i-power_density', 'b-interlayer_material', 'b-thickness', 'b-device', 'b-experiment_evoking_word', 'i-cathode_material', 'b-resistance', 'i-support_material', 'i-voltage', 'b-support_material']\n",
    "kw['ee']['sc_comics'] = ['site', 'dopant']\n",
    "kw['re']['structured_re'] = ['capacity','voltage','coulombic efficiency','conductivity','energy']\n",
    "kw['re']['sc_comics'] = ['target', 'condition', 'equivalent']\n",
    "kw['sar']['synthesis_actions'] = ['cooling', 'heating', 'mixing', 'non-altering', 'purification', 'reaction', 'shaping', 'starting']\n",
    "kw['sc']['sofc_sent'] = ['yes','no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74c07ffe-898a-42d5-8f70-cb889fd06d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_answer(a,answer_set):\n",
    "    a = a.strip().replace(' ', '')\n",
    "    if(a in answer_set):\n",
    "        return a\n",
    "    dis = [Levenshtein.distance(a,x) for x in answer_set]\n",
    "    idx = np.argmin(dis)\n",
    "    return answer_set[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12275a7a-8591-4bb8-9cc3-a80effd1aca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('exact', 0.7261794634597595),\n",
       "             ('f1', 0.8554270623306843),\n",
       "             ('total', 1081)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_similar_answer(a,answer_set):\n",
    "        a = a.strip().replace(' ', '')\n",
    "        if(a in answer_set):\n",
    "            return a\n",
    "        dis = [Levenshtein.distance(a,x) for x in answer_set]\n",
    "        idx = np.argmin(dis)\n",
    "        return answer_set[idx]\n",
    "\n",
    "out_dict = {i : [] for i in tasks}\n",
    "out_dict[\"ner\"] = {i : [] for i in [\"matscholar\", \"sofc_token\", \"sc_comics\"]}\n",
    "out_dict[\"pc\"] = {i : [] for i in [\"glass_non_glass\"]}\n",
    "out_dict[\"sf\"] = {i : [] for i in [\"sofc_token\"]}\n",
    "out_dict[\"ee\"] = {i : [] for i in [\"sc_comics\"]}\n",
    "out_dict[\"re\"] = {i : [] for i in [\"structured_re\", \"sc_comics\"]}\n",
    "out_dict[\"sar\"] = {i : [] for i in [\"synthesis_actions\"]}\n",
    "out_dict[\"sc\"] = {i : [] for i in [\"sofc_sent\"]}\n",
    "out_dict[\"qna\"] = {i : [] for i in [\"squad\"]}\n",
    "\n",
    "for _,sent in enumerate(valfile):\n",
    "    task = None\n",
    "    dataset = None\n",
    "    system = sent['system']\n",
    "    output = predictions[_]\n",
    "    answer = sent['answer']\n",
    "    if 'SOFC' in system:\n",
    "        task = 'sc'\n",
    "        dataset = 'sofc_sent'\n",
    "    elif 'named entity' in system:\n",
    "        task = 'ner'\n",
    "        if 'b-dsc' in system:\n",
    "            dataset = 'matscholar'\n",
    "        elif 'b-device' in system:\n",
    "            dataset = 'sofc_token'\n",
    "        elif 'doping' in system:\n",
    "            dataset = 'sc_comics'\n",
    "    elif 'synthesis action' in system:\n",
    "        task = 'sar'\n",
    "        dataset = 'synthesis_actions'\n",
    "    elif 'slots' in system:\n",
    "        task = 'sf'\n",
    "        dataset = 'sofc_token'\n",
    "    elif 'event, identify the roles of the arguments' in system:\n",
    "        task = 'ee'\n",
    "        dataset = 'sc_comics'\n",
    "    elif 'extract relation' in system:\n",
    "        task = 're'\n",
    "        for ds in kw[task]:\n",
    "            if kw[task][ds][0] in system:\n",
    "                dataset = ds\n",
    "                break   \n",
    "    elif 'inorganic glass' in system:\n",
    "        task = 'pc'\n",
    "        dataset = 'glass_non_glass'\n",
    "    else:\n",
    "        task = 'qna'\n",
    "        dataset = 'squad'\n",
    "    \n",
    "    assert task != None\n",
    "    assert dataset != None\n",
    "    \n",
    "    if task in [\"ner\", \"sf\", \"ee\", \"sar\"]:\n",
    "        answer = [i.split(\" : \") for i in answer.lower().split('\\n')]\n",
    "        try:\n",
    "            temp = [[j.strip().lower() for j in i.split(\":\")] for i in output.split('\\n')]\n",
    "            temp = [i for i in temp if len(i) == 2]\n",
    "            output = temp\n",
    "        except:\n",
    "            print(f\"for tc {_}, output pattern mismatched. {output}\")\n",
    "    else:\n",
    "        answer = answer.strip().lower()\n",
    "        output = output.strip().lower()\n",
    "\n",
    "    out_dict[task][dataset].append((answer, output))\n",
    "\n",
    "scores = {i : [] for i in tasks}\n",
    "scores[\"ner\"] = {i : (0,0) for i in [\"matscholar\", \"sofc_token\", \"sc_comics\"]}\n",
    "scores[\"pc\"] = {i : (0,0) for i in [\"glass_non_glass\"]}\n",
    "scores[\"sf\"] = {i : (0,0) for i in [\"sofc_token\"]}\n",
    "scores[\"ee\"] = {i : (0,0) for i in [\"sc_comics\"]}\n",
    "scores[\"re\"] = {i : (0,0) for i in [\"structured_re\", \"sc_comics\"]}\n",
    "scores[\"sar\"] = {i : (0,0) for i in [\"synthesis_actions\"]}\n",
    "scores[\"sc\"] = {i : (0,0) for i in [\"sofc_sent\"]}\n",
    "scores[\"qna\"] = {i : (0,0) for i in [\"squad\"]}\n",
    "\n",
    "def evaluate(task, dataset):\n",
    "    all_gt = []\n",
    "    all_pred = []\n",
    "    for gt, pred in out_dict[task][dataset]:\n",
    "        dict = {}\n",
    "        for word, entity in gt:\n",
    "            dict[word] = [entity, 'O']\n",
    "                \n",
    "        for word, entity in pred:\n",
    "            to_put = most_similar_answer(entity, kw[task][dataset])\n",
    "            if word in dict:\n",
    "                dict[word][1] = to_put\n",
    "            else:\n",
    "                dict[word] = ['O', to_put]\n",
    "                \n",
    "        for i in dict.values():\n",
    "            all_gt.append(i[0])\n",
    "            all_pred.append(i[1])\n",
    "\n",
    "    micro_f1 = f1_score(all_gt, all_pred, average='micro', labels = list(kw[task][dataset]))\n",
    "    macro_f1 = f1_score(all_gt, all_pred, average='macro', labels = list(kw[task][dataset]))\n",
    "    scores[task][dataset] = micro_f1, macro_f1\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "def evaluate_pc(dataset):\n",
    "    out_dict['pc'][dataset] = [(most_similar_answer(i[0], kw['pc'][dataset]), most_similar_answer(i[1], kw['pc'][dataset])) for i in out_dict['pc'][dataset]]\n",
    "\n",
    "    micro_f1 = f1_score(*list(zip(*out_dict['pc'][dataset])), average='micro', labels = list(kw['pc'][dataset]))\n",
    "    macro_f1 = f1_score(*list(zip(*out_dict['pc'][dataset])), average='macro', labels = list(kw['pc'][dataset]))\n",
    "    scores['pc'][dataset] = micro_f1, macro_f1\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "def evaluate_sc(dataset):\n",
    "    out_dict['sc'][dataset] = [(most_similar_answer(i[0], kw['sc'][dataset]), most_similar_answer(i[1], kw['sc'][dataset])) for i in out_dict['sc'][dataset]]\n",
    "    \n",
    "    micro_f1 = f1_score(*list(zip(*out_dict['sc'][dataset])), average='micro', labels = list(kw['sc'][dataset]))\n",
    "    macro_f1 = f1_score(*list(zip(*out_dict['sc'][dataset])), average='macro', labels = list(kw['sc'][dataset]))\n",
    "    scores['sc'][dataset] = micro_f1, macro_f1\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "def evaluate_re(dataset):\n",
    "    out_dict['re'][dataset] = [(most_similar_answer(i[0], kw['re'][dataset]), most_similar_answer(i[1], kw['re'][dataset])) for i in out_dict['re'][dataset]]\n",
    "    \n",
    "    micro_f1 = f1_score(*list(zip(*out_dict['re'][dataset])), average='micro', labels = list(kw['re'][dataset]))\n",
    "    macro_f1 = f1_score(*list(zip(*out_dict['re'][dataset])), average='macro', labels = list(kw['re'][dataset]))\n",
    "    scores['re'][dataset] = micro_f1, macro_f1\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "    \n",
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def get_raw_scores(dataset):\n",
    "    exact_scores = []\n",
    "    f1_scores = []\n",
    "    for article in dataset:\n",
    "        gold_answers = [article[0] if normalize_answer(article[0]) else None]\n",
    "        if not gold_answers[0]:\n",
    "            print(\"sad\")\n",
    "            gold_answers = ['']\n",
    "        a_pred = article[1]\n",
    "        exact_scores.append(max(compute_exact(a, a_pred) for a in gold_answers))\n",
    "        f1_scores.append(max(compute_f1(a, a_pred) for a in gold_answers))\n",
    "    return exact_scores, f1_scores\n",
    "\n",
    "def make_eval_dict(exact_scores, f1_scores):\n",
    "    total = len(exact_scores)\n",
    "    assert len(exact_scores) == len(f1_scores)\n",
    "    return collections.OrderedDict([\n",
    "        ('exact', 1.0 * sum(k for k in exact_scores) / total),\n",
    "        ('f1', 1.0 * sum(k for k in f1_scores) / total),\n",
    "        ('total', total),\n",
    "    ])\n",
    "\n",
    "def most_similar_answer(a,answer_set):\n",
    "    a = a.strip().replace(' ', '')\n",
    "    if(a in answer_set):\n",
    "        return a\n",
    "    dis = [Levenshtein.distance(a,x) for x in answer_set]\n",
    "    idx = np.argmin(dis)\n",
    "    return answer_set[idx]\n",
    "\n",
    "def evaluate_qna(dataset):\n",
    "    exact_raw, f1_raw = get_raw_scores(out_dict['qna'][dataset])\n",
    "    out_eval = make_eval_dict(exact_raw, f1_raw)\n",
    "    scores['qna'][dataset] = out_eval['f1'], out_eval['exact']\n",
    "    \n",
    "    return out_eval\n",
    "\n",
    "evaluate_qna('squad')\n",
    "# evaluate('ner', 'matscholar')\n",
    "# evaluate('ner', 'sofc_token')\n",
    "# evaluate('ner', 'sc_comics')\n",
    "# evaluate('sar', 'synthesis_actions')\n",
    "# evaluate('ee', 'sc_comics')\n",
    "# evaluate('sf', 'sofc_token')\n",
    "\n",
    "# evaluate_sc('sofc_sent')\n",
    "# evaluate_pc('glass_non_glass')\n",
    "# evaluate_re('structured_re')\n",
    "# evaluate_re('sc_comics')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3c81aa7-b5e9-402a-a47b-6cbaf1a2002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,sent in enumerate(valfile):\n",
    "    task = None\n",
    "    dataset = None\n",
    "    system = sent['system']\n",
    "    output = predictions[_]\n",
    "    answer = sent['answer']\n",
    "    if 'SOFC' in system:\n",
    "        task = 'sc'\n",
    "        dataset = 'sofc_sent'\n",
    "    elif 'named entity' in system:\n",
    "        task = 'ner'\n",
    "        if 'b-dsc' in system:\n",
    "            dataset = 'matscholar'\n",
    "        elif 'b-device' in system:\n",
    "            dataset = 'sofc_token'\n",
    "        elif 'doping' in system:\n",
    "            dataset = 'sc_comics'\n",
    "    elif 'synthesis action' in system:\n",
    "        task = 'sar'\n",
    "        dataset = 'synthesis_actions'\n",
    "    elif 'slots' in system:\n",
    "        task = 'sf'\n",
    "        dataset = 'sofc_token'\n",
    "    elif 'event, identify the roles of the arguments' in system:\n",
    "        task = 'ee'\n",
    "        dataset = 'sc_comics'\n",
    "    elif 'extract relation' in system:\n",
    "        task = 're'\n",
    "        for ds in kw[task]:\n",
    "            if kw[task][ds][0] in system:\n",
    "                dataset = ds\n",
    "                break   \n",
    "    elif 'inorganic glass' in system:\n",
    "        task = 'pc'\n",
    "        dataset = 'glass_non_glass'\n",
    "\n",
    "    assert task != None\n",
    "    assert dataset != None\n",
    "    \n",
    "    if task in [\"ner\", \"sf\", \"ee\", \"sar\"]:\n",
    "        answer = [i.split(\" : \") for i in answer.lower().split('\\n')]\n",
    "        try:\n",
    "            temp = [[j.strip().lower() for j in i.split(\":\")] for i in output.split('\\n')]\n",
    "            temp = [i for i in temp if len(i) == 2]\n",
    "            output = temp\n",
    "        except:\n",
    "            print(f\"for tc {_}, output pattern mismatched. {output}\")\n",
    "    else:\n",
    "        answer = answer.strip().lower()\n",
    "        output = output.strip().lower()\n",
    "\n",
    "    out_dict[task][dataset].append((answer, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46fb884a-86f7-488a-8d6d-8d610955adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {i : [] for i in tasks}\n",
    "scores[\"ner\"] = {i : (0,0) for i in [\"matscholar\", \"sofc_token\", \"sc_comics\"]}\n",
    "scores[\"pc\"] = {i : (0,0) for i in [\"glass_non_glass\"]}\n",
    "scores[\"sf\"] = {i : (0,0) for i in [\"sofc_token\"]}\n",
    "scores[\"ee\"] = {i : (0,0) for i in [\"sc_comics\"]}\n",
    "scores[\"re\"] = {i : (0,0) for i in [\"structured_re\", \"sc_comics\"]}\n",
    "scores[\"sar\"] = {i : (0,0) for i in [\"synthesis_actions\"]}\n",
    "scores[\"sc\"] = {i : (0,0) for i in [\"sofc_sent\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13eb2c0c-9b5c-4953-b0ef-a21c49d3b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse/btech/cs1200389/.conda/envs/matllama-inference/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9904306220095693, 0.9924803055621867)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(task, dataset):\n",
    "    all_gt = []\n",
    "    all_pred = []\n",
    "    for gt, pred in out_dict[task][dataset]:\n",
    "        dict = {}\n",
    "        for word, entity in gt:\n",
    "            dict[word] = [entity, 'O']\n",
    "                \n",
    "        for word, entity in pred:\n",
    "            to_put = most_similar_answer(entity, kw[task][dataset])\n",
    "            if word in dict:\n",
    "                dict[word][1] = to_put\n",
    "            else:\n",
    "                dict[word] = ['O', to_put]\n",
    "                \n",
    "        for i in dict.values():\n",
    "            all_gt.append(i[0])\n",
    "            all_pred.append(i[1])\n",
    "\n",
    "    micro_f1 = f1_score(all_gt, all_pred, average='micro', labels = list(kw[task][dataset]))\n",
    "    macro_f1 = f1_score(all_gt, all_pred, average='macro', labels = list(kw[task][dataset]))\n",
    "    scores[task][dataset] = micro_f1, macro_f1\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "def evaluate_pc(dataset):\n",
    "    out_dict['pc'][dataset] = [(most_similar_answer(i[0], kw['pc'][dataset]), most_similar_answer(i[1], kw['pc'][dataset])) for i in out_dict['pc'][dataset]]\n",
    "\n",
    "    micro_f1 = f1_score(*list(zip(*out_dict['pc'][dataset])), average='micro', labels = list(kw['pc'][dataset]))\n",
    "    macro_f1 = f1_score(*list(zip(*out_dict['pc'][dataset])), average='macro', labels = list(kw['pc'][dataset]))\n",
    "    scores['pc'][dataset] = micro_f1, macro_f1\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "def evaluate_sc(dataset):\n",
    "    out_dict['sc'][dataset] = [(most_similar_answer(i[0], kw['sc'][dataset]), most_similar_answer(i[1], kw['sc'][dataset])) for i in out_dict['sc'][dataset]]\n",
    "    \n",
    "    micro_f1 = f1_score(*list(zip(*out_dict['sc'][dataset])), average='micro', labels = list(kw['sc'][dataset]))\n",
    "    macro_f1 = f1_score(*list(zip(*out_dict['sc'][dataset])), average='macro', labels = list(kw['sc'][dataset]))\n",
    "    scores['sc'][dataset] = micro_f1, macro_f1\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "def evaluate_re(dataset):\n",
    "    out_dict['re'][dataset] = [(most_similar_answer(i[0], kw['re'][dataset]), most_similar_answer(i[1], kw['re'][dataset])) for i in out_dict['re'][dataset]]\n",
    "    \n",
    "    micro_f1 = f1_score(*list(zip(*out_dict['re'][dataset])), average='micro', labels = list(kw['re'][dataset]))\n",
    "    macro_f1 = f1_score(*list(zip(*out_dict['re'][dataset])), average='macro', labels = list(kw['re'][dataset]))\n",
    "    scores['re'][dataset] = micro_f1, macro_f1\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "evaluate('ner', 'matscholar')\n",
    "evaluate('ner', 'sofc_token')\n",
    "evaluate('ner', 'sc_comics')\n",
    "evaluate('sar', 'synthesis_actions')\n",
    "evaluate('ee', 'sc_comics')\n",
    "evaluate('sf', 'sofc_token')\n",
    "\n",
    "evaluate_sc('sofc_sent')\n",
    "evaluate_pc('glass_non_glass')\n",
    "evaluate_re('structured_re')\n",
    "evaluate_re('sc_comics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a120f363-07b3-421e-8da6-0bab36b00da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({(i,j): scores[i][j] for i in scores.keys() for j in scores[i].keys()},orient='index', columns = ['micro-f1', 'macro-f1'])\n",
    "df = df.apply(lambda x: 100 * round(x, 5))\n",
    "\n",
    "# print()\n",
    "# print(args.pred_file)\n",
    "# print(df.to_string())\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59d5eb87-9e00-442e-b25e-f53374556a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "daddy_df = [df, df, df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "89c711e4-60e3-49be-ac81-e285f7df7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3c987bf4-796b-430e-a863-73588f7d469c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'num_seeds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_mean, df_std], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39msort_index(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[38;5;28mround\u001b[39m(x,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===eval over average of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mnum_seeds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m runs===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_df\u001b[38;5;241m.\u001b[39mto_string())\n\u001b[1;32m     16\u001b[0m prefix \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'num_seeds'"
     ]
    }
   ],
   "source": [
    "df_sum = reduce(lambda x, y: x.add(y, fill_value=0), daddy_df)\n",
    "df_mean = df_sum / len(daddy_df)\n",
    "df_concat = pd.concat(daddy_df)\n",
    "df_std = df_concat.groupby(df_concat.index).std()\n",
    "\n",
    "df_mean.columns = pd.MultiIndex.from_tuples([(i, f\"mean\") for i in df_mean.columns])\n",
    "df_std.columns = pd.MultiIndex.from_tuples([(i, f\"std\") for i in df_std.columns])\n",
    "\n",
    "combined_df = pd.concat([df_mean, df_std], axis=1)\n",
    "combined_df = combined_df.sort_index(axis=1).apply(lambda x : round(x,5))\n",
    "\n",
    "\n",
    "print(f\"===eval over average of {args.num_seeds} runs===\")\n",
    "print(combined_df.to_string())\n",
    "\n",
    "prefix = args.checkpoint.split('/')[-1]\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "datetimestring = now.strftime(\"%y-%m-%d-%H-%M\")\n",
    "\n",
    "combined_df.to_csv(f'csvs/{prefix}-{datetimestring}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1bb362e2-35fa-4448-a207-3119d4e2c6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">macro-f1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">micro-f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(ner, matscholar)</th>\n",
       "      <td>80.555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.442</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(ner, sofc_token)</th>\n",
       "      <td>76.826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.260</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(ner, sc_comics)</th>\n",
       "      <td>88.974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.752</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(pc, glass_non_glass)</th>\n",
       "      <td>92.945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(sf, sofc_token)</th>\n",
       "      <td>68.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.169</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(ee, sc_comics)</th>\n",
       "      <td>87.843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(re, structured_re)</th>\n",
       "      <td>100.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(re, sc_comics)</th>\n",
       "      <td>99.248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.043</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(sar, synthesis_actions)</th>\n",
       "      <td>94.737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.682</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(sc, sofc_sent)</th>\n",
       "      <td>78.980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.978</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         macro-f1      micro-f1     \n",
       "                             mean  std     mean  std\n",
       "(ner, matscholar)          80.555  0.0   85.442  0.0\n",
       "(ner, sofc_token)          76.826  0.0   88.260  0.0\n",
       "(ner, sc_comics)           88.974  0.0   91.752  0.0\n",
       "(pc, glass_non_glass)      92.945  0.0   94.000  0.0\n",
       "(sf, sofc_token)           68.625  0.0   78.169  0.0\n",
       "(ee, sc_comics)            87.843  0.0   91.000  0.0\n",
       "(re, structured_re)       100.000  0.0  100.000  0.0\n",
       "(re, sc_comics)            99.248  0.0   99.043  0.0\n",
       "(sar, synthesis_actions)   94.737  0.0   96.682  0.0\n",
       "(sc, sofc_sent)            78.980  0.0   93.978  0.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38537697-6b4e-4560-b195-1fda23a80f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matllama-inference",
   "language": "python",
   "name": "matllama-inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
