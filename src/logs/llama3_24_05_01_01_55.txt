This is the arguments from the loaded checkpoint.
Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, hidden_size=4096, ffn_hidden_size=14336, num_attention_heads=32, num_attention_heads_kv=8, kv_channels=128, max_position_embeddings=8192, make_vocab_size_divisible_by=128, layernorm_epsilon=1e-05, apply_residual_connection_post_layernorm=False, use_bias=False, use_rms_norm=True, use_post_ln=False, onnx_safe=None, glu_activation='swiglu', position_embedding_type=<PositionEmbeddingType.rotary: 1>, rope_scaling_factor=1.0, rope_theta=500000.0, parallel_attn=False, parallel_layernorm=False, tie_embed_logits=False, sliding_window_size=None, attention_dropout=0.0, hidden_dropout=0.0, lima_dropout=False, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, micro_batch_size=2, global_batch_size=1024, rampup_batch_size=None, recompute_granularity='selective', distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, train_iters=14369, skip_iters=[], train_samples=None, log_interval=1, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='/scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3/tensorboards/llama2-7b-tp1-pp4', masked_softmax_fusion=True, bias_gelu_fusion=False, bias_dropout_fusion=False, use_flash_attn=True, optimizer='adam', dataloader_type='cyclic', async_tensor_model_parallel_allreduce=True, no_persist_layer_norm=False, sequence_parallel=False, gradient_accumulation_fusion=True, seed=1234, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=0.0003, lr_decay_style='cosine', lr_decay_iters=14369, lr_decay_samples=None, lr_warmup_fraction=None, lr_warmup_iters=2000, lr_warmup_samples=0, min_lr=3e-05, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='/scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3', save_interval=150, no_save_optim=None, no_save_rng=None, load='/scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3', load_iters=None, no_load_optim=None, no_load_rng=None, finetune=False, perform_initialization=True, use_checkpoint_args=True, fp16=False, bf16=True, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=True, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, pipeline_model_parallel_size=4, pipeline_model_parallel_split_rank=None, num_layers_per_virtual_pipeline_stage=None, distributed_backend='nccl', DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_only=False, eval_iters=10, eval_interval=150, data_path=None, split='969, 30, 1', train_data_path=['/scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/train/train_text_document'], valid_data_path=['/scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/val/val_text_document'], test_data_path=['/scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/redp_val_text_document'], vocab_file='/scratch/cse/btech/cs1200448/hf-to-meditron-weights/8b/tokenizer.model', merge_file=None, vocab_extra_ids=0, vocab_extra_ids_list=None, seq_length=2048, variable_seq_lengths=False, scalar_loss_mask=0.0, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='Tiktoken', tokenizer_model=None, new_tokens=False, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=False, log_validation_ppl_to_tensorboard=True, log_memory_to_tensorboard=True, log_world_size_to_tensorboard=False, wandb_logger=False, wandb_project=None, wandb_entity='meditron', wandb_name=None, wandb_id=None, wandb_resume='allow', wandb_api_key=None, metrics=[], inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', model_name='llama2', model_type='encoder_or_decoder', data_type='gpt', log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, rank=0, world_size=8, iteration=600, padded_vocab_size=128256, transformer_pipeline_model_parallel_size=4, data_parallel_size=2, virtual_pipeline_model_parallel_size=None, params_dtype=torch.bfloat16, consumed_train_samples=1228800, consumed_valid_samples=163840, do_train=1, do_valid=1, do_test=1, curr_iteration=1199)
using world size: 8, data-parallel-size: 2, tensor-model-parallel size: 1, pipeline-model-parallel size: 4 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:Tiktoken
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
setting number of micro-batches to constant 256
> building Tiktoken tokenizer ...
> initializing torch distributed ...
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 4
> setting random seeds to 1234 ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-05
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.0
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_load ....................................... None
  bf16 ............................................ True
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 2
  data_path ....................................... None
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  data_type ....................................... gpt
  dataloader_type ................................. cyclic
  DDP_impl ........................................ local
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  encoder_num_layers .............................. 32
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 150
  eval_iters ...................................... 10
  eval_only ....................................... False
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_signal_handler ............................. False
  ffn_hidden_size ................................. 14336
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_e4m3 ........................................ False
  fp8_hybrid ...................................... False
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 1024
  glu_activation .................................. swiglu
  gradient_accumulation_fusion .................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 4096
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  iteration ....................................... 1200
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lima_dropout .................................... False
  load ............................................ /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
  load_iters ...................................... None
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... True
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0003
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 2000
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 8192
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  metrics ......................................... []
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 3e-05
  mmap_warmup ..................................... False
  model_name ...................................... llama2
  model_type ...................................... encoder_or_decoder
  new_tokens ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_kv .......................... 8
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_layers ...................................... 32
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  optimizer ....................................... adam
  override_opt_param_scheduler .................... False
  padded_vocab_size ............................... 128256
  parallel_attn ................................... False
  parallel_layernorm .............................. False
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 4
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... PositionEmbeddingType.rotary
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ 1
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  rope_scaling_factor ............................. 1.0
  rope_theta ...................................... 500000.0
  sample_rate ..................................... 1.0
  save ............................................ /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
  save_interval ................................... 150
  scalar_loss_mask ................................ 0.0
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_iters ...................................... []
  sliding_window_size ............................. None
  split ........................................... 969, 30, 1
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3/tensorboards/llama2-7b-tp1-pp4
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. ['/scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/redp_val_text_document']
  tie_embed_logits ................................ False
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. Tiktoken
  train_data_path ................................. ['/scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/train/train_text_document']
  train_iters ..................................... 14369
  train_samples ................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 4
  use_bias ........................................ False
  use_checkpoint_args ............................. True
  use_checkpoint_opt_param_scheduler .............. False
  use_contiguous_buffers_in_local_ddp ............. True
  use_cpu_initialization .......................... None
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. True
  use_one_sent_docs ............................... False
  use_post_ln ..................................... False
  use_ring_exchange_p2p ........................... False
  use_rms_norm .................................... True
  valid_data_path ................................. ['/scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/val/val_text_document']
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_extra_ids_list ............................ None
  vocab_file ...................................... /scratch/cse/btech/cs1200448/hf-to-meditron-weights/8b/tokenizer.model
  wandb_api_key ................................... None
  wandb_entity .................................... meditron
  wandb_id ........................................ None
  wandb_logger .................................... False
  wandb_name ...................................... None
  wandb_project ................................... None
  wandb_resume .................................... allow
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 8
-------------------- end of arguments ---------------------
time to initialize megatron (seconds): 88.437
[after megatron is initialized] datetime: 2024-05-01 01:58:21 
Building model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1744896000
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1744896000
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2270232576
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2270236672
> learning rate decay style: cosine
 loading checkpoint from /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3 at iteration 1200
 checkpoint version 3.0
 > using checkpoint value 0.0003 for learning rate
 > using checkpoint value 3e-05 for minimum learning rate
 > using checkpoint value 2048000 for warmup iterations
 > using checkpoint value 14713856 for total number of iterations
 > using checkpoint value cosine for learning rate decay style
 > using checkpoint value 0.1 for start weight decay
 > using checkpoint value 0.1 for end weight decay
 > using checkpoint value 14713856 for total number of weight decay iterations
 > using checkpoint value constant for weight decay incr style
  successfully loaded checkpoint from /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3 at iteration 1200
(min, max) time across ranks (ms):
    load-checkpoint ................................: (93274.22, 93274.95)
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-05-01 02:00:05 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      14713856
    validation: 983040
    test:       10240
> building train, validation, and test datasets ...
Separate data paths provided for train, valid & test. Split string will be ignored.
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.059578 seconds
    number of documents: 4399676
    number of tokens: 30135507860
    train:
     document indices in [0, 4399676) total of 4399676 documents
 > loading doc-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/train/train_text_document_train_indexmap_14713856ns_2048sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/train/train_text_document_train_indexmap_14713856ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/train/train_text_document_train_indexmap_14713856ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.007 seconds
    total number of tokens: 30135507860
    total number of samples: 14714604
    total number of epochs: 1
 > building dataset index ...
    warming up index mmap file...
    reading sizes...
    reading pointers...
    reading document index...
    warming up data mmap file...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 1.192190 seconds
    number of documents: 106395
    number of tokens: 514977472
    valid:
     document indices in [0, 106395) total of 106395 documents
 > loading doc-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/val/val_text_document_valid_indexmap_983040ns_2048sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/val/val_text_document_valid_indexmap_983040ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/val/val_text_document_valid_indexmap_983040ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.096 seconds
    total number of tokens: 514977472
    total number of samples: 1005816
    total number of epochs: 4
 > building dataset index ...
    warming up index mmap file...
    reading sizes...
    reading pointers...
    reading document index...
    warming up data mmap file...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.363436 seconds
    number of documents: 46530
    number of tokens: 68328730
    test:
     document indices in [0, 46530) total of 46530 documents
 > loading doc-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/redp_val_text_document_test_indexmap_10240ns_2048sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/redp_val_text_document_test_indexmap_10240ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/redp_val_text_document_test_indexmap_10240ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.086 seconds
    total number of tokens: 68328730
    total number of samples: 33364
    total number of epochs: 1
> finished creating datasets ...
[after dataloaders are built] datetime: 2024-05-01 02:00:10 
done with setup ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (103854.55, 103878.16)
    train/valid/test-data-iterators-setup ..........: (5278.80, 5395.42)
training ...
[before the start of training step] datetime: 2024-05-01 02:00:10 
 iteration     1201/   14369 | consumed samples:      1229824 | elapsed time per iteration (ms): 150940.0 | rate (tokens/sec): 13893.94 | learning rate: 1.801E-04 | global batch size:  1024 | lm loss: 1.743731E+00 | loss scale: 1.0 | grad norm: 2.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 2] (after 1201 iterations) memory (MB) | allocated: 30033.376953125 | max allocated: 54222.16357421875 | reserved: 54482.0 | max reserved: 54482.0
[Rank 6] (after 1201 iterations) memory (MB) | allocated: 39051.4482421875 | max allocated: 52246.119140625 | reserved: 54388.0 | max reserved: 54388.0
[Rank 0] (after 1201 iterations) memory (MB) | allocated: 38987.376953125 | max allocated: 71068.55078125 | reserved: 72260.0 | max reserved: 72260.0
[Rank 4] (after 1201 iterations) memory (MB) | allocated: 30033.376953125 | max allocated: 46233.9013671875 | reserved: 46558.0 | max reserved: 46558.0
 iteration     1202/   14369 | consumed samples:      1230848 | elapsed time per iteration (ms): 123161.6 | rate (tokens/sec): 17027.64 | learning rate: 1.803E-04 | global batch size:  1024 | lm loss: 1.731942E+00 | loss scale: 1.0 | grad norm: 1.201 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1203/   14369 | consumed samples:      1231872 | elapsed time per iteration (ms): 122939.9 | rate (tokens/sec): 17058.35 | learning rate: 1.804E-04 | global batch size:  1024 | lm loss: 1.781098E+00 | loss scale: 1.0 | grad norm: 3.773 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1204/   14369 | consumed samples:      1232896 | elapsed time per iteration (ms): 121406.9 | rate (tokens/sec): 17273.75 | learning rate: 1.806E-04 | global batch size:  1024 | lm loss: 1.781218E+00 | loss scale: 1.0 | grad norm: 3.727 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1205/   14369 | consumed samples:      1233920 | elapsed time per iteration (ms): 120561.9 | rate (tokens/sec): 17394.81 | learning rate: 1.808E-04 | global batch size:  1024 | lm loss: 1.773893E+00 | loss scale: 1.0 | grad norm: 2.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1206/   14369 | consumed samples:      1234944 | elapsed time per iteration (ms): 117292.6 | rate (tokens/sec): 17879.67 | learning rate: 1.809E-04 | global batch size:  1024 | lm loss: 1.782252E+00 | loss scale: 1.0 | grad norm: 3.058 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1207/   14369 | consumed samples:      1235968 | elapsed time per iteration (ms): 116506.9 | rate (tokens/sec): 18000.24 | learning rate: 1.810E-04 | global batch size:  1024 | lm loss: 1.766264E+00 | loss scale: 1.0 | grad norm: 2.167 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1208/   14369 | consumed samples:      1236992 | elapsed time per iteration (ms): 113833.0 | rate (tokens/sec): 18423.06 | learning rate: 1.812E-04 | global batch size:  1024 | lm loss: 1.761836E+00 | loss scale: 1.0 | grad norm: 2.447 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1209/   14369 | consumed samples:      1238016 | elapsed time per iteration (ms): 114419.9 | rate (tokens/sec): 18328.56 | learning rate: 1.813E-04 | global batch size:  1024 | lm loss: 1.745082E+00 | loss scale: 1.0 | grad norm: 1.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1210/   14369 | consumed samples:      1239040 | elapsed time per iteration (ms): 113921.7 | rate (tokens/sec): 18408.71 | learning rate: 1.815E-04 | global batch size:  1024 | lm loss: 1.754125E+00 | loss scale: 1.0 | grad norm: 2.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1211/   14369 | consumed samples:      1240064 | elapsed time per iteration (ms): 113623.0 | rate (tokens/sec): 18457.12 | learning rate: 1.816E-04 | global batch size:  1024 | lm loss: 1.770490E+00 | loss scale: 1.0 | grad norm: 1.773 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1212/   14369 | consumed samples:      1241088 | elapsed time per iteration (ms): 114439.8 | rate (tokens/sec): 18325.37 | learning rate: 1.818E-04 | global batch size:  1024 | lm loss: 1.751748E+00 | loss scale: 1.0 | grad norm: 1.784 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1213/   14369 | consumed samples:      1242112 | elapsed time per iteration (ms): 114094.9 | rate (tokens/sec): 18380.77 | learning rate: 1.819E-04 | global batch size:  1024 | lm loss: 1.761418E+00 | loss scale: 1.0 | grad norm: 1.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1214/   14369 | consumed samples:      1243136 | elapsed time per iteration (ms): 114583.8 | rate (tokens/sec): 18302.34 | learning rate: 1.821E-04 | global batch size:  1024 | lm loss: 1.726111E+00 | loss scale: 1.0 | grad norm: 1.211 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1215/   14369 | consumed samples:      1244160 | elapsed time per iteration (ms): 113824.9 | rate (tokens/sec): 18424.37 | learning rate: 1.822E-04 | global batch size:  1024 | lm loss: 1.737042E+00 | loss scale: 1.0 | grad norm: 1.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1216/   14369 | consumed samples:      1245184 | elapsed time per iteration (ms): 113946.3 | rate (tokens/sec): 18404.74 | learning rate: 1.824E-04 | global batch size:  1024 | lm loss: 1.750041E+00 | loss scale: 1.0 | grad norm: 1.786 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1217/   14369 | consumed samples:      1246208 | elapsed time per iteration (ms): 113955.9 | rate (tokens/sec): 18403.19 | learning rate: 1.825E-04 | global batch size:  1024 | lm loss: 1.767444E+00 | loss scale: 1.0 | grad norm: 1.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1218/   14369 | consumed samples:      1247232 | elapsed time per iteration (ms): 113761.8 | rate (tokens/sec): 18434.59 | learning rate: 1.827E-04 | global batch size:  1024 | lm loss: 1.738428E+00 | loss scale: 1.0 | grad norm: 1.890 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1219/   14369 | consumed samples:      1248256 | elapsed time per iteration (ms): 113615.0 | rate (tokens/sec): 18458.41 | learning rate: 1.828E-04 | global batch size:  1024 | lm loss: 1.748047E+00 | loss scale: 1.0 | grad norm: 1.982 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1220/   14369 | consumed samples:      1249280 | elapsed time per iteration (ms): 113145.9 | rate (tokens/sec): 18534.94 | learning rate: 1.830E-04 | global batch size:  1024 | lm loss: 1.729753E+00 | loss scale: 1.0 | grad norm: 1.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1221/   14369 | consumed samples:      1250304 | elapsed time per iteration (ms): 112195.9 | rate (tokens/sec): 18691.87 | learning rate: 1.831E-04 | global batch size:  1024 | lm loss: 1.745954E+00 | loss scale: 1.0 | grad norm: 1.788 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1222/   14369 | consumed samples:      1251328 | elapsed time per iteration (ms): 113279.9 | rate (tokens/sec): 18513.01 | learning rate: 1.833E-04 | global batch size:  1024 | lm loss: 1.721727E+00 | loss scale: 1.0 | grad norm: 1.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1223/   14369 | consumed samples:      1252352 | elapsed time per iteration (ms): 112656.4 | rate (tokens/sec): 18615.47 | learning rate: 1.834E-04 | global batch size:  1024 | lm loss: 1.735035E+00 | loss scale: 1.0 | grad norm: 1.775 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1224/   14369 | consumed samples:      1253376 | elapsed time per iteration (ms): 112623.7 | rate (tokens/sec): 18620.88 | learning rate: 1.836E-04 | global batch size:  1024 | lm loss: 1.730112E+00 | loss scale: 1.0 | grad norm: 1.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1225/   14369 | consumed samples:      1254400 | elapsed time per iteration (ms): 111537.9 | rate (tokens/sec): 18802.14 | learning rate: 1.837E-04 | global batch size:  1024 | lm loss: 1.729127E+00 | loss scale: 1.0 | grad norm: 1.666 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1226/   14369 | consumed samples:      1255424 | elapsed time per iteration (ms): 112926.9 | rate (tokens/sec): 18570.88 | learning rate: 1.839E-04 | global batch size:  1024 | lm loss: 1.745874E+00 | loss scale: 1.0 | grad norm: 1.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1227/   14369 | consumed samples:      1256448 | elapsed time per iteration (ms): 112169.3 | rate (tokens/sec): 18696.30 | learning rate: 1.841E-04 | global batch size:  1024 | lm loss: 1.718039E+00 | loss scale: 1.0 | grad norm: 1.783 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1228/   14369 | consumed samples:      1257472 | elapsed time per iteration (ms): 113222.8 | rate (tokens/sec): 18522.35 | learning rate: 1.842E-04 | global batch size:  1024 | lm loss: 1.738762E+00 | loss scale: 1.0 | grad norm: 1.597 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1229/   14369 | consumed samples:      1258496 | elapsed time per iteration (ms): 111759.9 | rate (tokens/sec): 18764.80 | learning rate: 1.843E-04 | global batch size:  1024 | lm loss: 1.749207E+00 | loss scale: 1.0 | grad norm: 1.788 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1230/   14369 | consumed samples:      1259520 | elapsed time per iteration (ms): 112321.8 | rate (tokens/sec): 18670.93 | learning rate: 1.845E-04 | global batch size:  1024 | lm loss: 1.729525E+00 | loss scale: 1.0 | grad norm: 1.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1231/   14369 | consumed samples:      1260544 | elapsed time per iteration (ms): 112260.0 | rate (tokens/sec): 18681.20 | learning rate: 1.846E-04 | global batch size:  1024 | lm loss: 1.728726E+00 | loss scale: 1.0 | grad norm: 1.698 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1232/   14369 | consumed samples:      1261568 | elapsed time per iteration (ms): 113024.9 | rate (tokens/sec): 18554.78 | learning rate: 1.848E-04 | global batch size:  1024 | lm loss: 1.718408E+00 | loss scale: 1.0 | grad norm: 1.802 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1233/   14369 | consumed samples:      1262592 | elapsed time per iteration (ms): 113534.9 | rate (tokens/sec): 18471.43 | learning rate: 1.849E-04 | global batch size:  1024 | lm loss: 1.749670E+00 | loss scale: 1.0 | grad norm: 1.415 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1234/   14369 | consumed samples:      1263616 | elapsed time per iteration (ms): 117339.9 | rate (tokens/sec): 17872.46 | learning rate: 1.851E-04 | global batch size:  1024 | lm loss: 1.735288E+00 | loss scale: 1.0 | grad norm: 2.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1235/   14369 | consumed samples:      1264640 | elapsed time per iteration (ms): 120040.8 | rate (tokens/sec): 17470.33 | learning rate: 1.852E-04 | global batch size:  1024 | lm loss: 1.738645E+00 | loss scale: 1.0 | grad norm: 1.445 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1236/   14369 | consumed samples:      1265664 | elapsed time per iteration (ms): 117069.9 | rate (tokens/sec): 17913.67 | learning rate: 1.854E-04 | global batch size:  1024 | lm loss: 1.734133E+00 | loss scale: 1.0 | grad norm: 1.674 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1237/   14369 | consumed samples:      1266688 | elapsed time per iteration (ms): 113934.7 | rate (tokens/sec): 18406.61 | learning rate: 1.855E-04 | global batch size:  1024 | lm loss: 1.756093E+00 | loss scale: 1.0 | grad norm: 1.691 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1238/   14369 | consumed samples:      1267712 | elapsed time per iteration (ms): 114714.8 | rate (tokens/sec): 18281.44 | learning rate: 1.857E-04 | global batch size:  1024 | lm loss: 1.725123E+00 | loss scale: 1.0 | grad norm: 1.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1239/   14369 | consumed samples:      1268736 | elapsed time per iteration (ms): 114299.7 | rate (tokens/sec): 18347.83 | learning rate: 1.858E-04 | global batch size:  1024 | lm loss: 1.752447E+00 | loss scale: 1.0 | grad norm: 1.662 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1240/   14369 | consumed samples:      1269760 | elapsed time per iteration (ms): 114281.9 | rate (tokens/sec): 18350.69 | learning rate: 1.860E-04 | global batch size:  1024 | lm loss: 1.745857E+00 | loss scale: 1.0 | grad norm: 1.630 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1241/   14369 | consumed samples:      1270784 | elapsed time per iteration (ms): 115874.2 | rate (tokens/sec): 18098.52 | learning rate: 1.861E-04 | global batch size:  1024 | lm loss: 1.749643E+00 | loss scale: 1.0 | grad norm: 1.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1242/   14369 | consumed samples:      1271808 | elapsed time per iteration (ms): 113563.8 | rate (tokens/sec): 18466.73 | learning rate: 1.863E-04 | global batch size:  1024 | lm loss: 1.734917E+00 | loss scale: 1.0 | grad norm: 1.902 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1243/   14369 | consumed samples:      1272832 | elapsed time per iteration (ms): 113575.8 | rate (tokens/sec): 18464.77 | learning rate: 1.864E-04 | global batch size:  1024 | lm loss: 1.735458E+00 | loss scale: 1.0 | grad norm: 1.675 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1244/   14369 | consumed samples:      1273856 | elapsed time per iteration (ms): 113301.9 | rate (tokens/sec): 18509.42 | learning rate: 1.866E-04 | global batch size:  1024 | lm loss: 1.727121E+00 | loss scale: 1.0 | grad norm: 1.854 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1245/   14369 | consumed samples:      1274880 | elapsed time per iteration (ms): 112841.9 | rate (tokens/sec): 18584.87 | learning rate: 1.867E-04 | global batch size:  1024 | lm loss: 1.738587E+00 | loss scale: 1.0 | grad norm: 1.707 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1246/   14369 | consumed samples:      1275904 | elapsed time per iteration (ms): 113300.0 | rate (tokens/sec): 18509.72 | learning rate: 1.869E-04 | global batch size:  1024 | lm loss: 1.723439E+00 | loss scale: 1.0 | grad norm: 1.743 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1247/   14369 | consumed samples:      1276928 | elapsed time per iteration (ms): 113381.8 | rate (tokens/sec): 18496.38 | learning rate: 1.870E-04 | global batch size:  1024 | lm loss: 1.722980E+00 | loss scale: 1.0 | grad norm: 1.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1248/   14369 | consumed samples:      1277952 | elapsed time per iteration (ms): 113792.5 | rate (tokens/sec): 18429.62 | learning rate: 1.872E-04 | global batch size:  1024 | lm loss: 1.728081E+00 | loss scale: 1.0 | grad norm: 1.603 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1249/   14369 | consumed samples:      1278976 | elapsed time per iteration (ms): 112559.9 | rate (tokens/sec): 18631.43 | learning rate: 1.874E-04 | global batch size:  1024 | lm loss: 1.729942E+00 | loss scale: 1.0 | grad norm: 1.609 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1250/   14369 | consumed samples:      1280000 | elapsed time per iteration (ms): 112721.7 | rate (tokens/sec): 18604.69 | learning rate: 1.875E-04 | global batch size:  1024 | lm loss: 1.735840E+00 | loss scale: 1.0 | grad norm: 1.879 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1251/   14369 | consumed samples:      1281024 | elapsed time per iteration (ms): 113181.8 | rate (tokens/sec): 18529.06 | learning rate: 1.876E-04 | global batch size:  1024 | lm loss: 1.723171E+00 | loss scale: 1.0 | grad norm: 1.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1252/   14369 | consumed samples:      1282048 | elapsed time per iteration (ms): 112809.8 | rate (tokens/sec): 18590.16 | learning rate: 1.878E-04 | global batch size:  1024 | lm loss: 1.738818E+00 | loss scale: 1.0 | grad norm: 1.773 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1253/   14369 | consumed samples:      1283072 | elapsed time per iteration (ms): 112519.9 | rate (tokens/sec): 18638.06 | learning rate: 1.879E-04 | global batch size:  1024 | lm loss: 1.712365E+00 | loss scale: 1.0 | grad norm: 1.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1254/   14369 | consumed samples:      1284096 | elapsed time per iteration (ms): 113219.9 | rate (tokens/sec): 18522.82 | learning rate: 1.881E-04 | global batch size:  1024 | lm loss: 1.719922E+00 | loss scale: 1.0 | grad norm: 1.971 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1255/   14369 | consumed samples:      1285120 | elapsed time per iteration (ms): 112795.9 | rate (tokens/sec): 18592.45 | learning rate: 1.882E-04 | global batch size:  1024 | lm loss: 1.722795E+00 | loss scale: 1.0 | grad norm: 1.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1256/   14369 | consumed samples:      1286144 | elapsed time per iteration (ms): 112855.0 | rate (tokens/sec): 18582.71 | learning rate: 1.884E-04 | global batch size:  1024 | lm loss: 1.731323E+00 | loss scale: 1.0 | grad norm: 1.855 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1257/   14369 | consumed samples:      1287168 | elapsed time per iteration (ms): 112966.9 | rate (tokens/sec): 18564.31 | learning rate: 1.886E-04 | global batch size:  1024 | lm loss: 1.719095E+00 | loss scale: 1.0 | grad norm: 1.243 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1258/   14369 | consumed samples:      1288192 | elapsed time per iteration (ms): 112415.9 | rate (tokens/sec): 18655.30 | learning rate: 1.887E-04 | global batch size:  1024 | lm loss: 1.726707E+00 | loss scale: 1.0 | grad norm: 1.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1259/   14369 | consumed samples:      1289216 | elapsed time per iteration (ms): 113227.0 | rate (tokens/sec): 18521.66 | learning rate: 1.888E-04 | global batch size:  1024 | lm loss: 1.738259E+00 | loss scale: 1.0 | grad norm: 1.614 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1260/   14369 | consumed samples:      1290240 | elapsed time per iteration (ms): 113224.2 | rate (tokens/sec): 18522.11 | learning rate: 1.890E-04 | global batch size:  1024 | lm loss: 1.739022E+00 | loss scale: 1.0 | grad norm: 1.912 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1261/   14369 | consumed samples:      1291264 | elapsed time per iteration (ms): 112743.8 | rate (tokens/sec): 18601.05 | learning rate: 1.891E-04 | global batch size:  1024 | lm loss: 1.735633E+00 | loss scale: 1.0 | grad norm: 1.179 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1262/   14369 | consumed samples:      1292288 | elapsed time per iteration (ms): 113061.9 | rate (tokens/sec): 18548.70 | learning rate: 1.893E-04 | global batch size:  1024 | lm loss: 1.732390E+00 | loss scale: 1.0 | grad norm: 1.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1263/   14369 | consumed samples:      1293312 | elapsed time per iteration (ms): 111399.0 | rate (tokens/sec): 18825.59 | learning rate: 1.894E-04 | global batch size:  1024 | lm loss: 1.741106E+00 | loss scale: 1.0 | grad norm: 1.574 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1264/   14369 | consumed samples:      1294336 | elapsed time per iteration (ms): 112540.9 | rate (tokens/sec): 18634.58 | learning rate: 1.896E-04 | global batch size:  1024 | lm loss: 1.728126E+00 | loss scale: 1.0 | grad norm: 1.585 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1265/   14369 | consumed samples:      1295360 | elapsed time per iteration (ms): 112445.7 | rate (tokens/sec): 18650.36 | learning rate: 1.897E-04 | global batch size:  1024 | lm loss: 1.723256E+00 | loss scale: 1.0 | grad norm: 1.965 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1266/   14369 | consumed samples:      1296384 | elapsed time per iteration (ms): 113380.0 | rate (tokens/sec): 18496.66 | learning rate: 1.899E-04 | global batch size:  1024 | lm loss: 1.730311E+00 | loss scale: 1.0 | grad norm: 1.244 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1267/   14369 | consumed samples:      1297408 | elapsed time per iteration (ms): 113880.2 | rate (tokens/sec): 18415.42 | learning rate: 1.900E-04 | global batch size:  1024 | lm loss: 1.727145E+00 | loss scale: 1.0 | grad norm: 2.176 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1268/   14369 | consumed samples:      1298432 | elapsed time per iteration (ms): 112093.3 | rate (tokens/sec): 18708.99 | learning rate: 1.902E-04 | global batch size:  1024 | lm loss: 1.737665E+00 | loss scale: 1.0 | grad norm: 1.272 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1269/   14369 | consumed samples:      1299456 | elapsed time per iteration (ms): 112742.5 | rate (tokens/sec): 18601.25 | learning rate: 1.903E-04 | global batch size:  1024 | lm loss: 1.741620E+00 | loss scale: 1.0 | grad norm: 2.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1270/   14369 | consumed samples:      1300480 | elapsed time per iteration (ms): 111360.6 | rate (tokens/sec): 18832.08 | learning rate: 1.905E-04 | global batch size:  1024 | lm loss: 1.722560E+00 | loss scale: 1.0 | grad norm: 1.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1271/   14369 | consumed samples:      1301504 | elapsed time per iteration (ms): 112480.8 | rate (tokens/sec): 18644.53 | learning rate: 1.906E-04 | global batch size:  1024 | lm loss: 1.722677E+00 | loss scale: 1.0 | grad norm: 1.467 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1272/   14369 | consumed samples:      1302528 | elapsed time per iteration (ms): 112725.3 | rate (tokens/sec): 18604.09 | learning rate: 1.908E-04 | global batch size:  1024 | lm loss: 1.755591E+00 | loss scale: 1.0 | grad norm: 1.728 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1273/   14369 | consumed samples:      1303552 | elapsed time per iteration (ms): 112055.1 | rate (tokens/sec): 18715.36 | learning rate: 1.909E-04 | global batch size:  1024 | lm loss: 1.722435E+00 | loss scale: 1.0 | grad norm: 1.704 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1274/   14369 | consumed samples:      1304576 | elapsed time per iteration (ms): 111861.6 | rate (tokens/sec): 18747.73 | learning rate: 1.911E-04 | global batch size:  1024 | lm loss: 1.720113E+00 | loss scale: 1.0 | grad norm: 2.036 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1275/   14369 | consumed samples:      1305600 | elapsed time per iteration (ms): 112086.9 | rate (tokens/sec): 18710.06 | learning rate: 1.912E-04 | global batch size:  1024 | lm loss: 1.726827E+00 | loss scale: 1.0 | grad norm: 1.575 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1276/   14369 | consumed samples:      1306624 | elapsed time per iteration (ms): 112441.8 | rate (tokens/sec): 18650.99 | learning rate: 1.914E-04 | global batch size:  1024 | lm loss: 1.725354E+00 | loss scale: 1.0 | grad norm: 1.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1277/   14369 | consumed samples:      1307648 | elapsed time per iteration (ms): 112500.0 | rate (tokens/sec): 18641.35 | learning rate: 1.915E-04 | global batch size:  1024 | lm loss: 1.743695E+00 | loss scale: 1.0 | grad norm: 1.923 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1278/   14369 | consumed samples:      1308672 | elapsed time per iteration (ms): 112173.6 | rate (tokens/sec): 18695.59 | learning rate: 1.917E-04 | global batch size:  1024 | lm loss: 1.720074E+00 | loss scale: 1.0 | grad norm: 1.359 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1279/   14369 | consumed samples:      1309696 | elapsed time per iteration (ms): 112666.2 | rate (tokens/sec): 18613.86 | learning rate: 1.919E-04 | global batch size:  1024 | lm loss: 1.737194E+00 | loss scale: 1.0 | grad norm: 1.617 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1280/   14369 | consumed samples:      1310720 | elapsed time per iteration (ms): 112006.9 | rate (tokens/sec): 18723.42 | learning rate: 1.920E-04 | global batch size:  1024 | lm loss: 1.741219E+00 | loss scale: 1.0 | grad norm: 1.633 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1281/   14369 | consumed samples:      1311744 | elapsed time per iteration (ms): 111889.3 | rate (tokens/sec): 18743.10 | learning rate: 1.921E-04 | global batch size:  1024 | lm loss: 1.735369E+00 | loss scale: 1.0 | grad norm: 1.647 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1282/   14369 | consumed samples:      1312768 | elapsed time per iteration (ms): 111661.0 | rate (tokens/sec): 18781.41 | learning rate: 1.923E-04 | global batch size:  1024 | lm loss: 1.721766E+00 | loss scale: 1.0 | grad norm: 1.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1283/   14369 | consumed samples:      1313792 | elapsed time per iteration (ms): 113320.7 | rate (tokens/sec): 18506.35 | learning rate: 1.924E-04 | global batch size:  1024 | lm loss: 1.750233E+00 | loss scale: 1.0 | grad norm: 1.892 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1284/   14369 | consumed samples:      1314816 | elapsed time per iteration (ms): 113096.0 | rate (tokens/sec): 18543.11 | learning rate: 1.926E-04 | global batch size:  1024 | lm loss: 1.733200E+00 | loss scale: 1.0 | grad norm: 1.271 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1285/   14369 | consumed samples:      1315840 | elapsed time per iteration (ms): 111874.9 | rate (tokens/sec): 18745.51 | learning rate: 1.927E-04 | global batch size:  1024 | lm loss: 1.751372E+00 | loss scale: 1.0 | grad norm: 1.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1286/   14369 | consumed samples:      1316864 | elapsed time per iteration (ms): 111946.9 | rate (tokens/sec): 18733.46 | learning rate: 1.929E-04 | global batch size:  1024 | lm loss: 1.765949E+00 | loss scale: 1.0 | grad norm: 1.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1287/   14369 | consumed samples:      1317888 | elapsed time per iteration (ms): 111653.0 | rate (tokens/sec): 18782.76 | learning rate: 1.930E-04 | global batch size:  1024 | lm loss: 1.723074E+00 | loss scale: 1.0 | grad norm: 1.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1288/   14369 | consumed samples:      1318912 | elapsed time per iteration (ms): 112030.9 | rate (tokens/sec): 18719.40 | learning rate: 1.932E-04 | global batch size:  1024 | lm loss: 1.721044E+00 | loss scale: 1.0 | grad norm: 1.678 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1289/   14369 | consumed samples:      1319936 | elapsed time per iteration (ms): 111819.9 | rate (tokens/sec): 18754.73 | learning rate: 1.933E-04 | global batch size:  1024 | lm loss: 1.723873E+00 | loss scale: 1.0 | grad norm: 1.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1290/   14369 | consumed samples:      1320960 | elapsed time per iteration (ms): 112468.9 | rate (tokens/sec): 18646.51 | learning rate: 1.935E-04 | global batch size:  1024 | lm loss: 1.719976E+00 | loss scale: 1.0 | grad norm: 1.556 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1291/   14369 | consumed samples:      1321984 | elapsed time per iteration (ms): 114034.8 | rate (tokens/sec): 18390.46 | learning rate: 1.936E-04 | global batch size:  1024 | lm loss: 1.707635E+00 | loss scale: 1.0 | grad norm: 1.736 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1292/   14369 | consumed samples:      1323008 | elapsed time per iteration (ms): 111228.8 | rate (tokens/sec): 18854.40 | learning rate: 1.938E-04 | global batch size:  1024 | lm loss: 1.708404E+00 | loss scale: 1.0 | grad norm: 1.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1293/   14369 | consumed samples:      1324032 | elapsed time per iteration (ms): 112860.0 | rate (tokens/sec): 18581.89 | learning rate: 1.939E-04 | global batch size:  1024 | lm loss: 1.729343E+00 | loss scale: 1.0 | grad norm: 1.857 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1294/   14369 | consumed samples:      1325056 | elapsed time per iteration (ms): 112286.9 | rate (tokens/sec): 18676.73 | learning rate: 1.941E-04 | global batch size:  1024 | lm loss: 1.731146E+00 | loss scale: 1.0 | grad norm: 1.561 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1295/   14369 | consumed samples:      1326080 | elapsed time per iteration (ms): 112053.0 | rate (tokens/sec): 18715.71 | learning rate: 1.942E-04 | global batch size:  1024 | lm loss: 1.742652E+00 | loss scale: 1.0 | grad norm: 1.889 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1296/   14369 | consumed samples:      1327104 | elapsed time per iteration (ms): 112681.7 | rate (tokens/sec): 18611.30 | learning rate: 1.944E-04 | global batch size:  1024 | lm loss: 1.730334E+00 | loss scale: 1.0 | grad norm: 1.747 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1297/   14369 | consumed samples:      1328128 | elapsed time per iteration (ms): 113019.7 | rate (tokens/sec): 18555.64 | learning rate: 1.945E-04 | global batch size:  1024 | lm loss: 1.723068E+00 | loss scale: 1.0 | grad norm: 1.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1298/   14369 | consumed samples:      1329152 | elapsed time per iteration (ms): 112047.5 | rate (tokens/sec): 18716.64 | learning rate: 1.947E-04 | global batch size:  1024 | lm loss: 1.735416E+00 | loss scale: 1.0 | grad norm: 1.891 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1299/   14369 | consumed samples:      1330176 | elapsed time per iteration (ms): 112361.8 | rate (tokens/sec): 18664.28 | learning rate: 1.948E-04 | global batch size:  1024 | lm loss: 1.723045E+00 | loss scale: 1.0 | grad norm: 1.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1300/   14369 | consumed samples:      1331200 | elapsed time per iteration (ms): 112644.5 | rate (tokens/sec): 18617.44 | learning rate: 1.950E-04 | global batch size:  1024 | lm loss: 1.723362E+00 | loss scale: 1.0 | grad norm: 1.754 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1301/   14369 | consumed samples:      1332224 | elapsed time per iteration (ms): 111538.0 | rate (tokens/sec): 18802.13 | learning rate: 1.952E-04 | global batch size:  1024 | lm loss: 1.717531E+00 | loss scale: 1.0 | grad norm: 1.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1302/   14369 | consumed samples:      1333248 | elapsed time per iteration (ms): 113022.0 | rate (tokens/sec): 18555.25 | learning rate: 1.953E-04 | global batch size:  1024 | lm loss: 1.721193E+00 | loss scale: 1.0 | grad norm: 1.851 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1303/   14369 | consumed samples:      1334272 | elapsed time per iteration (ms): 111988.8 | rate (tokens/sec): 18726.44 | learning rate: 1.954E-04 | global batch size:  1024 | lm loss: 1.712987E+00 | loss scale: 1.0 | grad norm: 1.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1304/   14369 | consumed samples:      1335296 | elapsed time per iteration (ms): 111517.8 | rate (tokens/sec): 18805.54 | learning rate: 1.956E-04 | global batch size:  1024 | lm loss: 1.722506E+00 | loss scale: 1.0 | grad norm: 1.600 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1305/   14369 | consumed samples:      1336320 | elapsed time per iteration (ms): 111801.7 | rate (tokens/sec): 18757.78 | learning rate: 1.957E-04 | global batch size:  1024 | lm loss: 1.721043E+00 | loss scale: 1.0 | grad norm: 1.928 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1306/   14369 | consumed samples:      1337344 | elapsed time per iteration (ms): 111901.9 | rate (tokens/sec): 18740.99 | learning rate: 1.959E-04 | global batch size:  1024 | lm loss: 1.727608E+00 | loss scale: 1.0 | grad norm: 2.213 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1307/   14369 | consumed samples:      1338368 | elapsed time per iteration (ms): 113191.2 | rate (tokens/sec): 18527.52 | learning rate: 1.960E-04 | global batch size:  1024 | lm loss: 1.710424E+00 | loss scale: 1.0 | grad norm: 0.989 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1308/   14369 | consumed samples:      1339392 | elapsed time per iteration (ms): 112108.0 | rate (tokens/sec): 18706.54 | learning rate: 1.962E-04 | global batch size:  1024 | lm loss: 1.739862E+00 | loss scale: 1.0 | grad norm: 3.362 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1309/   14369 | consumed samples:      1340416 | elapsed time per iteration (ms): 112300.8 | rate (tokens/sec): 18674.42 | learning rate: 1.963E-04 | global batch size:  1024 | lm loss: 1.750690E+00 | loss scale: 1.0 | grad norm: 1.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1310/   14369 | consumed samples:      1341440 | elapsed time per iteration (ms): 112614.8 | rate (tokens/sec): 18622.35 | learning rate: 1.965E-04 | global batch size:  1024 | lm loss: 1.758424E+00 | loss scale: 1.0 | grad norm: 2.872 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1311/   14369 | consumed samples:      1342464 | elapsed time per iteration (ms): 112250.1 | rate (tokens/sec): 18682.85 | learning rate: 1.966E-04 | global batch size:  1024 | lm loss: 1.760634E+00 | loss scale: 1.0 | grad norm: 1.903 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1312/   14369 | consumed samples:      1343488 | elapsed time per iteration (ms): 112061.8 | rate (tokens/sec): 18714.25 | learning rate: 1.968E-04 | global batch size:  1024 | lm loss: 1.749357E+00 | loss scale: 1.0 | grad norm: 2.560 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1313/   14369 | consumed samples:      1344512 | elapsed time per iteration (ms): 112481.7 | rate (tokens/sec): 18644.38 | learning rate: 1.969E-04 | global batch size:  1024 | lm loss: 1.744806E+00 | loss scale: 1.0 | grad norm: 1.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1314/   14369 | consumed samples:      1345536 | elapsed time per iteration (ms): 111899.9 | rate (tokens/sec): 18741.32 | learning rate: 1.971E-04 | global batch size:  1024 | lm loss: 1.741116E+00 | loss scale: 1.0 | grad norm: 2.651 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1315/   14369 | consumed samples:      1346560 | elapsed time per iteration (ms): 112308.7 | rate (tokens/sec): 18673.10 | learning rate: 1.972E-04 | global batch size:  1024 | lm loss: 1.734208E+00 | loss scale: 1.0 | grad norm: 1.560 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1316/   14369 | consumed samples:      1347584 | elapsed time per iteration (ms): 111620.0 | rate (tokens/sec): 18788.31 | learning rate: 1.974E-04 | global batch size:  1024 | lm loss: 1.761469E+00 | loss scale: 1.0 | grad norm: 2.030 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1317/   14369 | consumed samples:      1348608 | elapsed time per iteration (ms): 111803.7 | rate (tokens/sec): 18757.45 | learning rate: 1.975E-04 | global batch size:  1024 | lm loss: 1.733356E+00 | loss scale: 1.0 | grad norm: 1.535 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1318/   14369 | consumed samples:      1349632 | elapsed time per iteration (ms): 111200.0 | rate (tokens/sec): 18859.28 | learning rate: 1.977E-04 | global batch size:  1024 | lm loss: 1.736129E+00 | loss scale: 1.0 | grad norm: 1.868 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1319/   14369 | consumed samples:      1350656 | elapsed time per iteration (ms): 111863.9 | rate (tokens/sec): 18747.36 | learning rate: 1.978E-04 | global batch size:  1024 | lm loss: 1.738982E+00 | loss scale: 1.0 | grad norm: 1.939 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1320/   14369 | consumed samples:      1351680 | elapsed time per iteration (ms): 112200.5 | rate (tokens/sec): 18691.11 | learning rate: 1.980E-04 | global batch size:  1024 | lm loss: 1.739618E+00 | loss scale: 1.0 | grad norm: 1.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1321/   14369 | consumed samples:      1352704 | elapsed time per iteration (ms): 112220.0 | rate (tokens/sec): 18687.86 | learning rate: 1.981E-04 | global batch size:  1024 | lm loss: 1.741026E+00 | loss scale: 1.0 | grad norm: 1.566 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1322/   14369 | consumed samples:      1353728 | elapsed time per iteration (ms): 112246.9 | rate (tokens/sec): 18683.38 | learning rate: 1.983E-04 | global batch size:  1024 | lm loss: 1.710870E+00 | loss scale: 1.0 | grad norm: 2.298 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1323/   14369 | consumed samples:      1354752 | elapsed time per iteration (ms): 111834.7 | rate (tokens/sec): 18752.25 | learning rate: 1.984E-04 | global batch size:  1024 | lm loss: 1.736495E+00 | loss scale: 1.0 | grad norm: 1.256 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1324/   14369 | consumed samples:      1355776 | elapsed time per iteration (ms): 111605.7 | rate (tokens/sec): 18790.72 | learning rate: 1.986E-04 | global batch size:  1024 | lm loss: 1.735607E+00 | loss scale: 1.0 | grad norm: 1.984 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1325/   14369 | consumed samples:      1356800 | elapsed time per iteration (ms): 111594.8 | rate (tokens/sec): 18792.56 | learning rate: 1.987E-04 | global batch size:  1024 | lm loss: 1.726623E+00 | loss scale: 1.0 | grad norm: 1.517 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1326/   14369 | consumed samples:      1357824 | elapsed time per iteration (ms): 112069.4 | rate (tokens/sec): 18712.97 | learning rate: 1.989E-04 | global batch size:  1024 | lm loss: 1.738895E+00 | loss scale: 1.0 | grad norm: 2.149 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1327/   14369 | consumed samples:      1358848 | elapsed time per iteration (ms): 111553.0 | rate (tokens/sec): 18799.60 | learning rate: 1.990E-04 | global batch size:  1024 | lm loss: 1.749199E+00 | loss scale: 1.0 | grad norm: 2.080 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1328/   14369 | consumed samples:      1359872 | elapsed time per iteration (ms): 111981.0 | rate (tokens/sec): 18727.75 | learning rate: 1.992E-04 | global batch size:  1024 | lm loss: 1.726781E+00 | loss scale: 1.0 | grad norm: 0.974 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1329/   14369 | consumed samples:      1360896 | elapsed time per iteration (ms): 112261.8 | rate (tokens/sec): 18680.91 | learning rate: 1.993E-04 | global batch size:  1024 | lm loss: 1.739126E+00 | loss scale: 1.0 | grad norm: 2.727 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1330/   14369 | consumed samples:      1361920 | elapsed time per iteration (ms): 111620.0 | rate (tokens/sec): 18788.31 | learning rate: 1.995E-04 | global batch size:  1024 | lm loss: 1.748467E+00 | loss scale: 1.0 | grad norm: 1.552 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1331/   14369 | consumed samples:      1362944 | elapsed time per iteration (ms): 112586.9 | rate (tokens/sec): 18626.96 | learning rate: 1.997E-04 | global batch size:  1024 | lm loss: 1.801496E+00 | loss scale: 1.0 | grad norm: 2.141 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1332/   14369 | consumed samples:      1363968 | elapsed time per iteration (ms): 112448.9 | rate (tokens/sec): 18649.82 | learning rate: 1.998E-04 | global batch size:  1024 | lm loss: 1.771491E+00 | loss scale: 1.0 | grad norm: 2.048 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1333/   14369 | consumed samples:      1364992 | elapsed time per iteration (ms): 112780.0 | rate (tokens/sec): 18595.07 | learning rate: 1.999E-04 | global batch size:  1024 | lm loss: 1.751402E+00 | loss scale: 1.0 | grad norm: 1.887 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1334/   14369 | consumed samples:      1366016 | elapsed time per iteration (ms): 112001.9 | rate (tokens/sec): 18724.25 | learning rate: 2.001E-04 | global batch size:  1024 | lm loss: 1.745941E+00 | loss scale: 1.0 | grad norm: 2.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1335/   14369 | consumed samples:      1367040 | elapsed time per iteration (ms): 111821.7 | rate (tokens/sec): 18754.43 | learning rate: 2.002E-04 | global batch size:  1024 | lm loss: 1.743994E+00 | loss scale: 1.0 | grad norm: 1.420 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1336/   14369 | consumed samples:      1368064 | elapsed time per iteration (ms): 115167.0 | rate (tokens/sec): 18209.67 | learning rate: 2.004E-04 | global batch size:  1024 | lm loss: 1.738048E+00 | loss scale: 1.0 | grad norm: 2.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1337/   14369 | consumed samples:      1369088 | elapsed time per iteration (ms): 111485.8 | rate (tokens/sec): 18810.93 | learning rate: 2.005E-04 | global batch size:  1024 | lm loss: 1.744173E+00 | loss scale: 1.0 | grad norm: 1.712 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1338/   14369 | consumed samples:      1370112 | elapsed time per iteration (ms): 112476.0 | rate (tokens/sec): 18645.33 | learning rate: 2.007E-04 | global batch size:  1024 | lm loss: 1.735873E+00 | loss scale: 1.0 | grad norm: 2.148 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1339/   14369 | consumed samples:      1371136 | elapsed time per iteration (ms): 111742.0 | rate (tokens/sec): 18767.80 | learning rate: 2.008E-04 | global batch size:  1024 | lm loss: 1.749625E+00 | loss scale: 1.0 | grad norm: 1.497 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1340/   14369 | consumed samples:      1372160 | elapsed time per iteration (ms): 111577.9 | rate (tokens/sec): 18795.41 | learning rate: 2.010E-04 | global batch size:  1024 | lm loss: 1.740148E+00 | loss scale: 1.0 | grad norm: 2.024 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1341/   14369 | consumed samples:      1373184 | elapsed time per iteration (ms): 113139.9 | rate (tokens/sec): 18535.92 | learning rate: 2.011E-04 | global batch size:  1024 | lm loss: 1.711412E+00 | loss scale: 1.0 | grad norm: 1.331 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1342/   14369 | consumed samples:      1374208 | elapsed time per iteration (ms): 112826.9 | rate (tokens/sec): 18587.34 | learning rate: 2.013E-04 | global batch size:  1024 | lm loss: 1.744799E+00 | loss scale: 1.0 | grad norm: 1.727 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1343/   14369 | consumed samples:      1375232 | elapsed time per iteration (ms): 112100.0 | rate (tokens/sec): 18707.86 | learning rate: 2.014E-04 | global batch size:  1024 | lm loss: 1.731592E+00 | loss scale: 1.0 | grad norm: 2.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1344/   14369 | consumed samples:      1376256 | elapsed time per iteration (ms): 112549.4 | rate (tokens/sec): 18633.17 | learning rate: 2.016E-04 | global batch size:  1024 | lm loss: 1.728744E+00 | loss scale: 1.0 | grad norm: 1.974 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1345/   14369 | consumed samples:      1377280 | elapsed time per iteration (ms): 111952.0 | rate (tokens/sec): 18732.60 | learning rate: 2.017E-04 | global batch size:  1024 | lm loss: 1.732099E+00 | loss scale: 1.0 | grad norm: 1.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1346/   14369 | consumed samples:      1378304 | elapsed time per iteration (ms): 111139.9 | rate (tokens/sec): 18869.48 | learning rate: 2.019E-04 | global batch size:  1024 | lm loss: 1.742607E+00 | loss scale: 1.0 | grad norm: 1.916 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1347/   14369 | consumed samples:      1379328 | elapsed time per iteration (ms): 112921.8 | rate (tokens/sec): 18571.73 | learning rate: 2.020E-04 | global batch size:  1024 | lm loss: 1.730813E+00 | loss scale: 1.0 | grad norm: 1.500 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1348/   14369 | consumed samples:      1380352 | elapsed time per iteration (ms): 111500.8 | rate (tokens/sec): 18808.40 | learning rate: 2.022E-04 | global batch size:  1024 | lm loss: 1.709753E+00 | loss scale: 1.0 | grad norm: 1.712 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1349/   14369 | consumed samples:      1381376 | elapsed time per iteration (ms): 112801.7 | rate (tokens/sec): 18591.50 | learning rate: 2.023E-04 | global batch size:  1024 | lm loss: 1.746636E+00 | loss scale: 1.0 | grad norm: 1.796 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1350/   14369 | consumed samples:      1382400 | elapsed time per iteration (ms): 113300.0 | rate (tokens/sec): 18509.72 | learning rate: 2.025E-04 | global batch size:  1024 | lm loss: 1.727029E+00 | loss scale: 1.0 | grad norm: 1.508 | number of skipped iterations:   0 | number of nan iterations:   0 |
------------------------------------------------------------------------------------------------
 validation loss at iteration 1350 | lm loss value: 1.877497E+00 | lm loss PPL: 6.537124E+00 | 
------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------
 test loss at iteration 1350 | lm loss value: 2.988312E+00 | lm loss PPL: 1.985213E+01 | 
------------------------------------------------------------------------------------------
saving checkpoint at iteration    1350 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
  successfully saved checkpoint at iteration    1350 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
(min, max) time across ranks (ms):
    save-checkpoint ................................: (115472.50, 115484.71)
 iteration     1351/   14369 | consumed samples:      1383424 | elapsed time per iteration (ms): 1209766.9 | rate (tokens/sec): 1733.52 | learning rate: 2.026E-04 | global batch size:  1024 | lm loss: 1.709083E+00 | loss scale: 1.0 | grad norm: 1.827 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1352/   14369 | consumed samples:      1384448 | elapsed time per iteration (ms): 111093.0 | rate (tokens/sec): 18877.44 | learning rate: 2.028E-04 | global batch size:  1024 | lm loss: 1.740370E+00 | loss scale: 1.0 | grad norm: 1.594 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1353/   14369 | consumed samples:      1385472 | elapsed time per iteration (ms): 111540.7 | rate (tokens/sec): 18801.67 | learning rate: 2.029E-04 | global batch size:  1024 | lm loss: 1.699630E+00 | loss scale: 1.0 | grad norm: 1.992 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1354/   14369 | consumed samples:      1386496 | elapsed time per iteration (ms): 112262.7 | rate (tokens/sec): 18680.76 | learning rate: 2.031E-04 | global batch size:  1024 | lm loss: 1.726323E+00 | loss scale: 1.0 | grad norm: 1.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1355/   14369 | consumed samples:      1387520 | elapsed time per iteration (ms): 112121.8 | rate (tokens/sec): 18704.22 | learning rate: 2.032E-04 | global batch size:  1024 | lm loss: 1.709962E+00 | loss scale: 1.0 | grad norm: 1.703 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1356/   14369 | consumed samples:      1388544 | elapsed time per iteration (ms): 113200.0 | rate (tokens/sec): 18526.07 | learning rate: 2.034E-04 | global batch size:  1024 | lm loss: 1.703156E+00 | loss scale: 1.0 | grad norm: 1.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1357/   14369 | consumed samples:      1389568 | elapsed time per iteration (ms): 113066.9 | rate (tokens/sec): 18547.89 | learning rate: 2.035E-04 | global batch size:  1024 | lm loss: 1.704790E+00 | loss scale: 1.0 | grad norm: 1.528 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1358/   14369 | consumed samples:      1390592 | elapsed time per iteration (ms): 112634.9 | rate (tokens/sec): 18619.02 | learning rate: 2.037E-04 | global batch size:  1024 | lm loss: 1.719223E+00 | loss scale: 1.0 | grad norm: 1.487 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1359/   14369 | consumed samples:      1391616 | elapsed time per iteration (ms): 112587.0 | rate (tokens/sec): 18626.94 | learning rate: 2.038E-04 | global batch size:  1024 | lm loss: 1.732114E+00 | loss scale: 1.0 | grad norm: 1.647 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1360/   14369 | consumed samples:      1392640 | elapsed time per iteration (ms): 112855.8 | rate (tokens/sec): 18582.58 | learning rate: 2.040E-04 | global batch size:  1024 | lm loss: 1.713408E+00 | loss scale: 1.0 | grad norm: 1.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1361/   14369 | consumed samples:      1393664 | elapsed time per iteration (ms): 112498.0 | rate (tokens/sec): 18641.68 | learning rate: 2.041E-04 | global batch size:  1024 | lm loss: 1.743189E+00 | loss scale: 1.0 | grad norm: 2.023 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1362/   14369 | consumed samples:      1394688 | elapsed time per iteration (ms): 112744.7 | rate (tokens/sec): 18600.89 | learning rate: 2.043E-04 | global batch size:  1024 | lm loss: 1.707303E+00 | loss scale: 1.0 | grad norm: 1.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1363/   14369 | consumed samples:      1395712 | elapsed time per iteration (ms): 112876.8 | rate (tokens/sec): 18579.12 | learning rate: 2.044E-04 | global batch size:  1024 | lm loss: 1.762479E+00 | loss scale: 1.0 | grad norm: 2.567 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1364/   14369 | consumed samples:      1396736 | elapsed time per iteration (ms): 111997.0 | rate (tokens/sec): 18725.07 | learning rate: 2.046E-04 | global batch size:  1024 | lm loss: 1.738631E+00 | loss scale: 1.0 | grad norm: 1.158 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1365/   14369 | consumed samples:      1397760 | elapsed time per iteration (ms): 112221.7 | rate (tokens/sec): 18687.58 | learning rate: 2.047E-04 | global batch size:  1024 | lm loss: 1.721002E+00 | loss scale: 1.0 | grad norm: 1.821 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1366/   14369 | consumed samples:      1398784 | elapsed time per iteration (ms): 113222.3 | rate (tokens/sec): 18522.43 | learning rate: 2.049E-04 | global batch size:  1024 | lm loss: 1.736320E+00 | loss scale: 1.0 | grad norm: 1.283 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1367/   14369 | consumed samples:      1399808 | elapsed time per iteration (ms): 113017.8 | rate (tokens/sec): 18555.95 | learning rate: 2.050E-04 | global batch size:  1024 | lm loss: 1.710353E+00 | loss scale: 1.0 | grad norm: 1.479 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1368/   14369 | consumed samples:      1400832 | elapsed time per iteration (ms): 112886.9 | rate (tokens/sec): 18577.46 | learning rate: 2.052E-04 | global batch size:  1024 | lm loss: 1.717716E+00 | loss scale: 1.0 | grad norm: 1.537 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1369/   14369 | consumed samples:      1401856 | elapsed time per iteration (ms): 112400.9 | rate (tokens/sec): 18657.79 | learning rate: 2.053E-04 | global batch size:  1024 | lm loss: 1.737283E+00 | loss scale: 1.0 | grad norm: 1.647 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1370/   14369 | consumed samples:      1402880 | elapsed time per iteration (ms): 113948.8 | rate (tokens/sec): 18404.33 | learning rate: 2.055E-04 | global batch size:  1024 | lm loss: 1.712297E+00 | loss scale: 1.0 | grad norm: 1.925 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1371/   14369 | consumed samples:      1403904 | elapsed time per iteration (ms): 113170.9 | rate (tokens/sec): 18530.85 | learning rate: 2.056E-04 | global batch size:  1024 | lm loss: 1.734256E+00 | loss scale: 1.0 | grad norm: 1.568 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1372/   14369 | consumed samples:      1404928 | elapsed time per iteration (ms): 113042.5 | rate (tokens/sec): 18551.90 | learning rate: 2.058E-04 | global batch size:  1024 | lm loss: 1.740985E+00 | loss scale: 1.0 | grad norm: 1.562 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1373/   14369 | consumed samples:      1405952 | elapsed time per iteration (ms): 112399.2 | rate (tokens/sec): 18658.07 | learning rate: 2.059E-04 | global batch size:  1024 | lm loss: 1.692083E+00 | loss scale: 1.0 | grad norm: 1.456 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1374/   14369 | consumed samples:      1406976 | elapsed time per iteration (ms): 113561.9 | rate (tokens/sec): 18467.04 | learning rate: 2.061E-04 | global batch size:  1024 | lm loss: 1.748876E+00 | loss scale: 1.0 | grad norm: 1.927 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1375/   14369 | consumed samples:      1408000 | elapsed time per iteration (ms): 114439.8 | rate (tokens/sec): 18325.37 | learning rate: 2.062E-04 | global batch size:  1024 | lm loss: 1.730671E+00 | loss scale: 1.0 | grad norm: 1.205 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1376/   14369 | consumed samples:      1409024 | elapsed time per iteration (ms): 113265.0 | rate (tokens/sec): 18515.44 | learning rate: 2.064E-04 | global batch size:  1024 | lm loss: 1.736349E+00 | loss scale: 1.0 | grad norm: 1.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1377/   14369 | consumed samples:      1410048 | elapsed time per iteration (ms): 113384.7 | rate (tokens/sec): 18495.90 | learning rate: 2.065E-04 | global batch size:  1024 | lm loss: 1.715854E+00 | loss scale: 1.0 | grad norm: 1.591 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1378/   14369 | consumed samples:      1411072 | elapsed time per iteration (ms): 112635.0 | rate (tokens/sec): 18619.01 | learning rate: 2.067E-04 | global batch size:  1024 | lm loss: 1.727033E+00 | loss scale: 1.0 | grad norm: 1.542 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1379/   14369 | consumed samples:      1412096 | elapsed time per iteration (ms): 112919.9 | rate (tokens/sec): 18572.03 | learning rate: 2.068E-04 | global batch size:  1024 | lm loss: 1.712468E+00 | loss scale: 1.0 | grad norm: 1.393 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1380/   14369 | consumed samples:      1413120 | elapsed time per iteration (ms): 113377.7 | rate (tokens/sec): 18497.03 | learning rate: 2.070E-04 | global batch size:  1024 | lm loss: 1.705935E+00 | loss scale: 1.0 | grad norm: 1.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1381/   14369 | consumed samples:      1414144 | elapsed time per iteration (ms): 112996.9 | rate (tokens/sec): 18559.38 | learning rate: 2.071E-04 | global batch size:  1024 | lm loss: 1.698493E+00 | loss scale: 1.0 | grad norm: 1.397 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1382/   14369 | consumed samples:      1415168 | elapsed time per iteration (ms): 113001.0 | rate (tokens/sec): 18558.70 | learning rate: 2.073E-04 | global batch size:  1024 | lm loss: 1.712908E+00 | loss scale: 1.0 | grad norm: 1.654 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1383/   14369 | consumed samples:      1416192 | elapsed time per iteration (ms): 113703.0 | rate (tokens/sec): 18444.12 | learning rate: 2.075E-04 | global batch size:  1024 | lm loss: 1.724305E+00 | loss scale: 1.0 | grad norm: 1.506 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1384/   14369 | consumed samples:      1417216 | elapsed time per iteration (ms): 112240.7 | rate (tokens/sec): 18684.41 | learning rate: 2.076E-04 | global batch size:  1024 | lm loss: 1.718360E+00 | loss scale: 1.0 | grad norm: 1.892 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1385/   14369 | consumed samples:      1418240 | elapsed time per iteration (ms): 112981.9 | rate (tokens/sec): 18561.83 | learning rate: 2.077E-04 | global batch size:  1024 | lm loss: 1.705716E+00 | loss scale: 1.0 | grad norm: 1.224 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1386/   14369 | consumed samples:      1419264 | elapsed time per iteration (ms): 112741.9 | rate (tokens/sec): 18601.36 | learning rate: 2.079E-04 | global batch size:  1024 | lm loss: 1.716607E+00 | loss scale: 1.0 | grad norm: 1.640 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1387/   14369 | consumed samples:      1420288 | elapsed time per iteration (ms): 112581.8 | rate (tokens/sec): 18627.81 | learning rate: 2.080E-04 | global batch size:  1024 | lm loss: 1.688278E+00 | loss scale: 1.0 | grad norm: 1.139 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1388/   14369 | consumed samples:      1421312 | elapsed time per iteration (ms): 111795.4 | rate (tokens/sec): 18758.85 | learning rate: 2.082E-04 | global batch size:  1024 | lm loss: 1.725397E+00 | loss scale: 1.0 | grad norm: 1.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1389/   14369 | consumed samples:      1422336 | elapsed time per iteration (ms): 111520.0 | rate (tokens/sec): 18805.16 | learning rate: 2.083E-04 | global batch size:  1024 | lm loss: 1.725925E+00 | loss scale: 1.0 | grad norm: 1.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1390/   14369 | consumed samples:      1423360 | elapsed time per iteration (ms): 111866.9 | rate (tokens/sec): 18746.85 | learning rate: 2.085E-04 | global batch size:  1024 | lm loss: 1.706606E+00 | loss scale: 1.0 | grad norm: 1.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1391/   14369 | consumed samples:      1424384 | elapsed time per iteration (ms): 112376.9 | rate (tokens/sec): 18661.77 | learning rate: 2.086E-04 | global batch size:  1024 | lm loss: 1.716059E+00 | loss scale: 1.0 | grad norm: 1.608 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1392/   14369 | consumed samples:      1425408 | elapsed time per iteration (ms): 112105.9 | rate (tokens/sec): 18706.88 | learning rate: 2.088E-04 | global batch size:  1024 | lm loss: 1.720688E+00 | loss scale: 1.0 | grad norm: 1.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1393/   14369 | consumed samples:      1426432 | elapsed time per iteration (ms): 112180.1 | rate (tokens/sec): 18694.52 | learning rate: 2.089E-04 | global batch size:  1024 | lm loss: 1.723950E+00 | loss scale: 1.0 | grad norm: 1.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1394/   14369 | consumed samples:      1427456 | elapsed time per iteration (ms): 112523.6 | rate (tokens/sec): 18637.44 | learning rate: 2.091E-04 | global batch size:  1024 | lm loss: 1.705519E+00 | loss scale: 1.0 | grad norm: 1.292 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1395/   14369 | consumed samples:      1428480 | elapsed time per iteration (ms): 111443.2 | rate (tokens/sec): 18818.12 | learning rate: 2.092E-04 | global batch size:  1024 | lm loss: 1.729150E+00 | loss scale: 1.0 | grad norm: 1.902 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1396/   14369 | consumed samples:      1429504 | elapsed time per iteration (ms): 113518.7 | rate (tokens/sec): 18474.07 | learning rate: 2.094E-04 | global batch size:  1024 | lm loss: 1.723503E+00 | loss scale: 1.0 | grad norm: 1.233 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1397/   14369 | consumed samples:      1430528 | elapsed time per iteration (ms): 111793.0 | rate (tokens/sec): 18759.25 | learning rate: 2.095E-04 | global batch size:  1024 | lm loss: 1.718941E+00 | loss scale: 1.0 | grad norm: 1.603 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1398/   14369 | consumed samples:      1431552 | elapsed time per iteration (ms): 112360.0 | rate (tokens/sec): 18664.57 | learning rate: 2.097E-04 | global batch size:  1024 | lm loss: 1.708140E+00 | loss scale: 1.0 | grad norm: 1.662 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1399/   14369 | consumed samples:      1432576 | elapsed time per iteration (ms): 113248.7 | rate (tokens/sec): 18518.12 | learning rate: 2.098E-04 | global batch size:  1024 | lm loss: 1.718094E+00 | loss scale: 1.0 | grad norm: 1.669 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1400/   14369 | consumed samples:      1433600 | elapsed time per iteration (ms): 112572.9 | rate (tokens/sec): 18629.28 | learning rate: 2.100E-04 | global batch size:  1024 | lm loss: 1.709415E+00 | loss scale: 1.0 | grad norm: 1.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1401/   14369 | consumed samples:      1434624 | elapsed time per iteration (ms): 111639.9 | rate (tokens/sec): 18784.97 | learning rate: 2.101E-04 | global batch size:  1024 | lm loss: 1.710095E+00 | loss scale: 1.0 | grad norm: 1.730 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1402/   14369 | consumed samples:      1435648 | elapsed time per iteration (ms): 112681.7 | rate (tokens/sec): 18611.29 | learning rate: 2.103E-04 | global batch size:  1024 | lm loss: 1.699758E+00 | loss scale: 1.0 | grad norm: 1.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1403/   14369 | consumed samples:      1436672 | elapsed time per iteration (ms): 111419.0 | rate (tokens/sec): 18822.21 | learning rate: 2.104E-04 | global batch size:  1024 | lm loss: 1.696756E+00 | loss scale: 1.0 | grad norm: 1.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1404/   14369 | consumed samples:      1437696 | elapsed time per iteration (ms): 111415.0 | rate (tokens/sec): 18822.88 | learning rate: 2.106E-04 | global batch size:  1024 | lm loss: 1.722769E+00 | loss scale: 1.0 | grad norm: 1.509 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1405/   14369 | consumed samples:      1438720 | elapsed time per iteration (ms): 112434.9 | rate (tokens/sec): 18652.15 | learning rate: 2.107E-04 | global batch size:  1024 | lm loss: 1.695758E+00 | loss scale: 1.0 | grad norm: 1.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1406/   14369 | consumed samples:      1439744 | elapsed time per iteration (ms): 112348.7 | rate (tokens/sec): 18666.46 | learning rate: 2.109E-04 | global batch size:  1024 | lm loss: 1.693202E+00 | loss scale: 1.0 | grad norm: 1.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1407/   14369 | consumed samples:      1440768 | elapsed time per iteration (ms): 114299.1 | rate (tokens/sec): 18347.93 | learning rate: 2.110E-04 | global batch size:  1024 | lm loss: 1.712197E+00 | loss scale: 1.0 | grad norm: 1.324 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1408/   14369 | consumed samples:      1441792 | elapsed time per iteration (ms): 112076.6 | rate (tokens/sec): 18711.77 | learning rate: 2.112E-04 | global batch size:  1024 | lm loss: 1.703118E+00 | loss scale: 1.0 | grad norm: 1.846 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1409/   14369 | consumed samples:      1442816 | elapsed time per iteration (ms): 112703.7 | rate (tokens/sec): 18607.66 | learning rate: 2.113E-04 | global batch size:  1024 | lm loss: 1.721713E+00 | loss scale: 1.0 | grad norm: 0.879 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1410/   14369 | consumed samples:      1443840 | elapsed time per iteration (ms): 112941.0 | rate (tokens/sec): 18568.56 | learning rate: 2.115E-04 | global batch size:  1024 | lm loss: 1.705216E+00 | loss scale: 1.0 | grad norm: 1.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1411/   14369 | consumed samples:      1444864 | elapsed time per iteration (ms): 111824.6 | rate (tokens/sec): 18753.93 | learning rate: 2.116E-04 | global batch size:  1024 | lm loss: 1.717112E+00 | loss scale: 1.0 | grad norm: 1.524 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1412/   14369 | consumed samples:      1445888 | elapsed time per iteration (ms): 113071.5 | rate (tokens/sec): 18547.13 | learning rate: 2.118E-04 | global batch size:  1024 | lm loss: 1.713992E+00 | loss scale: 1.0 | grad norm: 2.058 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1413/   14369 | consumed samples:      1446912 | elapsed time per iteration (ms): 112353.0 | rate (tokens/sec): 18665.74 | learning rate: 2.119E-04 | global batch size:  1024 | lm loss: 1.700101E+00 | loss scale: 1.0 | grad norm: 1.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1414/   14369 | consumed samples:      1447936 | elapsed time per iteration (ms): 111965.9 | rate (tokens/sec): 18730.27 | learning rate: 2.121E-04 | global batch size:  1024 | lm loss: 1.707425E+00 | loss scale: 1.0 | grad norm: 1.552 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1415/   14369 | consumed samples:      1448960 | elapsed time per iteration (ms): 112580.0 | rate (tokens/sec): 18628.10 | learning rate: 2.122E-04 | global batch size:  1024 | lm loss: 1.730745E+00 | loss scale: 1.0 | grad norm: 1.840 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1416/   14369 | consumed samples:      1449984 | elapsed time per iteration (ms): 111739.9 | rate (tokens/sec): 18768.16 | learning rate: 2.124E-04 | global batch size:  1024 | lm loss: 1.742458E+00 | loss scale: 1.0 | grad norm: 1.673 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1417/   14369 | consumed samples:      1451008 | elapsed time per iteration (ms): 112212.1 | rate (tokens/sec): 18689.18 | learning rate: 2.125E-04 | global batch size:  1024 | lm loss: 1.711249E+00 | loss scale: 1.0 | grad norm: 1.906 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1418/   14369 | consumed samples:      1452032 | elapsed time per iteration (ms): 111326.9 | rate (tokens/sec): 18837.79 | learning rate: 2.127E-04 | global batch size:  1024 | lm loss: 1.721886E+00 | loss scale: 1.0 | grad norm: 1.104 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1419/   14369 | consumed samples:      1453056 | elapsed time per iteration (ms): 113226.9 | rate (tokens/sec): 18521.68 | learning rate: 2.128E-04 | global batch size:  1024 | lm loss: 1.718773E+00 | loss scale: 1.0 | grad norm: 1.912 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1420/   14369 | consumed samples:      1454080 | elapsed time per iteration (ms): 112216.8 | rate (tokens/sec): 18688.40 | learning rate: 2.130E-04 | global batch size:  1024 | lm loss: 1.729417E+00 | loss scale: 1.0 | grad norm: 1.135 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1421/   14369 | consumed samples:      1455104 | elapsed time per iteration (ms): 111646.0 | rate (tokens/sec): 18783.94 | learning rate: 2.131E-04 | global batch size:  1024 | lm loss: 1.717351E+00 | loss scale: 1.0 | grad norm: 1.952 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1422/   14369 | consumed samples:      1456128 | elapsed time per iteration (ms): 111994.7 | rate (tokens/sec): 18725.45 | learning rate: 2.133E-04 | global batch size:  1024 | lm loss: 1.718234E+00 | loss scale: 1.0 | grad norm: 1.315 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1423/   14369 | consumed samples:      1457152 | elapsed time per iteration (ms): 111707.0 | rate (tokens/sec): 18773.68 | learning rate: 2.134E-04 | global batch size:  1024 | lm loss: 1.761140E+00 | loss scale: 1.0 | grad norm: 48.843 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1424/   14369 | consumed samples:      1458176 | elapsed time per iteration (ms): 112235.2 | rate (tokens/sec): 18685.33 | learning rate: 2.136E-04 | global batch size:  1024 | lm loss: 1.815395E+00 | loss scale: 1.0 | grad norm: 4.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1425/   14369 | consumed samples:      1459200 | elapsed time per iteration (ms): 112107.5 | rate (tokens/sec): 18706.62 | learning rate: 2.137E-04 | global batch size:  1024 | lm loss: 1.748695E+00 | loss scale: 1.0 | grad norm: 1.738 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1426/   14369 | consumed samples:      1460224 | elapsed time per iteration (ms): 112277.6 | rate (tokens/sec): 18678.28 | learning rate: 2.139E-04 | global batch size:  1024 | lm loss: 1.745479E+00 | loss scale: 1.0 | grad norm: 1.985 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1427/   14369 | consumed samples:      1461248 | elapsed time per iteration (ms): 111306.7 | rate (tokens/sec): 18841.19 | learning rate: 2.140E-04 | global batch size:  1024 | lm loss: 1.741308E+00 | loss scale: 1.0 | grad norm: 1.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1428/   14369 | consumed samples:      1462272 | elapsed time per iteration (ms): 111957.0 | rate (tokens/sec): 18731.76 | learning rate: 2.142E-04 | global batch size:  1024 | lm loss: 1.735529E+00 | loss scale: 1.0 | grad norm: 1.955 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1429/   14369 | consumed samples:      1463296 | elapsed time per iteration (ms): 111922.9 | rate (tokens/sec): 18737.47 | learning rate: 2.143E-04 | global batch size:  1024 | lm loss: 1.754018E+00 | loss scale: 1.0 | grad norm: 1.628 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1430/   14369 | consumed samples:      1464320 | elapsed time per iteration (ms): 111558.7 | rate (tokens/sec): 18798.63 | learning rate: 2.145E-04 | global batch size:  1024 | lm loss: 1.730290E+00 | loss scale: 1.0 | grad norm: 2.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1431/   14369 | consumed samples:      1465344 | elapsed time per iteration (ms): 115355.8 | rate (tokens/sec): 18179.86 | learning rate: 2.146E-04 | global batch size:  1024 | lm loss: 1.727625E+00 | loss scale: 1.0 | grad norm: 1.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1432/   14369 | consumed samples:      1466368 | elapsed time per iteration (ms): 112267.0 | rate (tokens/sec): 18680.04 | learning rate: 2.148E-04 | global batch size:  1024 | lm loss: 1.711152E+00 | loss scale: 1.0 | grad norm: 2.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1433/   14369 | consumed samples:      1467392 | elapsed time per iteration (ms): 111892.1 | rate (tokens/sec): 18742.63 | learning rate: 2.149E-04 | global batch size:  1024 | lm loss: 1.740424E+00 | loss scale: 1.0 | grad norm: 2.163 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1434/   14369 | consumed samples:      1468416 | elapsed time per iteration (ms): 111852.9 | rate (tokens/sec): 18749.20 | learning rate: 2.151E-04 | global batch size:  1024 | lm loss: 1.818337E+00 | loss scale: 1.0 | grad norm: 1.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1435/   14369 | consumed samples:      1469440 | elapsed time per iteration (ms): 111959.9 | rate (tokens/sec): 18731.28 | learning rate: 2.152E-04 | global batch size:  1024 | lm loss: 1.843251E+00 | loss scale: 1.0 | grad norm: 2.064 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1436/   14369 | consumed samples:      1470464 | elapsed time per iteration (ms): 111379.9 | rate (tokens/sec): 18828.82 | learning rate: 2.154E-04 | global batch size:  1024 | lm loss: 1.790059E+00 | loss scale: 1.0 | grad norm: 1.374 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1437/   14369 | consumed samples:      1471488 | elapsed time per iteration (ms): 111826.9 | rate (tokens/sec): 18753.56 | learning rate: 2.155E-04 | global batch size:  1024 | lm loss: 1.738498E+00 | loss scale: 1.0 | grad norm: 2.247 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1438/   14369 | consumed samples:      1472512 | elapsed time per iteration (ms): 111667.0 | rate (tokens/sec): 18780.41 | learning rate: 2.157E-04 | global batch size:  1024 | lm loss: 1.713323E+00 | loss scale: 1.0 | grad norm: 1.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1439/   14369 | consumed samples:      1473536 | elapsed time per iteration (ms): 111840.5 | rate (tokens/sec): 18751.28 | learning rate: 2.158E-04 | global batch size:  1024 | lm loss: 1.709455E+00 | loss scale: 1.0 | grad norm: 2.182 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1440/   14369 | consumed samples:      1474560 | elapsed time per iteration (ms): 111738.9 | rate (tokens/sec): 18768.33 | learning rate: 2.160E-04 | global batch size:  1024 | lm loss: 1.708860E+00 | loss scale: 1.0 | grad norm: 1.174 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1441/   14369 | consumed samples:      1475584 | elapsed time per iteration (ms): 110915.9 | rate (tokens/sec): 18907.60 | learning rate: 2.161E-04 | global batch size:  1024 | lm loss: 1.748564E+00 | loss scale: 1.0 | grad norm: 2.564 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1442/   14369 | consumed samples:      1476608 | elapsed time per iteration (ms): 113867.0 | rate (tokens/sec): 18417.55 | learning rate: 2.163E-04 | global batch size:  1024 | lm loss: 1.741032E+00 | loss scale: 1.0 | grad norm: 1.670 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1443/   14369 | consumed samples:      1477632 | elapsed time per iteration (ms): 113060.0 | rate (tokens/sec): 18549.01 | learning rate: 2.164E-04 | global batch size:  1024 | lm loss: 1.718568E+00 | loss scale: 1.0 | grad norm: 1.612 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1444/   14369 | consumed samples:      1478656 | elapsed time per iteration (ms): 112101.8 | rate (tokens/sec): 18707.56 | learning rate: 2.166E-04 | global batch size:  1024 | lm loss: 1.733791E+00 | loss scale: 1.0 | grad norm: 1.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1445/   14369 | consumed samples:      1479680 | elapsed time per iteration (ms): 111546.0 | rate (tokens/sec): 18800.78 | learning rate: 2.167E-04 | global batch size:  1024 | lm loss: 1.726361E+00 | loss scale: 1.0 | grad norm: 1.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1446/   14369 | consumed samples:      1480704 | elapsed time per iteration (ms): 112882.7 | rate (tokens/sec): 18578.15 | learning rate: 2.169E-04 | global batch size:  1024 | lm loss: 1.725782E+00 | loss scale: 1.0 | grad norm: 1.553 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1447/   14369 | consumed samples:      1481728 | elapsed time per iteration (ms): 113140.0 | rate (tokens/sec): 18535.90 | learning rate: 2.170E-04 | global batch size:  1024 | lm loss: 1.708186E+00 | loss scale: 1.0 | grad norm: 1.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1448/   14369 | consumed samples:      1482752 | elapsed time per iteration (ms): 111680.9 | rate (tokens/sec): 18778.08 | learning rate: 2.172E-04 | global batch size:  1024 | lm loss: 1.713842E+00 | loss scale: 1.0 | grad norm: 1.787 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1449/   14369 | consumed samples:      1483776 | elapsed time per iteration (ms): 111462.7 | rate (tokens/sec): 18814.84 | learning rate: 2.173E-04 | global batch size:  1024 | lm loss: 1.689094E+00 | loss scale: 1.0 | grad norm: 1.499 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1450/   14369 | consumed samples:      1484800 | elapsed time per iteration (ms): 112207.0 | rate (tokens/sec): 18690.02 | learning rate: 2.175E-04 | global batch size:  1024 | lm loss: 1.694985E+00 | loss scale: 1.0 | grad norm: 1.643 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1451/   14369 | consumed samples:      1485824 | elapsed time per iteration (ms): 112594.7 | rate (tokens/sec): 18625.68 | learning rate: 2.176E-04 | global batch size:  1024 | lm loss: 1.694665E+00 | loss scale: 1.0 | grad norm: 1.988 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1452/   14369 | consumed samples:      1486848 | elapsed time per iteration (ms): 112060.0 | rate (tokens/sec): 18714.54 | learning rate: 2.178E-04 | global batch size:  1024 | lm loss: 1.696805E+00 | loss scale: 1.0 | grad norm: 1.060 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1453/   14369 | consumed samples:      1487872 | elapsed time per iteration (ms): 111779.9 | rate (tokens/sec): 18761.44 | learning rate: 2.179E-04 | global batch size:  1024 | lm loss: 1.720268E+00 | loss scale: 1.0 | grad norm: 1.777 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1454/   14369 | consumed samples:      1488896 | elapsed time per iteration (ms): 111339.9 | rate (tokens/sec): 18835.58 | learning rate: 2.181E-04 | global batch size:  1024 | lm loss: 1.717583E+00 | loss scale: 1.0 | grad norm: 1.492 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1455/   14369 | consumed samples:      1489920 | elapsed time per iteration (ms): 112799.9 | rate (tokens/sec): 18591.79 | learning rate: 2.182E-04 | global batch size:  1024 | lm loss: 1.709565E+00 | loss scale: 1.0 | grad norm: 1.592 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1456/   14369 | consumed samples:      1490944 | elapsed time per iteration (ms): 112041.8 | rate (tokens/sec): 18717.59 | learning rate: 2.184E-04 | global batch size:  1024 | lm loss: 1.690220E+00 | loss scale: 1.0 | grad norm: 1.510 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1457/   14369 | consumed samples:      1491968 | elapsed time per iteration (ms): 112526.9 | rate (tokens/sec): 18636.90 | learning rate: 2.185E-04 | global batch size:  1024 | lm loss: 1.702196E+00 | loss scale: 1.0 | grad norm: 1.601 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1458/   14369 | consumed samples:      1492992 | elapsed time per iteration (ms): 112007.2 | rate (tokens/sec): 18723.37 | learning rate: 2.187E-04 | global batch size:  1024 | lm loss: 1.704311E+00 | loss scale: 1.0 | grad norm: 1.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1459/   14369 | consumed samples:      1494016 | elapsed time per iteration (ms): 112222.7 | rate (tokens/sec): 18687.41 | learning rate: 2.188E-04 | global batch size:  1024 | lm loss: 1.718690E+00 | loss scale: 1.0 | grad norm: 1.709 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1460/   14369 | consumed samples:      1495040 | elapsed time per iteration (ms): 111609.5 | rate (tokens/sec): 18790.08 | learning rate: 2.190E-04 | global batch size:  1024 | lm loss: 1.707239E+00 | loss scale: 1.0 | grad norm: 1.323 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1461/   14369 | consumed samples:      1496064 | elapsed time per iteration (ms): 111606.9 | rate (tokens/sec): 18790.52 | learning rate: 2.191E-04 | global batch size:  1024 | lm loss: 1.714564E+00 | loss scale: 1.0 | grad norm: 1.632 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1462/   14369 | consumed samples:      1497088 | elapsed time per iteration (ms): 111122.6 | rate (tokens/sec): 18872.42 | learning rate: 2.193E-04 | global batch size:  1024 | lm loss: 1.687233E+00 | loss scale: 1.0 | grad norm: 1.357 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1463/   14369 | consumed samples:      1498112 | elapsed time per iteration (ms): 112148.7 | rate (tokens/sec): 18699.75 | learning rate: 2.194E-04 | global batch size:  1024 | lm loss: 1.710321E+00 | loss scale: 1.0 | grad norm: 1.643 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1464/   14369 | consumed samples:      1499136 | elapsed time per iteration (ms): 112950.0 | rate (tokens/sec): 18567.09 | learning rate: 2.196E-04 | global batch size:  1024 | lm loss: 1.691703E+00 | loss scale: 1.0 | grad norm: 1.263 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1465/   14369 | consumed samples:      1500160 | elapsed time per iteration (ms): 112729.0 | rate (tokens/sec): 18603.49 | learning rate: 2.197E-04 | global batch size:  1024 | lm loss: 1.703596E+00 | loss scale: 1.0 | grad norm: 1.645 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1466/   14369 | consumed samples:      1501184 | elapsed time per iteration (ms): 116724.0 | rate (tokens/sec): 17966.76 | learning rate: 2.199E-04 | global batch size:  1024 | lm loss: 1.706583E+00 | loss scale: 1.0 | grad norm: 1.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1467/   14369 | consumed samples:      1502208 | elapsed time per iteration (ms): 111897.6 | rate (tokens/sec): 18741.71 | learning rate: 2.200E-04 | global batch size:  1024 | lm loss: 1.698027E+00 | loss scale: 1.0 | grad norm: 1.529 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1468/   14369 | consumed samples:      1503232 | elapsed time per iteration (ms): 112499.9 | rate (tokens/sec): 18641.37 | learning rate: 2.202E-04 | global batch size:  1024 | lm loss: 1.727410E+00 | loss scale: 1.0 | grad norm: 18.509 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1469/   14369 | consumed samples:      1504256 | elapsed time per iteration (ms): 112052.3 | rate (tokens/sec): 18715.83 | learning rate: 2.203E-04 | global batch size:  1024 | lm loss: 1.717178E+00 | loss scale: 1.0 | grad norm: 3.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1470/   14369 | consumed samples:      1505280 | elapsed time per iteration (ms): 112713.0 | rate (tokens/sec): 18606.12 | learning rate: 2.205E-04 | global batch size:  1024 | lm loss: 1.737436E+00 | loss scale: 1.0 | grad norm: 1.424 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1471/   14369 | consumed samples:      1506304 | elapsed time per iteration (ms): 111386.9 | rate (tokens/sec): 18827.64 | learning rate: 2.206E-04 | global batch size:  1024 | lm loss: 1.720928E+00 | loss scale: 1.0 | grad norm: 2.133 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1472/   14369 | consumed samples:      1507328 | elapsed time per iteration (ms): 112020.0 | rate (tokens/sec): 18721.22 | learning rate: 2.208E-04 | global batch size:  1024 | lm loss: 1.715042E+00 | loss scale: 1.0 | grad norm: 1.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1473/   14369 | consumed samples:      1508352 | elapsed time per iteration (ms): 112599.8 | rate (tokens/sec): 18624.84 | learning rate: 2.209E-04 | global batch size:  1024 | lm loss: 1.718492E+00 | loss scale: 1.0 | grad norm: 2.220 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1474/   14369 | consumed samples:      1509376 | elapsed time per iteration (ms): 111041.9 | rate (tokens/sec): 18886.14 | learning rate: 2.211E-04 | global batch size:  1024 | lm loss: 1.716145E+00 | loss scale: 1.0 | grad norm: 1.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1475/   14369 | consumed samples:      1510400 | elapsed time per iteration (ms): 111633.0 | rate (tokens/sec): 18786.13 | learning rate: 2.212E-04 | global batch size:  1024 | lm loss: 1.701424E+00 | loss scale: 1.0 | grad norm: 1.559 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1476/   14369 | consumed samples:      1511424 | elapsed time per iteration (ms): 112121.8 | rate (tokens/sec): 18704.24 | learning rate: 2.214E-04 | global batch size:  1024 | lm loss: 1.732224E+00 | loss scale: 1.0 | grad norm: 1.243 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1477/   14369 | consumed samples:      1512448 | elapsed time per iteration (ms): 112903.3 | rate (tokens/sec): 18574.76 | learning rate: 2.215E-04 | global batch size:  1024 | lm loss: 1.729590E+00 | loss scale: 1.0 | grad norm: 1.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1478/   14369 | consumed samples:      1513472 | elapsed time per iteration (ms): 117005.1 | rate (tokens/sec): 17923.59 | learning rate: 2.217E-04 | global batch size:  1024 | lm loss: 1.705754E+00 | loss scale: 1.0 | grad norm: 1.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1479/   14369 | consumed samples:      1514496 | elapsed time per iteration (ms): 111861.0 | rate (tokens/sec): 18747.84 | learning rate: 2.218E-04 | global batch size:  1024 | lm loss: 1.732916E+00 | loss scale: 1.0 | grad norm: 1.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1480/   14369 | consumed samples:      1515520 | elapsed time per iteration (ms): 111880.9 | rate (tokens/sec): 18744.51 | learning rate: 2.220E-04 | global batch size:  1024 | lm loss: 1.713537E+00 | loss scale: 1.0 | grad norm: 1.457 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1481/   14369 | consumed samples:      1516544 | elapsed time per iteration (ms): 112040.7 | rate (tokens/sec): 18717.77 | learning rate: 2.221E-04 | global batch size:  1024 | lm loss: 1.702027E+00 | loss scale: 1.0 | grad norm: 1.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1482/   14369 | consumed samples:      1517568 | elapsed time per iteration (ms): 111841.7 | rate (tokens/sec): 18751.08 | learning rate: 2.223E-04 | global batch size:  1024 | lm loss: 1.722352E+00 | loss scale: 1.0 | grad norm: 1.583 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1483/   14369 | consumed samples:      1518592 | elapsed time per iteration (ms): 113112.1 | rate (tokens/sec): 18540.48 | learning rate: 2.224E-04 | global batch size:  1024 | lm loss: 1.696080E+00 | loss scale: 1.0 | grad norm: 1.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1484/   14369 | consumed samples:      1519616 | elapsed time per iteration (ms): 113476.9 | rate (tokens/sec): 18480.87 | learning rate: 2.226E-04 | global batch size:  1024 | lm loss: 1.715071E+00 | loss scale: 1.0 | grad norm: 1.771 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1485/   14369 | consumed samples:      1520640 | elapsed time per iteration (ms): 113277.7 | rate (tokens/sec): 18513.37 | learning rate: 2.227E-04 | global batch size:  1024 | lm loss: 1.704600E+00 | loss scale: 1.0 | grad norm: 1.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1486/   14369 | consumed samples:      1521664 | elapsed time per iteration (ms): 112604.0 | rate (tokens/sec): 18624.13 | learning rate: 2.229E-04 | global batch size:  1024 | lm loss: 1.717497E+00 | loss scale: 1.0 | grad norm: 1.366 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1487/   14369 | consumed samples:      1522688 | elapsed time per iteration (ms): 113381.9 | rate (tokens/sec): 18496.36 | learning rate: 2.230E-04 | global batch size:  1024 | lm loss: 1.708618E+00 | loss scale: 1.0 | grad norm: 1.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1488/   14369 | consumed samples:      1523712 | elapsed time per iteration (ms): 112572.3 | rate (tokens/sec): 18629.38 | learning rate: 2.232E-04 | global batch size:  1024 | lm loss: 1.730702E+00 | loss scale: 1.0 | grad norm: 1.616 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1489/   14369 | consumed samples:      1524736 | elapsed time per iteration (ms): 112479.1 | rate (tokens/sec): 18644.82 | learning rate: 2.233E-04 | global batch size:  1024 | lm loss: 1.703120E+00 | loss scale: 1.0 | grad norm: 1.424 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1490/   14369 | consumed samples:      1525760 | elapsed time per iteration (ms): 112594.7 | rate (tokens/sec): 18625.67 | learning rate: 2.235E-04 | global batch size:  1024 | lm loss: 1.696716E+00 | loss scale: 1.0 | grad norm: 1.633 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1491/   14369 | consumed samples:      1526784 | elapsed time per iteration (ms): 111848.5 | rate (tokens/sec): 18749.94 | learning rate: 2.236E-04 | global batch size:  1024 | lm loss: 1.711892E+00 | loss scale: 1.0 | grad norm: 1.094 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1492/   14369 | consumed samples:      1527808 | elapsed time per iteration (ms): 112312.3 | rate (tokens/sec): 18672.51 | learning rate: 2.238E-04 | global batch size:  1024 | lm loss: 1.687629E+00 | loss scale: 1.0 | grad norm: 1.794 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1493/   14369 | consumed samples:      1528832 | elapsed time per iteration (ms): 112442.7 | rate (tokens/sec): 18650.84 | learning rate: 2.239E-04 | global batch size:  1024 | lm loss: 1.695172E+00 | loss scale: 1.0 | grad norm: 0.920 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1494/   14369 | consumed samples:      1529856 | elapsed time per iteration (ms): 112056.8 | rate (tokens/sec): 18715.08 | learning rate: 2.241E-04 | global batch size:  1024 | lm loss: 1.680836E+00 | loss scale: 1.0 | grad norm: 1.427 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1495/   14369 | consumed samples:      1530880 | elapsed time per iteration (ms): 111821.7 | rate (tokens/sec): 18754.43 | learning rate: 2.242E-04 | global batch size:  1024 | lm loss: 1.703612E+00 | loss scale: 1.0 | grad norm: 1.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1496/   14369 | consumed samples:      1531904 | elapsed time per iteration (ms): 112559.9 | rate (tokens/sec): 18631.43 | learning rate: 2.244E-04 | global batch size:  1024 | lm loss: 1.720670E+00 | loss scale: 1.0 | grad norm: 1.363 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1497/   14369 | consumed samples:      1532928 | elapsed time per iteration (ms): 112801.7 | rate (tokens/sec): 18591.49 | learning rate: 2.245E-04 | global batch size:  1024 | lm loss: 1.706590E+00 | loss scale: 1.0 | grad norm: 1.328 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1498/   14369 | consumed samples:      1533952 | elapsed time per iteration (ms): 112520.0 | rate (tokens/sec): 18638.03 | learning rate: 2.247E-04 | global batch size:  1024 | lm loss: 1.711916E+00 | loss scale: 1.0 | grad norm: 1.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1499/   14369 | consumed samples:      1534976 | elapsed time per iteration (ms): 112639.9 | rate (tokens/sec): 18618.20 | learning rate: 2.248E-04 | global batch size:  1024 | lm loss: 1.722426E+00 | loss scale: 1.0 | grad norm: 1.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1500/   14369 | consumed samples:      1536000 | elapsed time per iteration (ms): 111766.9 | rate (tokens/sec): 18763.63 | learning rate: 2.250E-04 | global batch size:  1024 | lm loss: 1.714390E+00 | loss scale: 1.0 | grad norm: 1.581 | number of skipped iterations:   0 | number of nan iterations:   0 |
------------------------------------------------------------------------------------------------
 validation loss at iteration 1500 | lm loss value: 1.852696E+00 | lm loss PPL: 6.376988E+00 | 
------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------
 test loss at iteration 1500 | lm loss value: 2.927793E+00 | lm loss PPL: 1.868634E+01 | 
------------------------------------------------------------------------------------------
saving checkpoint at iteration    1500 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
  successfully saved checkpoint at iteration    1500 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
(min, max) time across ranks (ms):
    save-checkpoint ................................: (115892.17, 115911.97)
 iteration     1501/   14369 | consumed samples:      1537024 | elapsed time per iteration (ms): 1212471.2 | rate (tokens/sec): 1729.65 | learning rate: 2.251E-04 | global batch size:  1024 | lm loss: 1.710801E+00 | loss scale: 1.0 | grad norm: 1.603 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1502/   14369 | consumed samples:      1538048 | elapsed time per iteration (ms): 118495.9 | rate (tokens/sec): 17698.10 | learning rate: 2.253E-04 | global batch size:  1024 | lm loss: 1.723076E+00 | loss scale: 1.0 | grad norm: 1.149 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1503/   14369 | consumed samples:      1539072 | elapsed time per iteration (ms): 114419.8 | rate (tokens/sec): 18328.57 | learning rate: 2.254E-04 | global batch size:  1024 | lm loss: 1.705454E+00 | loss scale: 1.0 | grad norm: 1.589 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1504/   14369 | consumed samples:      1540096 | elapsed time per iteration (ms): 115112.7 | rate (tokens/sec): 18218.24 | learning rate: 2.256E-04 | global batch size:  1024 | lm loss: 1.718202E+00 | loss scale: 1.0 | grad norm: 1.459 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1505/   14369 | consumed samples:      1541120 | elapsed time per iteration (ms): 113379.9 | rate (tokens/sec): 18496.69 | learning rate: 2.257E-04 | global batch size:  1024 | lm loss: 1.701916E+00 | loss scale: 1.0 | grad norm: 1.451 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1506/   14369 | consumed samples:      1542144 | elapsed time per iteration (ms): 114061.7 | rate (tokens/sec): 18386.13 | learning rate: 2.259E-04 | global batch size:  1024 | lm loss: 1.720748E+00 | loss scale: 1.0 | grad norm: 1.480 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1507/   14369 | consumed samples:      1543168 | elapsed time per iteration (ms): 113205.9 | rate (tokens/sec): 18525.12 | learning rate: 2.260E-04 | global batch size:  1024 | lm loss: 1.690382E+00 | loss scale: 1.0 | grad norm: 1.371 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1508/   14369 | consumed samples:      1544192 | elapsed time per iteration (ms): 112767.2 | rate (tokens/sec): 18597.18 | learning rate: 2.262E-04 | global batch size:  1024 | lm loss: 1.700682E+00 | loss scale: 1.0 | grad norm: 1.617 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1509/   14369 | consumed samples:      1545216 | elapsed time per iteration (ms): 114420.7 | rate (tokens/sec): 18328.43 | learning rate: 2.263E-04 | global batch size:  1024 | lm loss: 1.697721E+00 | loss scale: 1.0 | grad norm: 1.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1510/   14369 | consumed samples:      1546240 | elapsed time per iteration (ms): 113463.0 | rate (tokens/sec): 18483.13 | learning rate: 2.265E-04 | global batch size:  1024 | lm loss: 1.718910E+00 | loss scale: 1.0 | grad norm: 1.343 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1511/   14369 | consumed samples:      1547264 | elapsed time per iteration (ms): 112616.9 | rate (tokens/sec): 18622.01 | learning rate: 2.266E-04 | global batch size:  1024 | lm loss: 1.687049E+00 | loss scale: 1.0 | grad norm: 1.436 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1512/   14369 | consumed samples:      1548288 | elapsed time per iteration (ms): 112041.9 | rate (tokens/sec): 18717.57 | learning rate: 2.268E-04 | global batch size:  1024 | lm loss: 1.686827E+00 | loss scale: 1.0 | grad norm: 1.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1513/   14369 | consumed samples:      1549312 | elapsed time per iteration (ms): 113046.9 | rate (tokens/sec): 18551.17 | learning rate: 2.269E-04 | global batch size:  1024 | lm loss: 1.713602E+00 | loss scale: 1.0 | grad norm: 1.561 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1514/   14369 | consumed samples:      1550336 | elapsed time per iteration (ms): 112587.5 | rate (tokens/sec): 18626.86 | learning rate: 2.271E-04 | global batch size:  1024 | lm loss: 1.716485E+00 | loss scale: 1.0 | grad norm: 1.272 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1515/   14369 | consumed samples:      1551360 | elapsed time per iteration (ms): 113203.0 | rate (tokens/sec): 18525.58 | learning rate: 2.272E-04 | global batch size:  1024 | lm loss: 1.683576E+00 | loss scale: 1.0 | grad norm: 1.588 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1516/   14369 | consumed samples:      1552384 | elapsed time per iteration (ms): 111661.3 | rate (tokens/sec): 18781.37 | learning rate: 2.274E-04 | global batch size:  1024 | lm loss: 1.720749E+00 | loss scale: 1.0 | grad norm: 1.062 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1517/   14369 | consumed samples:      1553408 | elapsed time per iteration (ms): 112020.0 | rate (tokens/sec): 18721.22 | learning rate: 2.275E-04 | global batch size:  1024 | lm loss: 1.712018E+00 | loss scale: 1.0 | grad norm: 1.820 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1518/   14369 | consumed samples:      1554432 | elapsed time per iteration (ms): 111663.9 | rate (tokens/sec): 18780.93 | learning rate: 2.277E-04 | global batch size:  1024 | lm loss: 1.701038E+00 | loss scale: 1.0 | grad norm: 0.993 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1519/   14369 | consumed samples:      1555456 | elapsed time per iteration (ms): 112273.0 | rate (tokens/sec): 18679.04 | learning rate: 2.278E-04 | global batch size:  1024 | lm loss: 1.705933E+00 | loss scale: 1.0 | grad norm: 1.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1520/   14369 | consumed samples:      1556480 | elapsed time per iteration (ms): 111481.8 | rate (tokens/sec): 18811.62 | learning rate: 2.280E-04 | global batch size:  1024 | lm loss: 1.694519E+00 | loss scale: 1.0 | grad norm: 1.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1521/   14369 | consumed samples:      1557504 | elapsed time per iteration (ms): 111062.8 | rate (tokens/sec): 18882.58 | learning rate: 2.281E-04 | global batch size:  1024 | lm loss: 1.720297E+00 | loss scale: 1.0 | grad norm: 1.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1522/   14369 | consumed samples:      1558528 | elapsed time per iteration (ms): 111856.0 | rate (tokens/sec): 18748.68 | learning rate: 2.283E-04 | global batch size:  1024 | lm loss: 1.690838E+00 | loss scale: 1.0 | grad norm: 1.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1523/   14369 | consumed samples:      1559552 | elapsed time per iteration (ms): 112934.7 | rate (tokens/sec): 18569.60 | learning rate: 2.284E-04 | global batch size:  1024 | lm loss: 1.692882E+00 | loss scale: 1.0 | grad norm: 1.355 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1524/   14369 | consumed samples:      1560576 | elapsed time per iteration (ms): 113781.7 | rate (tokens/sec): 18431.36 | learning rate: 2.286E-04 | global batch size:  1024 | lm loss: 1.704243E+00 | loss scale: 1.0 | grad norm: 1.771 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1525/   14369 | consumed samples:      1561600 | elapsed time per iteration (ms): 113856.9 | rate (tokens/sec): 18419.20 | learning rate: 2.287E-04 | global batch size:  1024 | lm loss: 1.689294E+00 | loss scale: 1.0 | grad norm: 1.206 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1526/   14369 | consumed samples:      1562624 | elapsed time per iteration (ms): 113141.7 | rate (tokens/sec): 18535.63 | learning rate: 2.289E-04 | global batch size:  1024 | lm loss: 1.711790E+00 | loss scale: 1.0 | grad norm: 1.687 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1527/   14369 | consumed samples:      1563648 | elapsed time per iteration (ms): 112114.1 | rate (tokens/sec): 18705.52 | learning rate: 2.290E-04 | global batch size:  1024 | lm loss: 1.680030E+00 | loss scale: 1.0 | grad norm: 1.132 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1528/   14369 | consumed samples:      1564672 | elapsed time per iteration (ms): 112215.8 | rate (tokens/sec): 18688.57 | learning rate: 2.292E-04 | global batch size:  1024 | lm loss: 1.689130E+00 | loss scale: 1.0 | grad norm: 1.617 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1529/   14369 | consumed samples:      1565696 | elapsed time per iteration (ms): 112078.9 | rate (tokens/sec): 18711.39 | learning rate: 2.293E-04 | global batch size:  1024 | lm loss: 1.691975E+00 | loss scale: 1.0 | grad norm: 1.261 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1530/   14369 | consumed samples:      1566720 | elapsed time per iteration (ms): 117566.9 | rate (tokens/sec): 17837.95 | learning rate: 2.295E-04 | global batch size:  1024 | lm loss: 1.692399E+00 | loss scale: 1.0 | grad norm: 1.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1531/   14369 | consumed samples:      1567744 | elapsed time per iteration (ms): 116151.4 | rate (tokens/sec): 18055.33 | learning rate: 2.296E-04 | global batch size:  1024 | lm loss: 1.679769E+00 | loss scale: 1.0 | grad norm: 1.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1532/   14369 | consumed samples:      1568768 | elapsed time per iteration (ms): 124500.9 | rate (tokens/sec): 16844.47 | learning rate: 2.298E-04 | global batch size:  1024 | lm loss: 1.706076E+00 | loss scale: 1.0 | grad norm: 1.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1533/   14369 | consumed samples:      1569792 | elapsed time per iteration (ms): 119919.7 | rate (tokens/sec): 17487.97 | learning rate: 2.299E-04 | global batch size:  1024 | lm loss: 1.716551E+00 | loss scale: 1.0 | grad norm: 1.418 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1534/   14369 | consumed samples:      1570816 | elapsed time per iteration (ms): 122219.0 | rate (tokens/sec): 17158.97 | learning rate: 2.301E-04 | global batch size:  1024 | lm loss: 1.706305E+00 | loss scale: 1.0 | grad norm: 1.408 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1535/   14369 | consumed samples:      1571840 | elapsed time per iteration (ms): 119672.9 | rate (tokens/sec): 17524.04 | learning rate: 2.302E-04 | global batch size:  1024 | lm loss: 1.696787E+00 | loss scale: 1.0 | grad norm: 1.377 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1536/   14369 | consumed samples:      1572864 | elapsed time per iteration (ms): 124500.0 | rate (tokens/sec): 16844.60 | learning rate: 2.304E-04 | global batch size:  1024 | lm loss: 1.677096E+00 | loss scale: 1.0 | grad norm: 1.204 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1537/   14369 | consumed samples:      1573888 | elapsed time per iteration (ms): 117999.3 | rate (tokens/sec): 17772.58 | learning rate: 2.305E-04 | global batch size:  1024 | lm loss: 1.687279E+00 | loss scale: 1.0 | grad norm: 1.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1538/   14369 | consumed samples:      1574912 | elapsed time per iteration (ms): 116935.3 | rate (tokens/sec): 17934.30 | learning rate: 2.307E-04 | global batch size:  1024 | lm loss: 1.690961E+00 | loss scale: 1.0 | grad norm: 1.287 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1539/   14369 | consumed samples:      1575936 | elapsed time per iteration (ms): 115159.9 | rate (tokens/sec): 18210.79 | learning rate: 2.308E-04 | global batch size:  1024 | lm loss: 1.684788E+00 | loss scale: 1.0 | grad norm: 1.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1540/   14369 | consumed samples:      1576960 | elapsed time per iteration (ms): 114144.9 | rate (tokens/sec): 18372.72 | learning rate: 2.310E-04 | global batch size:  1024 | lm loss: 1.689228E+00 | loss scale: 1.0 | grad norm: 1.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1541/   14369 | consumed samples:      1577984 | elapsed time per iteration (ms): 112120.0 | rate (tokens/sec): 18704.53 | learning rate: 2.311E-04 | global batch size:  1024 | lm loss: 1.691959E+00 | loss scale: 1.0 | grad norm: 1.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1542/   14369 | consumed samples:      1579008 | elapsed time per iteration (ms): 112998.9 | rate (tokens/sec): 18559.05 | learning rate: 2.313E-04 | global batch size:  1024 | lm loss: 1.695987E+00 | loss scale: 1.0 | grad norm: 1.680 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1543/   14369 | consumed samples:      1580032 | elapsed time per iteration (ms): 114194.8 | rate (tokens/sec): 18364.69 | learning rate: 2.314E-04 | global batch size:  1024 | lm loss: 1.677784E+00 | loss scale: 1.0 | grad norm: 1.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1544/   14369 | consumed samples:      1581056 | elapsed time per iteration (ms): 114759.9 | rate (tokens/sec): 18274.26 | learning rate: 2.316E-04 | global batch size:  1024 | lm loss: 1.677672E+00 | loss scale: 1.0 | grad norm: 1.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1545/   14369 | consumed samples:      1582080 | elapsed time per iteration (ms): 113227.6 | rate (tokens/sec): 18521.56 | learning rate: 2.317E-04 | global batch size:  1024 | lm loss: 1.697183E+00 | loss scale: 1.0 | grad norm: 1.430 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1546/   14369 | consumed samples:      1583104 | elapsed time per iteration (ms): 112680.0 | rate (tokens/sec): 18611.57 | learning rate: 2.319E-04 | global batch size:  1024 | lm loss: 1.682421E+00 | loss scale: 1.0 | grad norm: 1.357 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1547/   14369 | consumed samples:      1584128 | elapsed time per iteration (ms): 111735.7 | rate (tokens/sec): 18768.87 | learning rate: 2.320E-04 | global batch size:  1024 | lm loss: 1.706270E+00 | loss scale: 1.0 | grad norm: 1.263 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1548/   14369 | consumed samples:      1585152 | elapsed time per iteration (ms): 112016.3 | rate (tokens/sec): 18721.84 | learning rate: 2.322E-04 | global batch size:  1024 | lm loss: 1.683823E+00 | loss scale: 1.0 | grad norm: 1.640 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1549/   14369 | consumed samples:      1586176 | elapsed time per iteration (ms): 113281.7 | rate (tokens/sec): 18512.72 | learning rate: 2.323E-04 | global batch size:  1024 | lm loss: 1.690404E+00 | loss scale: 1.0 | grad norm: 1.125 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1550/   14369 | consumed samples:      1587200 | elapsed time per iteration (ms): 113167.0 | rate (tokens/sec): 18531.48 | learning rate: 2.325E-04 | global batch size:  1024 | lm loss: 1.720193E+00 | loss scale: 1.0 | grad norm: 1.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1551/   14369 | consumed samples:      1588224 | elapsed time per iteration (ms): 112320.0 | rate (tokens/sec): 18671.22 | learning rate: 2.326E-04 | global batch size:  1024 | lm loss: 1.681118E+00 | loss scale: 1.0 | grad norm: 0.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1552/   14369 | consumed samples:      1589248 | elapsed time per iteration (ms): 112341.7 | rate (tokens/sec): 18667.62 | learning rate: 2.328E-04 | global batch size:  1024 | lm loss: 1.686388E+00 | loss scale: 1.0 | grad norm: 1.514 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1553/   14369 | consumed samples:      1590272 | elapsed time per iteration (ms): 112539.1 | rate (tokens/sec): 18634.88 | learning rate: 2.329E-04 | global batch size:  1024 | lm loss: 1.684737E+00 | loss scale: 1.0 | grad norm: 1.099 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1554/   14369 | consumed samples:      1591296 | elapsed time per iteration (ms): 111720.4 | rate (tokens/sec): 18771.43 | learning rate: 2.331E-04 | global batch size:  1024 | lm loss: 1.721001E+00 | loss scale: 1.0 | grad norm: 2.108 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1555/   14369 | consumed samples:      1592320 | elapsed time per iteration (ms): 111318.4 | rate (tokens/sec): 18839.23 | learning rate: 2.332E-04 | global batch size:  1024 | lm loss: 1.711411E+00 | loss scale: 1.0 | grad norm: 1.208 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1556/   14369 | consumed samples:      1593344 | elapsed time per iteration (ms): 111868.7 | rate (tokens/sec): 18746.54 | learning rate: 2.334E-04 | global batch size:  1024 | lm loss: 1.694595E+00 | loss scale: 1.0 | grad norm: 1.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1557/   14369 | consumed samples:      1594368 | elapsed time per iteration (ms): 111268.6 | rate (tokens/sec): 18847.65 | learning rate: 2.335E-04 | global batch size:  1024 | lm loss: 1.694921E+00 | loss scale: 1.0 | grad norm: 1.217 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1558/   14369 | consumed samples:      1595392 | elapsed time per iteration (ms): 112295.4 | rate (tokens/sec): 18675.32 | learning rate: 2.337E-04 | global batch size:  1024 | lm loss: 1.706418E+00 | loss scale: 1.0 | grad norm: 1.239 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1559/   14369 | consumed samples:      1596416 | elapsed time per iteration (ms): 111659.7 | rate (tokens/sec): 18781.64 | learning rate: 2.338E-04 | global batch size:  1024 | lm loss: 1.730654E+00 | loss scale: 1.0 | grad norm: 1.433 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1560/   14369 | consumed samples:      1597440 | elapsed time per iteration (ms): 112096.3 | rate (tokens/sec): 18708.49 | learning rate: 2.340E-04 | global batch size:  1024 | lm loss: 1.694008E+00 | loss scale: 1.0 | grad norm: 1.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1561/   14369 | consumed samples:      1598464 | elapsed time per iteration (ms): 110677.0 | rate (tokens/sec): 18948.40 | learning rate: 2.341E-04 | global batch size:  1024 | lm loss: 1.705754E+00 | loss scale: 1.0 | grad norm: 1.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1562/   14369 | consumed samples:      1599488 | elapsed time per iteration (ms): 111495.9 | rate (tokens/sec): 18809.23 | learning rate: 2.343E-04 | global batch size:  1024 | lm loss: 1.707732E+00 | loss scale: 1.0 | grad norm: 1.313 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1563/   14369 | consumed samples:      1600512 | elapsed time per iteration (ms): 112224.9 | rate (tokens/sec): 18687.05 | learning rate: 2.344E-04 | global batch size:  1024 | lm loss: 1.695634E+00 | loss scale: 1.0 | grad norm: 1.853 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1564/   14369 | consumed samples:      1601536 | elapsed time per iteration (ms): 111744.3 | rate (tokens/sec): 18767.42 | learning rate: 2.346E-04 | global batch size:  1024 | lm loss: 1.700924E+00 | loss scale: 1.0 | grad norm: 0.823 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1565/   14369 | consumed samples:      1602560 | elapsed time per iteration (ms): 111901.0 | rate (tokens/sec): 18741.13 | learning rate: 2.347E-04 | global batch size:  1024 | lm loss: 1.693745E+00 | loss scale: 1.0 | grad norm: 1.592 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1566/   14369 | consumed samples:      1603584 | elapsed time per iteration (ms): 111778.9 | rate (tokens/sec): 18761.61 | learning rate: 2.349E-04 | global batch size:  1024 | lm loss: 1.697784E+00 | loss scale: 1.0 | grad norm: 1.242 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1567/   14369 | consumed samples:      1604608 | elapsed time per iteration (ms): 112201.7 | rate (tokens/sec): 18690.91 | learning rate: 2.350E-04 | global batch size:  1024 | lm loss: 1.708140E+00 | loss scale: 1.0 | grad norm: 1.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1568/   14369 | consumed samples:      1605632 | elapsed time per iteration (ms): 111894.8 | rate (tokens/sec): 18742.17 | learning rate: 2.352E-04 | global batch size:  1024 | lm loss: 1.684261E+00 | loss scale: 1.0 | grad norm: 1.213 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1569/   14369 | consumed samples:      1606656 | elapsed time per iteration (ms): 111024.9 | rate (tokens/sec): 18889.03 | learning rate: 2.353E-04 | global batch size:  1024 | lm loss: 1.710611E+00 | loss scale: 1.0 | grad norm: 1.271 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1570/   14369 | consumed samples:      1607680 | elapsed time per iteration (ms): 111996.7 | rate (tokens/sec): 18725.12 | learning rate: 2.355E-04 | global batch size:  1024 | lm loss: 1.689256E+00 | loss scale: 1.0 | grad norm: 1.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1571/   14369 | consumed samples:      1608704 | elapsed time per iteration (ms): 115814.1 | rate (tokens/sec): 18107.92 | learning rate: 2.356E-04 | global batch size:  1024 | lm loss: 1.698377E+00 | loss scale: 1.0 | grad norm: 1.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1572/   14369 | consumed samples:      1609728 | elapsed time per iteration (ms): 111815.5 | rate (tokens/sec): 18755.47 | learning rate: 2.358E-04 | global batch size:  1024 | lm loss: 1.702830E+00 | loss scale: 1.0 | grad norm: 1.262 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1573/   14369 | consumed samples:      1610752 | elapsed time per iteration (ms): 112046.9 | rate (tokens/sec): 18716.74 | learning rate: 2.359E-04 | global batch size:  1024 | lm loss: 1.719501E+00 | loss scale: 1.0 | grad norm: 1.655 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1574/   14369 | consumed samples:      1611776 | elapsed time per iteration (ms): 111533.6 | rate (tokens/sec): 18802.86 | learning rate: 2.361E-04 | global batch size:  1024 | lm loss: 1.709005E+00 | loss scale: 1.0 | grad norm: 1.177 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1575/   14369 | consumed samples:      1612800 | elapsed time per iteration (ms): 111706.9 | rate (tokens/sec): 18773.70 | learning rate: 2.362E-04 | global batch size:  1024 | lm loss: 1.725213E+00 | loss scale: 1.0 | grad norm: 1.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1576/   14369 | consumed samples:      1613824 | elapsed time per iteration (ms): 110246.9 | rate (tokens/sec): 19022.32 | learning rate: 2.364E-04 | global batch size:  1024 | lm loss: 1.713362E+00 | loss scale: 1.0 | grad norm: 1.521 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1577/   14369 | consumed samples:      1614848 | elapsed time per iteration (ms): 111657.0 | rate (tokens/sec): 18782.10 | learning rate: 2.365E-04 | global batch size:  1024 | lm loss: 1.700088E+00 | loss scale: 1.0 | grad norm: 1.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1578/   14369 | consumed samples:      1615872 | elapsed time per iteration (ms): 112261.7 | rate (tokens/sec): 18680.92 | learning rate: 2.367E-04 | global batch size:  1024 | lm loss: 1.691700E+00 | loss scale: 1.0 | grad norm: 1.612 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1579/   14369 | consumed samples:      1616896 | elapsed time per iteration (ms): 111841.9 | rate (tokens/sec): 18751.05 | learning rate: 2.368E-04 | global batch size:  1024 | lm loss: 1.687438E+00 | loss scale: 1.0 | grad norm: 1.308 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1580/   14369 | consumed samples:      1617920 | elapsed time per iteration (ms): 111923.9 | rate (tokens/sec): 18737.31 | learning rate: 2.370E-04 | global batch size:  1024 | lm loss: 1.689234E+00 | loss scale: 1.0 | grad norm: 1.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1581/   14369 | consumed samples:      1618944 | elapsed time per iteration (ms): 110055.7 | rate (tokens/sec): 19055.36 | learning rate: 2.371E-04 | global batch size:  1024 | lm loss: 1.679022E+00 | loss scale: 1.0 | grad norm: 1.118 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1582/   14369 | consumed samples:      1619968 | elapsed time per iteration (ms): 111874.9 | rate (tokens/sec): 18745.50 | learning rate: 2.373E-04 | global batch size:  1024 | lm loss: 1.706316E+00 | loss scale: 1.0 | grad norm: 1.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1583/   14369 | consumed samples:      1620992 | elapsed time per iteration (ms): 111939.9 | rate (tokens/sec): 18734.62 | learning rate: 2.374E-04 | global batch size:  1024 | lm loss: 1.686302E+00 | loss scale: 1.0 | grad norm: 1.132 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1584/   14369 | consumed samples:      1622016 | elapsed time per iteration (ms): 111479.0 | rate (tokens/sec): 18812.08 | learning rate: 2.376E-04 | global batch size:  1024 | lm loss: 1.693875E+00 | loss scale: 1.0 | grad norm: 1.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1585/   14369 | consumed samples:      1623040 | elapsed time per iteration (ms): 112078.0 | rate (tokens/sec): 18711.54 | learning rate: 2.377E-04 | global batch size:  1024 | lm loss: 1.682450E+00 | loss scale: 1.0 | grad norm: 1.325 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1586/   14369 | consumed samples:      1624064 | elapsed time per iteration (ms): 111420.0 | rate (tokens/sec): 18822.04 | learning rate: 2.379E-04 | global batch size:  1024 | lm loss: 1.678070E+00 | loss scale: 1.0 | grad norm: 1.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1587/   14369 | consumed samples:      1625088 | elapsed time per iteration (ms): 111972.7 | rate (tokens/sec): 18729.14 | learning rate: 2.380E-04 | global batch size:  1024 | lm loss: 1.688336E+00 | loss scale: 1.0 | grad norm: 1.402 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1588/   14369 | consumed samples:      1626112 | elapsed time per iteration (ms): 112088.7 | rate (tokens/sec): 18709.75 | learning rate: 2.382E-04 | global batch size:  1024 | lm loss: 1.692132E+00 | loss scale: 1.0 | grad norm: 1.485 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1589/   14369 | consumed samples:      1627136 | elapsed time per iteration (ms): 111933.0 | rate (tokens/sec): 18735.78 | learning rate: 2.383E-04 | global batch size:  1024 | lm loss: 1.685513E+00 | loss scale: 1.0 | grad norm: 1.279 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1590/   14369 | consumed samples:      1628160 | elapsed time per iteration (ms): 111679.9 | rate (tokens/sec): 18778.24 | learning rate: 2.385E-04 | global batch size:  1024 | lm loss: 1.685792E+00 | loss scale: 1.0 | grad norm: 1.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1591/   14369 | consumed samples:      1629184 | elapsed time per iteration (ms): 111321.7 | rate (tokens/sec): 18838.67 | learning rate: 2.386E-04 | global batch size:  1024 | lm loss: 1.688260E+00 | loss scale: 1.0 | grad norm: 1.357 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1592/   14369 | consumed samples:      1630208 | elapsed time per iteration (ms): 112160.0 | rate (tokens/sec): 18697.86 | learning rate: 2.388E-04 | global batch size:  1024 | lm loss: 1.703367E+00 | loss scale: 1.0 | grad norm: 1.568 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1593/   14369 | consumed samples:      1631232 | elapsed time per iteration (ms): 111881.7 | rate (tokens/sec): 18744.38 | learning rate: 2.389E-04 | global batch size:  1024 | lm loss: 1.686770E+00 | loss scale: 1.0 | grad norm: 0.922 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1594/   14369 | consumed samples:      1632256 | elapsed time per iteration (ms): 112622.8 | rate (tokens/sec): 18621.02 | learning rate: 2.391E-04 | global batch size:  1024 | lm loss: 1.675486E+00 | loss scale: 1.0 | grad norm: 1.490 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1595/   14369 | consumed samples:      1633280 | elapsed time per iteration (ms): 111224.8 | rate (tokens/sec): 18855.08 | learning rate: 2.392E-04 | global batch size:  1024 | lm loss: 1.680873E+00 | loss scale: 1.0 | grad norm: 1.258 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1596/   14369 | consumed samples:      1634304 | elapsed time per iteration (ms): 111044.6 | rate (tokens/sec): 18885.68 | learning rate: 2.394E-04 | global batch size:  1024 | lm loss: 1.720658E+00 | loss scale: 1.0 | grad norm: 1.237 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1597/   14369 | consumed samples:      1635328 | elapsed time per iteration (ms): 111420.0 | rate (tokens/sec): 18822.04 | learning rate: 2.395E-04 | global batch size:  1024 | lm loss: 1.676618E+00 | loss scale: 1.0 | grad norm: 1.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1598/   14369 | consumed samples:      1636352 | elapsed time per iteration (ms): 111740.8 | rate (tokens/sec): 18768.00 | learning rate: 2.397E-04 | global batch size:  1024 | lm loss: 1.704023E+00 | loss scale: 1.0 | grad norm: 1.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1599/   14369 | consumed samples:      1637376 | elapsed time per iteration (ms): 111599.9 | rate (tokens/sec): 18791.70 | learning rate: 2.398E-04 | global batch size:  1024 | lm loss: 1.685625E+00 | loss scale: 1.0 | grad norm: 1.534 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1600/   14369 | consumed samples:      1638400 | elapsed time per iteration (ms): 112121.7 | rate (tokens/sec): 18704.24 | learning rate: 2.400E-04 | global batch size:  1024 | lm loss: 1.714117E+00 | loss scale: 1.0 | grad norm: 1.194 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1601/   14369 | consumed samples:      1639424 | elapsed time per iteration (ms): 111939.9 | rate (tokens/sec): 18734.63 | learning rate: 2.401E-04 | global batch size:  1024 | lm loss: 1.675247E+00 | loss scale: 1.0 | grad norm: 1.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1602/   14369 | consumed samples:      1640448 | elapsed time per iteration (ms): 112327.9 | rate (tokens/sec): 18669.91 | learning rate: 2.403E-04 | global batch size:  1024 | lm loss: 1.699589E+00 | loss scale: 1.0 | grad norm: 1.190 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1603/   14369 | consumed samples:      1641472 | elapsed time per iteration (ms): 111534.8 | rate (tokens/sec): 18802.67 | learning rate: 2.404E-04 | global batch size:  1024 | lm loss: 1.705786E+00 | loss scale: 1.0 | grad norm: 1.402 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1604/   14369 | consumed samples:      1642496 | elapsed time per iteration (ms): 111546.8 | rate (tokens/sec): 18800.64 | learning rate: 2.406E-04 | global batch size:  1024 | lm loss: 1.696605E+00 | loss scale: 1.0 | grad norm: 1.112 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1605/   14369 | consumed samples:      1643520 | elapsed time per iteration (ms): 113623.1 | rate (tokens/sec): 18457.09 | learning rate: 2.407E-04 | global batch size:  1024 | lm loss: 1.697661E+00 | loss scale: 1.0 | grad norm: 1.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1606/   14369 | consumed samples:      1644544 | elapsed time per iteration (ms): 111457.9 | rate (tokens/sec): 18815.63 | learning rate: 2.409E-04 | global batch size:  1024 | lm loss: 1.698167E+00 | loss scale: 1.0 | grad norm: 1.294 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1607/   14369 | consumed samples:      1645568 | elapsed time per iteration (ms): 111343.7 | rate (tokens/sec): 18834.95 | learning rate: 2.410E-04 | global batch size:  1024 | lm loss: 1.693411E+00 | loss scale: 1.0 | grad norm: 1.200 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1608/   14369 | consumed samples:      1646592 | elapsed time per iteration (ms): 111527.0 | rate (tokens/sec): 18803.98 | learning rate: 2.412E-04 | global batch size:  1024 | lm loss: 1.696190E+00 | loss scale: 1.0 | grad norm: 1.257 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1609/   14369 | consumed samples:      1647616 | elapsed time per iteration (ms): 112162.5 | rate (tokens/sec): 18697.45 | learning rate: 2.413E-04 | global batch size:  1024 | lm loss: 1.677544E+00 | loss scale: 1.0 | grad norm: 1.180 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1610/   14369 | consumed samples:      1648640 | elapsed time per iteration (ms): 111455.0 | rate (tokens/sec): 18816.14 | learning rate: 2.415E-04 | global batch size:  1024 | lm loss: 1.669825E+00 | loss scale: 1.0 | grad norm: 1.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1611/   14369 | consumed samples:      1649664 | elapsed time per iteration (ms): 111430.9 | rate (tokens/sec): 18820.20 | learning rate: 2.416E-04 | global batch size:  1024 | lm loss: 1.697219E+00 | loss scale: 1.0 | grad norm: 1.131 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1612/   14369 | consumed samples:      1650688 | elapsed time per iteration (ms): 112321.7 | rate (tokens/sec): 18670.94 | learning rate: 2.418E-04 | global batch size:  1024 | lm loss: 1.698862E+00 | loss scale: 1.0 | grad norm: 1.873 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1613/   14369 | consumed samples:      1651712 | elapsed time per iteration (ms): 111646.9 | rate (tokens/sec): 18783.79 | learning rate: 2.419E-04 | global batch size:  1024 | lm loss: 1.704076E+00 | loss scale: 1.0 | grad norm: 1.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1614/   14369 | consumed samples:      1652736 | elapsed time per iteration (ms): 111363.8 | rate (tokens/sec): 18831.55 | learning rate: 2.421E-04 | global batch size:  1024 | lm loss: 1.685566E+00 | loss scale: 1.0 | grad norm: 1.237 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1615/   14369 | consumed samples:      1653760 | elapsed time per iteration (ms): 111162.7 | rate (tokens/sec): 18865.61 | learning rate: 2.422E-04 | global batch size:  1024 | lm loss: 1.676787E+00 | loss scale: 1.0 | grad norm: 1.076 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1616/   14369 | consumed samples:      1654784 | elapsed time per iteration (ms): 112086.9 | rate (tokens/sec): 18710.05 | learning rate: 2.424E-04 | global batch size:  1024 | lm loss: 1.694940E+00 | loss scale: 1.0 | grad norm: 1.631 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1617/   14369 | consumed samples:      1655808 | elapsed time per iteration (ms): 112464.8 | rate (tokens/sec): 18647.18 | learning rate: 2.425E-04 | global batch size:  1024 | lm loss: 1.690480E+00 | loss scale: 1.0 | grad norm: 1.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1618/   14369 | consumed samples:      1656832 | elapsed time per iteration (ms): 112199.9 | rate (tokens/sec): 18691.21 | learning rate: 2.427E-04 | global batch size:  1024 | lm loss: 1.684964E+00 | loss scale: 1.0 | grad norm: 1.062 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1619/   14369 | consumed samples:      1657856 | elapsed time per iteration (ms): 111739.9 | rate (tokens/sec): 18768.16 | learning rate: 2.428E-04 | global batch size:  1024 | lm loss: 1.692536E+00 | loss scale: 1.0 | grad norm: 1.561 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1620/   14369 | consumed samples:      1658880 | elapsed time per iteration (ms): 111883.6 | rate (tokens/sec): 18744.04 | learning rate: 2.430E-04 | global batch size:  1024 | lm loss: 1.693655E+00 | loss scale: 1.0 | grad norm: 1.086 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1621/   14369 | consumed samples:      1659904 | elapsed time per iteration (ms): 111800.9 | rate (tokens/sec): 18757.92 | learning rate: 2.431E-04 | global batch size:  1024 | lm loss: 1.708516E+00 | loss scale: 1.0 | grad norm: 1.740 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1622/   14369 | consumed samples:      1660928 | elapsed time per iteration (ms): 111980.0 | rate (tokens/sec): 18727.91 | learning rate: 2.433E-04 | global batch size:  1024 | lm loss: 1.683840E+00 | loss scale: 1.0 | grad norm: 1.085 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1623/   14369 | consumed samples:      1661952 | elapsed time per iteration (ms): 112203.7 | rate (tokens/sec): 18690.58 | learning rate: 2.434E-04 | global batch size:  1024 | lm loss: 1.682101E+00 | loss scale: 1.0 | grad norm: 1.709 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1624/   14369 | consumed samples:      1662976 | elapsed time per iteration (ms): 112714.8 | rate (tokens/sec): 18605.82 | learning rate: 2.436E-04 | global batch size:  1024 | lm loss: 1.673850E+00 | loss scale: 1.0 | grad norm: 0.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1625/   14369 | consumed samples:      1664000 | elapsed time per iteration (ms): 112326.0 | rate (tokens/sec): 18670.22 | learning rate: 2.437E-04 | global batch size:  1024 | lm loss: 1.683410E+00 | loss scale: 1.0 | grad norm: 1.534 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1626/   14369 | consumed samples:      1665024 | elapsed time per iteration (ms): 112625.3 | rate (tokens/sec): 18620.61 | learning rate: 2.439E-04 | global batch size:  1024 | lm loss: 1.687615E+00 | loss scale: 1.0 | grad norm: 1.254 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1627/   14369 | consumed samples:      1666048 | elapsed time per iteration (ms): 111314.8 | rate (tokens/sec): 18839.83 | learning rate: 2.440E-04 | global batch size:  1024 | lm loss: 1.685466E+00 | loss scale: 1.0 | grad norm: 1.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1628/   14369 | consumed samples:      1667072 | elapsed time per iteration (ms): 112240.8 | rate (tokens/sec): 18684.40 | learning rate: 2.442E-04 | global batch size:  1024 | lm loss: 1.661107E+00 | loss scale: 1.0 | grad norm: 1.146 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1629/   14369 | consumed samples:      1668096 | elapsed time per iteration (ms): 111779.7 | rate (tokens/sec): 18761.47 | learning rate: 2.444E-04 | global batch size:  1024 | lm loss: 1.685875E+00 | loss scale: 1.0 | grad norm: 1.461 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1630/   14369 | consumed samples:      1669120 | elapsed time per iteration (ms): 111258.7 | rate (tokens/sec): 18849.33 | learning rate: 2.445E-04 | global batch size:  1024 | lm loss: 1.692560E+00 | loss scale: 1.0 | grad norm: 1.582 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1631/   14369 | consumed samples:      1670144 | elapsed time per iteration (ms): 111947.0 | rate (tokens/sec): 18733.43 | learning rate: 2.446E-04 | global batch size:  1024 | lm loss: 1.677372E+00 | loss scale: 1.0 | grad norm: 1.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1632/   14369 | consumed samples:      1671168 | elapsed time per iteration (ms): 111713.2 | rate (tokens/sec): 18772.64 | learning rate: 2.448E-04 | global batch size:  1024 | lm loss: 1.700047E+00 | loss scale: 1.0 | grad norm: 1.456 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1633/   14369 | consumed samples:      1672192 | elapsed time per iteration (ms): 112260.0 | rate (tokens/sec): 18681.20 | learning rate: 2.449E-04 | global batch size:  1024 | lm loss: 1.675683E+00 | loss scale: 1.0 | grad norm: 1.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1634/   14369 | consumed samples:      1673216 | elapsed time per iteration (ms): 111239.9 | rate (tokens/sec): 18852.52 | learning rate: 2.451E-04 | global batch size:  1024 | lm loss: 1.683542E+00 | loss scale: 1.0 | grad norm: 1.402 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1635/   14369 | consumed samples:      1674240 | elapsed time per iteration (ms): 112122.7 | rate (tokens/sec): 18704.08 | learning rate: 2.452E-04 | global batch size:  1024 | lm loss: 1.704675E+00 | loss scale: 1.0 | grad norm: 1.343 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1636/   14369 | consumed samples:      1675264 | elapsed time per iteration (ms): 112665.5 | rate (tokens/sec): 18613.97 | learning rate: 2.454E-04 | global batch size:  1024 | lm loss: 1.704641E+00 | loss scale: 1.0 | grad norm: 1.371 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1637/   14369 | consumed samples:      1676288 | elapsed time per iteration (ms): 112041.7 | rate (tokens/sec): 18717.60 | learning rate: 2.455E-04 | global batch size:  1024 | lm loss: 1.696687E+00 | loss scale: 1.0 | grad norm: 1.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1638/   14369 | consumed samples:      1677312 | elapsed time per iteration (ms): 111513.0 | rate (tokens/sec): 18806.34 | learning rate: 2.457E-04 | global batch size:  1024 | lm loss: 1.673496E+00 | loss scale: 1.0 | grad norm: 1.200 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1639/   14369 | consumed samples:      1678336 | elapsed time per iteration (ms): 111619.9 | rate (tokens/sec): 18788.34 | learning rate: 2.458E-04 | global batch size:  1024 | lm loss: 1.678822E+00 | loss scale: 1.0 | grad norm: 1.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1640/   14369 | consumed samples:      1679360 | elapsed time per iteration (ms): 116022.0 | rate (tokens/sec): 18075.47 | learning rate: 2.460E-04 | global batch size:  1024 | lm loss: 1.685847E+00 | loss scale: 1.0 | grad norm: 1.235 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1641/   14369 | consumed samples:      1680384 | elapsed time per iteration (ms): 112722.0 | rate (tokens/sec): 18604.63 | learning rate: 2.461E-04 | global batch size:  1024 | lm loss: 1.668197E+00 | loss scale: 1.0 | grad norm: 1.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1642/   14369 | consumed samples:      1681408 | elapsed time per iteration (ms): 111897.9 | rate (tokens/sec): 18741.66 | learning rate: 2.463E-04 | global batch size:  1024 | lm loss: 1.687798E+00 | loss scale: 1.0 | grad norm: 1.199 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1643/   14369 | consumed samples:      1682432 | elapsed time per iteration (ms): 111719.7 | rate (tokens/sec): 18771.55 | learning rate: 2.465E-04 | global batch size:  1024 | lm loss: 1.712131E+00 | loss scale: 1.0 | grad norm: 1.414 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1644/   14369 | consumed samples:      1683456 | elapsed time per iteration (ms): 111246.9 | rate (tokens/sec): 18851.33 | learning rate: 2.466E-04 | global batch size:  1024 | lm loss: 1.684461E+00 | loss scale: 1.0 | grad norm: 1.225 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1645/   14369 | consumed samples:      1684480 | elapsed time per iteration (ms): 112030.4 | rate (tokens/sec): 18719.50 | learning rate: 2.467E-04 | global batch size:  1024 | lm loss: 1.672123E+00 | loss scale: 1.0 | grad norm: 1.367 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1646/   14369 | consumed samples:      1685504 | elapsed time per iteration (ms): 111237.7 | rate (tokens/sec): 18852.88 | learning rate: 2.469E-04 | global batch size:  1024 | lm loss: 1.711653E+00 | loss scale: 1.0 | grad norm: 1.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1647/   14369 | consumed samples:      1686528 | elapsed time per iteration (ms): 111621.8 | rate (tokens/sec): 18788.01 | learning rate: 2.471E-04 | global batch size:  1024 | lm loss: 1.714906E+00 | loss scale: 1.0 | grad norm: 1.026 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1648/   14369 | consumed samples:      1687552 | elapsed time per iteration (ms): 111862.4 | rate (tokens/sec): 18747.61 | learning rate: 2.472E-04 | global batch size:  1024 | lm loss: 1.687940E+00 | loss scale: 1.0 | grad norm: 1.387 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1649/   14369 | consumed samples:      1688576 | elapsed time per iteration (ms): 111658.0 | rate (tokens/sec): 18781.92 | learning rate: 2.473E-04 | global batch size:  1024 | lm loss: 1.671870E+00 | loss scale: 1.0 | grad norm: 1.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1650/   14369 | consumed samples:      1689600 | elapsed time per iteration (ms): 111762.9 | rate (tokens/sec): 18764.30 | learning rate: 2.475E-04 | global batch size:  1024 | lm loss: 1.671713E+00 | loss scale: 1.0 | grad norm: 1.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
------------------------------------------------------------------------------------------------
 validation loss at iteration 1650 | lm loss value: 1.833331E+00 | lm loss PPL: 6.254683E+00 | 
------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------
 test loss at iteration 1650 | lm loss value: 2.866418E+00 | lm loss PPL: 1.757396E+01 | 
------------------------------------------------------------------------------------------
saving checkpoint at iteration    1650 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
  successfully saved checkpoint at iteration    1650 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
(min, max) time across ranks (ms):
    save-checkpoint ................................: (115043.30, 115063.29)
 iteration     1651/   14369 | consumed samples:      1690624 | elapsed time per iteration (ms): 1214007.0 | rate (tokens/sec): 1727.46 | learning rate: 2.476E-04 | global batch size:  1024 | lm loss: 1.689368E+00 | loss scale: 1.0 | grad norm: 1.192 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1652/   14369 | consumed samples:      1691648 | elapsed time per iteration (ms): 111094.8 | rate (tokens/sec): 18877.15 | learning rate: 2.478E-04 | global batch size:  1024 | lm loss: 1.681494E+00 | loss scale: 1.0 | grad norm: 1.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1653/   14369 | consumed samples:      1692672 | elapsed time per iteration (ms): 111322.2 | rate (tokens/sec): 18838.58 | learning rate: 2.479E-04 | global batch size:  1024 | lm loss: 1.681118E+00 | loss scale: 1.0 | grad norm: 0.945 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1654/   14369 | consumed samples:      1693696 | elapsed time per iteration (ms): 110671.9 | rate (tokens/sec): 18949.27 | learning rate: 2.481E-04 | global batch size:  1024 | lm loss: 1.700234E+00 | loss scale: 1.0 | grad norm: 1.356 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1655/   14369 | consumed samples:      1694720 | elapsed time per iteration (ms): 111494.5 | rate (tokens/sec): 18809.47 | learning rate: 2.482E-04 | global batch size:  1024 | lm loss: 1.682695E+00 | loss scale: 1.0 | grad norm: 1.199 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1656/   14369 | consumed samples:      1695744 | elapsed time per iteration (ms): 111396.0 | rate (tokens/sec): 18826.09 | learning rate: 2.484E-04 | global batch size:  1024 | lm loss: 1.700964E+00 | loss scale: 1.0 | grad norm: 1.213 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1657/   14369 | consumed samples:      1696768 | elapsed time per iteration (ms): 112619.9 | rate (tokens/sec): 18621.51 | learning rate: 2.485E-04 | global batch size:  1024 | lm loss: 1.683944E+00 | loss scale: 1.0 | grad norm: 1.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1658/   14369 | consumed samples:      1697792 | elapsed time per iteration (ms): 112114.9 | rate (tokens/sec): 18705.39 | learning rate: 2.487E-04 | global batch size:  1024 | lm loss: 1.669791E+00 | loss scale: 1.0 | grad norm: 1.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1659/   14369 | consumed samples:      1698816 | elapsed time per iteration (ms): 110979.9 | rate (tokens/sec): 18896.68 | learning rate: 2.489E-04 | global batch size:  1024 | lm loss: 1.695720E+00 | loss scale: 1.0 | grad norm: 1.858 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1660/   14369 | consumed samples:      1699840 | elapsed time per iteration (ms): 112181.7 | rate (tokens/sec): 18694.24 | learning rate: 2.490E-04 | global batch size:  1024 | lm loss: 1.679750E+00 | loss scale: 1.0 | grad norm: 1.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1661/   14369 | consumed samples:      1700864 | elapsed time per iteration (ms): 111787.0 | rate (tokens/sec): 18760.24 | learning rate: 2.491E-04 | global batch size:  1024 | lm loss: 1.680957E+00 | loss scale: 1.0 | grad norm: 0.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1662/   14369 | consumed samples:      1701888 | elapsed time per iteration (ms): 111401.8 | rate (tokens/sec): 18825.12 | learning rate: 2.493E-04 | global batch size:  1024 | lm loss: 1.679924E+00 | loss scale: 1.0 | grad norm: 1.117 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1663/   14369 | consumed samples:      1702912 | elapsed time per iteration (ms): 111881.9 | rate (tokens/sec): 18744.34 | learning rate: 2.494E-04 | global batch size:  1024 | lm loss: 1.691289E+00 | loss scale: 1.0 | grad norm: 1.557 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1664/   14369 | consumed samples:      1703936 | elapsed time per iteration (ms): 111927.0 | rate (tokens/sec): 18736.78 | learning rate: 2.496E-04 | global batch size:  1024 | lm loss: 1.684060E+00 | loss scale: 1.0 | grad norm: 1.099 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1665/   14369 | consumed samples:      1704960 | elapsed time per iteration (ms): 114040.0 | rate (tokens/sec): 18389.61 | learning rate: 2.497E-04 | global batch size:  1024 | lm loss: 1.677201E+00 | loss scale: 1.0 | grad norm: 1.281 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1666/   14369 | consumed samples:      1705984 | elapsed time per iteration (ms): 112481.9 | rate (tokens/sec): 18644.35 | learning rate: 2.499E-04 | global batch size:  1024 | lm loss: 1.660660E+00 | loss scale: 1.0 | grad norm: 1.287 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1667/   14369 | consumed samples:      1707008 | elapsed time per iteration (ms): 111912.3 | rate (tokens/sec): 18739.24 | learning rate: 2.500E-04 | global batch size:  1024 | lm loss: 1.666916E+00 | loss scale: 1.0 | grad norm: 1.196 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1668/   14369 | consumed samples:      1708032 | elapsed time per iteration (ms): 111954.7 | rate (tokens/sec): 18732.15 | learning rate: 2.502E-04 | global batch size:  1024 | lm loss: 1.671863E+00 | loss scale: 1.0 | grad norm: 1.303 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1669/   14369 | consumed samples:      1709056 | elapsed time per iteration (ms): 111399.9 | rate (tokens/sec): 18825.44 | learning rate: 2.503E-04 | global batch size:  1024 | lm loss: 1.702349E+00 | loss scale: 1.0 | grad norm: 1.140 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1670/   14369 | consumed samples:      1710080 | elapsed time per iteration (ms): 112379.7 | rate (tokens/sec): 18661.31 | learning rate: 2.505E-04 | global batch size:  1024 | lm loss: 1.707624E+00 | loss scale: 1.0 | grad norm: 1.732 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1671/   14369 | consumed samples:      1711104 | elapsed time per iteration (ms): 111358.9 | rate (tokens/sec): 18832.37 | learning rate: 2.506E-04 | global batch size:  1024 | lm loss: 1.699797E+00 | loss scale: 1.0 | grad norm: 1.187 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1672/   14369 | consumed samples:      1712128 | elapsed time per iteration (ms): 111368.4 | rate (tokens/sec): 18830.76 | learning rate: 2.508E-04 | global batch size:  1024 | lm loss: 1.676635E+00 | loss scale: 1.0 | grad norm: 1.500 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1673/   14369 | consumed samples:      1713152 | elapsed time per iteration (ms): 110982.1 | rate (tokens/sec): 18896.31 | learning rate: 2.509E-04 | global batch size:  1024 | lm loss: 1.698092E+00 | loss scale: 1.0 | grad norm: 1.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1674/   14369 | consumed samples:      1714176 | elapsed time per iteration (ms): 111606.9 | rate (tokens/sec): 18790.52 | learning rate: 2.511E-04 | global batch size:  1024 | lm loss: 1.686683E+00 | loss scale: 1.0 | grad norm: 1.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1675/   14369 | consumed samples:      1715200 | elapsed time per iteration (ms): 112540.8 | rate (tokens/sec): 18634.59 | learning rate: 2.512E-04 | global batch size:  1024 | lm loss: 1.677159E+00 | loss scale: 1.0 | grad norm: 1.159 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1676/   14369 | consumed samples:      1716224 | elapsed time per iteration (ms): 112437.3 | rate (tokens/sec): 18651.75 | learning rate: 2.514E-04 | global batch size:  1024 | lm loss: 1.689813E+00 | loss scale: 1.0 | grad norm: 1.277 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1677/   14369 | consumed samples:      1717248 | elapsed time per iteration (ms): 112220.0 | rate (tokens/sec): 18687.86 | learning rate: 2.516E-04 | global batch size:  1024 | lm loss: 1.689039E+00 | loss scale: 1.0 | grad norm: 1.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1678/   14369 | consumed samples:      1718272 | elapsed time per iteration (ms): 111959.9 | rate (tokens/sec): 18731.28 | learning rate: 2.517E-04 | global batch size:  1024 | lm loss: 1.692468E+00 | loss scale: 1.0 | grad norm: 1.214 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1679/   14369 | consumed samples:      1719296 | elapsed time per iteration (ms): 112066.9 | rate (tokens/sec): 18713.39 | learning rate: 2.518E-04 | global batch size:  1024 | lm loss: 1.700524E+00 | loss scale: 1.0 | grad norm: 1.409 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1680/   14369 | consumed samples:      1720320 | elapsed time per iteration (ms): 114587.0 | rate (tokens/sec): 18301.83 | learning rate: 2.520E-04 | global batch size:  1024 | lm loss: 1.676209E+00 | loss scale: 1.0 | grad norm: 1.174 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1681/   14369 | consumed samples:      1721344 | elapsed time per iteration (ms): 116260.0 | rate (tokens/sec): 18038.46 | learning rate: 2.521E-04 | global batch size:  1024 | lm loss: 1.681133E+00 | loss scale: 1.0 | grad norm: 1.163 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1682/   14369 | consumed samples:      1722368 | elapsed time per iteration (ms): 114959.9 | rate (tokens/sec): 18242.47 | learning rate: 2.523E-04 | global batch size:  1024 | lm loss: 1.693334E+00 | loss scale: 1.0 | grad norm: 1.332 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1683/   14369 | consumed samples:      1723392 | elapsed time per iteration (ms): 112881.3 | rate (tokens/sec): 18578.38 | learning rate: 2.524E-04 | global batch size:  1024 | lm loss: 1.674829E+00 | loss scale: 1.0 | grad norm: 1.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1684/   14369 | consumed samples:      1724416 | elapsed time per iteration (ms): 114085.9 | rate (tokens/sec): 18382.22 | learning rate: 2.526E-04 | global batch size:  1024 | lm loss: 1.670048E+00 | loss scale: 1.0 | grad norm: 1.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1685/   14369 | consumed samples:      1725440 | elapsed time per iteration (ms): 116700.2 | rate (tokens/sec): 17970.43 | learning rate: 2.527E-04 | global batch size:  1024 | lm loss: 1.662199E+00 | loss scale: 1.0 | grad norm: 1.035 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1686/   14369 | consumed samples:      1726464 | elapsed time per iteration (ms): 113820.0 | rate (tokens/sec): 18425.16 | learning rate: 2.529E-04 | global batch size:  1024 | lm loss: 1.685928E+00 | loss scale: 1.0 | grad norm: 1.596 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1687/   14369 | consumed samples:      1727488 | elapsed time per iteration (ms): 113980.9 | rate (tokens/sec): 18399.15 | learning rate: 2.530E-04 | global batch size:  1024 | lm loss: 1.672964E+00 | loss scale: 1.0 | grad norm: 1.052 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1688/   14369 | consumed samples:      1728512 | elapsed time per iteration (ms): 115139.9 | rate (tokens/sec): 18213.95 | learning rate: 2.532E-04 | global batch size:  1024 | lm loss: 1.693580E+00 | loss scale: 1.0 | grad norm: 1.534 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1689/   14369 | consumed samples:      1729536 | elapsed time per iteration (ms): 114854.7 | rate (tokens/sec): 18259.18 | learning rate: 2.533E-04 | global batch size:  1024 | lm loss: 1.687132E+00 | loss scale: 1.0 | grad norm: 1.151 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1690/   14369 | consumed samples:      1730560 | elapsed time per iteration (ms): 112696.9 | rate (tokens/sec): 18608.79 | learning rate: 2.535E-04 | global batch size:  1024 | lm loss: 1.690398E+00 | loss scale: 1.0 | grad norm: 1.498 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1691/   14369 | consumed samples:      1731584 | elapsed time per iteration (ms): 113100.0 | rate (tokens/sec): 18542.45 | learning rate: 2.537E-04 | global batch size:  1024 | lm loss: 1.680135E+00 | loss scale: 1.0 | grad norm: 1.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1692/   14369 | consumed samples:      1732608 | elapsed time per iteration (ms): 113439.9 | rate (tokens/sec): 18486.90 | learning rate: 2.538E-04 | global batch size:  1024 | lm loss: 1.691181E+00 | loss scale: 1.0 | grad norm: 1.489 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1693/   14369 | consumed samples:      1733632 | elapsed time per iteration (ms): 112901.7 | rate (tokens/sec): 18575.02 | learning rate: 2.539E-04 | global batch size:  1024 | lm loss: 1.693366E+00 | loss scale: 1.0 | grad norm: 1.239 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1694/   14369 | consumed samples:      1734656 | elapsed time per iteration (ms): 111958.9 | rate (tokens/sec): 18731.45 | learning rate: 2.541E-04 | global batch size:  1024 | lm loss: 1.693509E+00 | loss scale: 1.0 | grad norm: 1.210 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1695/   14369 | consumed samples:      1735680 | elapsed time per iteration (ms): 112358.5 | rate (tokens/sec): 18664.83 | learning rate: 2.542E-04 | global batch size:  1024 | lm loss: 1.697225E+00 | loss scale: 1.0 | grad norm: 1.371 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1696/   14369 | consumed samples:      1736704 | elapsed time per iteration (ms): 112467.2 | rate (tokens/sec): 18646.78 | learning rate: 2.544E-04 | global batch size:  1024 | lm loss: 1.674022E+00 | loss scale: 1.0 | grad norm: 0.961 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1697/   14369 | consumed samples:      1737728 | elapsed time per iteration (ms): 112377.9 | rate (tokens/sec): 18661.61 | learning rate: 2.545E-04 | global batch size:  1024 | lm loss: 1.676217E+00 | loss scale: 1.0 | grad norm: 1.165 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1698/   14369 | consumed samples:      1738752 | elapsed time per iteration (ms): 112019.9 | rate (tokens/sec): 18721.25 | learning rate: 2.547E-04 | global batch size:  1024 | lm loss: 1.685307E+00 | loss scale: 1.0 | grad norm: 1.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1699/   14369 | consumed samples:      1739776 | elapsed time per iteration (ms): 112519.9 | rate (tokens/sec): 18638.06 | learning rate: 2.548E-04 | global batch size:  1024 | lm loss: 1.696118E+00 | loss scale: 1.0 | grad norm: 1.325 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1700/   14369 | consumed samples:      1740800 | elapsed time per iteration (ms): 112261.7 | rate (tokens/sec): 18680.92 | learning rate: 2.550E-04 | global batch size:  1024 | lm loss: 1.680423E+00 | loss scale: 1.0 | grad norm: 1.251 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1701/   14369 | consumed samples:      1741824 | elapsed time per iteration (ms): 112315.4 | rate (tokens/sec): 18671.99 | learning rate: 2.551E-04 | global batch size:  1024 | lm loss: 1.680174E+00 | loss scale: 1.0 | grad norm: 1.127 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1702/   14369 | consumed samples:      1742848 | elapsed time per iteration (ms): 112720.0 | rate (tokens/sec): 18604.96 | learning rate: 2.553E-04 | global batch size:  1024 | lm loss: 1.682188E+00 | loss scale: 1.0 | grad norm: 6.092 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1703/   14369 | consumed samples:      1743872 | elapsed time per iteration (ms): 112316.1 | rate (tokens/sec): 18671.87 | learning rate: 2.554E-04 | global batch size:  1024 | lm loss: 1.716707E+00 | loss scale: 1.0 | grad norm: 2.387 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1704/   14369 | consumed samples:      1744896 | elapsed time per iteration (ms): 111956.7 | rate (tokens/sec): 18731.81 | learning rate: 2.556E-04 | global batch size:  1024 | lm loss: 1.698906E+00 | loss scale: 1.0 | grad norm: 1.544 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1705/   14369 | consumed samples:      1745920 | elapsed time per iteration (ms): 111862.0 | rate (tokens/sec): 18747.67 | learning rate: 2.557E-04 | global batch size:  1024 | lm loss: 1.689001E+00 | loss scale: 1.0 | grad norm: 1.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1706/   14369 | consumed samples:      1746944 | elapsed time per iteration (ms): 112298.9 | rate (tokens/sec): 18674.73 | learning rate: 2.559E-04 | global batch size:  1024 | lm loss: 1.683383E+00 | loss scale: 1.0 | grad norm: 1.867 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1707/   14369 | consumed samples:      1747968 | elapsed time per iteration (ms): 112681.7 | rate (tokens/sec): 18611.29 | learning rate: 2.561E-04 | global batch size:  1024 | lm loss: 1.687818E+00 | loss scale: 1.0 | grad norm: 0.943 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1708/   14369 | consumed samples:      1748992 | elapsed time per iteration (ms): 112399.9 | rate (tokens/sec): 18657.96 | learning rate: 2.562E-04 | global batch size:  1024 | lm loss: 1.705956E+00 | loss scale: 1.0 | grad norm: 2.542 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1709/   14369 | consumed samples:      1750016 | elapsed time per iteration (ms): 111301.7 | rate (tokens/sec): 18842.05 | learning rate: 2.563E-04 | global batch size:  1024 | lm loss: 1.723307E+00 | loss scale: 1.0 | grad norm: 1.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1710/   14369 | consumed samples:      1751040 | elapsed time per iteration (ms): 111600.0 | rate (tokens/sec): 18791.68 | learning rate: 2.565E-04 | global batch size:  1024 | lm loss: 1.700407E+00 | loss scale: 1.0 | grad norm: 1.942 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1711/   14369 | consumed samples:      1752064 | elapsed time per iteration (ms): 111226.4 | rate (tokens/sec): 18854.80 | learning rate: 2.566E-04 | global batch size:  1024 | lm loss: 1.716904E+00 | loss scale: 1.0 | grad norm: 1.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1712/   14369 | consumed samples:      1753088 | elapsed time per iteration (ms): 111799.9 | rate (tokens/sec): 18758.09 | learning rate: 2.568E-04 | global batch size:  1024 | lm loss: 1.693919E+00 | loss scale: 1.0 | grad norm: 1.374 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1713/   14369 | consumed samples:      1754112 | elapsed time per iteration (ms): 111599.9 | rate (tokens/sec): 18791.70 | learning rate: 2.569E-04 | global batch size:  1024 | lm loss: 1.695915E+00 | loss scale: 1.0 | grad norm: 1.161 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1714/   14369 | consumed samples:      1755136 | elapsed time per iteration (ms): 111904.9 | rate (tokens/sec): 18740.48 | learning rate: 2.571E-04 | global batch size:  1024 | lm loss: 1.699097E+00 | loss scale: 1.0 | grad norm: 1.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1715/   14369 | consumed samples:      1756160 | elapsed time per iteration (ms): 112640.0 | rate (tokens/sec): 18618.18 | learning rate: 2.572E-04 | global batch size:  1024 | lm loss: 1.675371E+00 | loss scale: 1.0 | grad norm: 1.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1716/   14369 | consumed samples:      1757184 | elapsed time per iteration (ms): 111836.9 | rate (tokens/sec): 18751.88 | learning rate: 2.574E-04 | global batch size:  1024 | lm loss: 1.695474E+00 | loss scale: 1.0 | grad norm: 1.487 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1717/   14369 | consumed samples:      1758208 | elapsed time per iteration (ms): 112159.7 | rate (tokens/sec): 18697.91 | learning rate: 2.575E-04 | global batch size:  1024 | lm loss: 1.677608E+00 | loss scale: 1.0 | grad norm: 1.162 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1718/   14369 | consumed samples:      1759232 | elapsed time per iteration (ms): 112812.0 | rate (tokens/sec): 18589.79 | learning rate: 2.577E-04 | global batch size:  1024 | lm loss: 1.664139E+00 | loss scale: 1.0 | grad norm: 1.104 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1719/   14369 | consumed samples:      1760256 | elapsed time per iteration (ms): 111406.9 | rate (tokens/sec): 18824.26 | learning rate: 2.578E-04 | global batch size:  1024 | lm loss: 1.685649E+00 | loss scale: 1.0 | grad norm: 1.212 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1720/   14369 | consumed samples:      1761280 | elapsed time per iteration (ms): 112042.0 | rate (tokens/sec): 18717.55 | learning rate: 2.580E-04 | global batch size:  1024 | lm loss: 1.675905E+00 | loss scale: 1.0 | grad norm: 1.593 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1721/   14369 | consumed samples:      1762304 | elapsed time per iteration (ms): 112339.9 | rate (tokens/sec): 18667.92 | learning rate: 2.582E-04 | global batch size:  1024 | lm loss: 1.701213E+00 | loss scale: 1.0 | grad norm: 0.943 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1722/   14369 | consumed samples:      1763328 | elapsed time per iteration (ms): 111715.5 | rate (tokens/sec): 18772.26 | learning rate: 2.583E-04 | global batch size:  1024 | lm loss: 1.669972E+00 | loss scale: 1.0 | grad norm: 1.388 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1723/   14369 | consumed samples:      1764352 | elapsed time per iteration (ms): 112505.9 | rate (tokens/sec): 18640.37 | learning rate: 2.584E-04 | global batch size:  1024 | lm loss: 1.662593E+00 | loss scale: 1.0 | grad norm: 1.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1724/   14369 | consumed samples:      1765376 | elapsed time per iteration (ms): 111980.0 | rate (tokens/sec): 18727.91 | learning rate: 2.586E-04 | global batch size:  1024 | lm loss: 1.675388E+00 | loss scale: 1.0 | grad norm: 1.197 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1725/   14369 | consumed samples:      1766400 | elapsed time per iteration (ms): 111752.9 | rate (tokens/sec): 18765.98 | learning rate: 2.587E-04 | global batch size:  1024 | lm loss: 1.676924E+00 | loss scale: 1.0 | grad norm: 1.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1726/   14369 | consumed samples:      1767424 | elapsed time per iteration (ms): 111646.9 | rate (tokens/sec): 18783.80 | learning rate: 2.589E-04 | global batch size:  1024 | lm loss: 1.686340E+00 | loss scale: 1.0 | grad norm: 1.397 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1727/   14369 | consumed samples:      1768448 | elapsed time per iteration (ms): 112421.8 | rate (tokens/sec): 18654.32 | learning rate: 2.590E-04 | global batch size:  1024 | lm loss: 1.675285E+00 | loss scale: 1.0 | grad norm: 1.185 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1728/   14369 | consumed samples:      1769472 | elapsed time per iteration (ms): 111981.8 | rate (tokens/sec): 18727.61 | learning rate: 2.592E-04 | global batch size:  1024 | lm loss: 1.655307E+00 | loss scale: 1.0 | grad norm: 1.228 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1729/   14369 | consumed samples:      1770496 | elapsed time per iteration (ms): 112547.0 | rate (tokens/sec): 18633.56 | learning rate: 2.593E-04 | global batch size:  1024 | lm loss: 1.703834E+00 | loss scale: 1.0 | grad norm: 1.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1730/   14369 | consumed samples:      1771520 | elapsed time per iteration (ms): 111662.6 | rate (tokens/sec): 18781.15 | learning rate: 2.595E-04 | global batch size:  1024 | lm loss: 1.684585E+00 | loss scale: 1.0 | grad norm: 0.884 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1731/   14369 | consumed samples:      1772544 | elapsed time per iteration (ms): 112460.0 | rate (tokens/sec): 18647.98 | learning rate: 2.596E-04 | global batch size:  1024 | lm loss: 1.685798E+00 | loss scale: 1.0 | grad norm: 1.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1732/   14369 | consumed samples:      1773568 | elapsed time per iteration (ms): 112739.9 | rate (tokens/sec): 18601.68 | learning rate: 2.598E-04 | global batch size:  1024 | lm loss: 1.698203E+00 | loss scale: 1.0 | grad norm: 0.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1733/   14369 | consumed samples:      1774592 | elapsed time per iteration (ms): 111608.9 | rate (tokens/sec): 18790.19 | learning rate: 2.599E-04 | global batch size:  1024 | lm loss: 1.698430E+00 | loss scale: 1.0 | grad norm: 1.404 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1734/   14369 | consumed samples:      1775616 | elapsed time per iteration (ms): 111959.9 | rate (tokens/sec): 18731.28 | learning rate: 2.601E-04 | global batch size:  1024 | lm loss: 1.670536E+00 | loss scale: 1.0 | grad norm: 1.285 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1735/   14369 | consumed samples:      1776640 | elapsed time per iteration (ms): 113866.9 | rate (tokens/sec): 18417.57 | learning rate: 2.602E-04 | global batch size:  1024 | lm loss: 1.660906E+00 | loss scale: 1.0 | grad norm: 1.091 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1736/   14369 | consumed samples:      1777664 | elapsed time per iteration (ms): 111453.0 | rate (tokens/sec): 18816.47 | learning rate: 2.604E-04 | global batch size:  1024 | lm loss: 1.687876E+00 | loss scale: 1.0 | grad norm: 1.430 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1737/   14369 | consumed samples:      1778688 | elapsed time per iteration (ms): 111642.0 | rate (tokens/sec): 18784.62 | learning rate: 2.606E-04 | global batch size:  1024 | lm loss: 1.667296E+00 | loss scale: 1.0 | grad norm: 1.054 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1738/   14369 | consumed samples:      1779712 | elapsed time per iteration (ms): 112901.7 | rate (tokens/sec): 18575.03 | learning rate: 2.607E-04 | global batch size:  1024 | lm loss: 1.683409E+00 | loss scale: 1.0 | grad norm: 1.433 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1739/   14369 | consumed samples:      1780736 | elapsed time per iteration (ms): 111921.7 | rate (tokens/sec): 18737.68 | learning rate: 2.608E-04 | global batch size:  1024 | lm loss: 1.665892E+00 | loss scale: 1.0 | grad norm: 1.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1740/   14369 | consumed samples:      1781760 | elapsed time per iteration (ms): 111560.0 | rate (tokens/sec): 18798.42 | learning rate: 2.610E-04 | global batch size:  1024 | lm loss: 1.703861E+00 | loss scale: 1.0 | grad norm: 1.341 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1741/   14369 | consumed samples:      1782784 | elapsed time per iteration (ms): 112159.9 | rate (tokens/sec): 18697.88 | learning rate: 2.611E-04 | global batch size:  1024 | lm loss: 1.692229E+00 | loss scale: 1.0 | grad norm: 1.204 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1742/   14369 | consumed samples:      1783808 | elapsed time per iteration (ms): 112679.9 | rate (tokens/sec): 18611.59 | learning rate: 2.613E-04 | global batch size:  1024 | lm loss: 1.669867E+00 | loss scale: 1.0 | grad norm: 1.277 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1743/   14369 | consumed samples:      1784832 | elapsed time per iteration (ms): 111579.9 | rate (tokens/sec): 18795.07 | learning rate: 2.614E-04 | global batch size:  1024 | lm loss: 1.660545E+00 | loss scale: 1.0 | grad norm: 1.470 | number of skipped iterations:   0 | number of nan iterations:   0 |
