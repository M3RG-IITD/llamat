This is the arguments from the loaded checkpoint.
Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, hidden_size=4096, ffn_hidden_size=14336, num_attention_heads=32, num_attention_heads_kv=8, kv_channels=128, max_position_embeddings=8192, make_vocab_size_divisible_by=128, layernorm_epsilon=1e-05, apply_residual_connection_post_layernorm=False, use_bias=False, use_rms_norm=True, use_post_ln=False, onnx_safe=None, glu_activation='swiglu', position_embedding_type=<PositionEmbeddingType.rotary: 1>, rope_scaling_factor=1.0, rope_theta=500000.0, parallel_attn=False, parallel_layernorm=False, tie_embed_logits=False, sliding_window_size=None, attention_dropout=0.0, hidden_dropout=0.0, lima_dropout=False, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, micro_batch_size=2, global_batch_size=1024, rampup_batch_size=None, recompute_granularity='selective', distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, train_iters=14369, skip_iters=[], train_samples=None, log_interval=1, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='/scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3/tensorboards/llama2-7b-tp1-pp4', masked_softmax_fusion=True, bias_gelu_fusion=False, bias_dropout_fusion=False, use_flash_attn=True, optimizer='adam', dataloader_type='cyclic', async_tensor_model_parallel_allreduce=True, no_persist_layer_norm=False, sequence_parallel=False, gradient_accumulation_fusion=True, seed=1234, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=0.0003, lr_decay_style='cosine', lr_decay_iters=14369, lr_decay_samples=None, lr_warmup_fraction=None, lr_warmup_iters=2000, lr_warmup_samples=0, min_lr=3e-05, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='/scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3', save_interval=150, no_save_optim=None, no_save_rng=None, load='/scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3', load_iters=None, no_load_optim=None, no_load_rng=None, finetune=False, perform_initialization=True, use_checkpoint_args=True, fp16=False, bf16=True, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=True, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, pipeline_model_parallel_size=4, pipeline_model_parallel_split_rank=None, num_layers_per_virtual_pipeline_stage=None, distributed_backend='nccl', DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_only=False, eval_iters=10, eval_interval=150, data_path=None, split='969, 30, 1', train_data_path=['/scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/train/train_text_document'], valid_data_path=['/scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/val/val_text_document'], test_data_path=['/scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/redp_val_text_document'], vocab_file='/scratch/cse/btech/cs1200448/hf-to-meditron-weights/8b/tokenizer.model', merge_file=None, vocab_extra_ids=0, vocab_extra_ids_list=None, seq_length=2048, variable_seq_lengths=False, scalar_loss_mask=0.0, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='Tiktoken', tokenizer_model=None, new_tokens=False, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=False, log_validation_ppl_to_tensorboard=True, log_memory_to_tensorboard=True, log_world_size_to_tensorboard=False, wandb_logger=False, wandb_project=None, wandb_entity='meditron', wandb_name=None, wandb_id=None, wandb_resume='allow', wandb_api_key=None, metrics=[], inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', model_name='llama2', model_type='encoder_or_decoder', data_type='gpt', log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, rank=0, world_size=8, iteration=300, padded_vocab_size=128256, transformer_pipeline_model_parallel_size=4, data_parallel_size=2, virtual_pipeline_model_parallel_size=None, params_dtype=torch.bfloat16, consumed_train_samples=614400, consumed_valid_samples=81920, do_train=1, do_valid=1, do_test=1, curr_iteration=599)
using world size: 8, data-parallel-size: 2, tensor-model-parallel size: 1, pipeline-model-parallel size: 4 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:Tiktoken
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
setting number of micro-batches to constant 256
> building Tiktoken tokenizer ...
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
> initializing torch distributed ...
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 4
> setting random seeds to 1234 ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-05
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.0
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_load ....................................... None
  bf16 ............................................ True
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 2
  data_path ....................................... None
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  data_type ....................................... gpt
  dataloader_type ................................. cyclic
  DDP_impl ........................................ local
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  encoder_num_layers .............................. 32
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 150
  eval_iters ...................................... 10
  eval_only ....................................... False
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_signal_handler ............................. False
  ffn_hidden_size ................................. 14336
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_e4m3 ........................................ False
  fp8_hybrid ...................................... False
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 1024
  glu_activation .................................. swiglu
  gradient_accumulation_fusion .................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 4096
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  iteration ....................................... 600
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lima_dropout .................................... False
  load ............................................ /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
  load_iters ...................................... None
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... True
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0003
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 2000
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 8192
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  metrics ......................................... []
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 3e-05
  mmap_warmup ..................................... False
  model_name ...................................... llama2
  model_type ...................................... encoder_or_decoder
  new_tokens ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_kv .......................... 8
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_layers ...................................... 32
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  optimizer ....................................... adam
  override_opt_param_scheduler .................... False
  padded_vocab_size ............................... 128256
  parallel_attn ................................... False
  parallel_layernorm .............................. False
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 4
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... PositionEmbeddingType.rotary
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ 1
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  rope_scaling_factor ............................. 1.0
  rope_theta ...................................... 500000.0
  sample_rate ..................................... 1.0
  save ............................................ /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
  save_interval ................................... 150
  scalar_loss_mask ................................ 0.0
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_iters ...................................... []
  sliding_window_size ............................. None
  split ........................................... 969, 30, 1
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3/tensorboards/llama2-7b-tp1-pp4
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. ['/scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/redp_val_text_document']
  tie_embed_logits ................................ False
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. Tiktoken
  train_data_path ................................. ['/scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/train/train_text_document']
  train_iters ..................................... 14369
  train_samples ................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 4
  use_bias ........................................ False
  use_checkpoint_args ............................. True
  use_checkpoint_opt_param_scheduler .............. False
  use_contiguous_buffers_in_local_ddp ............. True
  use_cpu_initialization .......................... None
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. True
  use_one_sent_docs ............................... False
  use_post_ln ..................................... False
  use_ring_exchange_p2p ........................... False
  use_rms_norm .................................... True
  valid_data_path ................................. ['/scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/val/val_text_document']
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_extra_ids_list ............................ None
  vocab_file ...................................... /scratch/cse/btech/cs1200448/hf-to-meditron-weights/8b/tokenizer.model
  wandb_api_key ................................... None
  wandb_entity .................................... meditron
  wandb_id ........................................ None
  wandb_logger .................................... False
  wandb_name ...................................... None
  wandb_project ................................... None
  wandb_resume .................................... allow
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 8
-------------------- end of arguments ---------------------
time to initialize megatron (seconds): 37.090
[after megatron is initialized] datetime: 2024-04-28 16:51:12 
Building model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1744896000
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1744896000
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2270236672
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2270232576
> learning rate decay style: cosine
 loading checkpoint from /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3 at iteration 600
 checkpoint version 3.0
 > using checkpoint value 0.0003 for learning rate
 > using checkpoint value 3e-05 for minimum learning rate
 > using checkpoint value 2048000 for warmup iterations
 > using checkpoint value 14713856 for total number of iterations
 > using checkpoint value cosine for learning rate decay style
 > using checkpoint value 0.1 for start weight decay
 > using checkpoint value 0.1 for end weight decay
 > using checkpoint value 14713856 for total number of weight decay iterations
 > using checkpoint value constant for weight decay incr style
  successfully loaded checkpoint from /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3 at iteration 600
(min, max) time across ranks (ms):
    load-checkpoint ................................: (42262.23, 42263.60)
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-04-28 16:52:02 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      14713856
    validation: 983040
    test:       10240
> building train, validation, and test datasets ...
Separate data paths provided for train, valid & test. Split string will be ignored.
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.136420 seconds
    number of documents: 4399676
    number of tokens: 30135507860
    train:
     document indices in [0, 4399676) total of 4399676 documents
 > loading doc-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/train/train_text_document_train_indexmap_14713856ns_2048sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/train/train_text_document_train_indexmap_14713856ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/train/train_text_document_train_indexmap_14713856ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.008 seconds
    total number of tokens: 30135507860
    total number of samples: 14714604
    total number of epochs: 1
 > building dataset index ...
    warming up index mmap file...
    reading sizes...
    reading pointers...
    reading document index...
    warming up data mmap file...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 1.267253 seconds
    number of documents: 106395
    number of tokens: 514977472
    valid:
     document indices in [0, 106395) total of 106395 documents
 > loading doc-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/val/val_text_document_valid_indexmap_983040ns_2048sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/val/val_text_document_valid_indexmap_983040ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/val/val_text_document_valid_indexmap_983040ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.030 seconds
    total number of tokens: 514977472
    total number of samples: 1005816
    total number of epochs: 4
 > building dataset index ...
    warming up index mmap file...
    reading sizes...
    reading pointers...
    reading document index...
    warming up data mmap file...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.350550 seconds
    number of documents: 46530
    number of tokens: 68328730
    test:
     document indices in [0, 46530) total of 46530 documents
 > loading doc-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/redp_val_text_document_test_indexmap_10240ns_2048sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/redp_val_text_document_test_indexmap_10240ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /scratch/civil/phd/cez198233/vaibhav_nlp/corpus/corpus_training/redp_val_text_document_test_indexmap_10240ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of tokens: 68328730
    total number of samples: 33364
    total number of epochs: 1
> finished creating datasets ...
[after dataloaders are built] datetime: 2024-04-28 16:52:07 
done with setup ...
training ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (49168.70, 49196.95)
    train/valid/test-data-iterators-setup ..........: (5000.77, 5157.72)
[before the start of training step] datetime: 2024-04-28 16:52:07 
 iteration      601/   14369 | consumed samples:       615424 | elapsed time per iteration (ms): 125714.3 | rate (tokens/sec): 16681.89 | learning rate: 9.015E-05 | global batch size:  1024 | lm loss: 1.947304E+00 | loss scale: 1.0 | grad norm: 2.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 6] (after 601 iterations) memory (MB) | allocated: 39051.4482421875 | max allocated: 52246.119140625 | reserved: 54388.0 | max reserved: 54388.0
[Rank 4] (after 601 iterations) memory (MB) | allocated: 30033.376953125 | max allocated: 46233.9013671875 | reserved: 46558.0 | max reserved: 46558.0
[Rank 0] (after 601 iterations) memory (MB) | allocated: 38987.376953125 | max allocated: 71068.55078125 | reserved: 72260.0 | max reserved: 72260.0
[Rank 2] (after 601 iterations) memory (MB) | allocated: 30033.376953125 | max allocated: 54222.16357421875 | reserved: 54482.0 | max reserved: 54482.0
 iteration      602/   14369 | consumed samples:       616448 | elapsed time per iteration (ms): 110475.7 | rate (tokens/sec): 18982.93 | learning rate: 9.030E-05 | global batch size:  1024 | lm loss: 1.928575E+00 | loss scale: 1.0 | grad norm: 1.490 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      603/   14369 | consumed samples:       617472 | elapsed time per iteration (ms): 111680.8 | rate (tokens/sec): 18778.09 | learning rate: 9.045E-05 | global batch size:  1024 | lm loss: 2.001817E+00 | loss scale: 1.0 | grad norm: 7.654 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      604/   14369 | consumed samples:       618496 | elapsed time per iteration (ms): 110784.5 | rate (tokens/sec): 18930.02 | learning rate: 9.060E-05 | global batch size:  1024 | lm loss: 2.097463E+00 | loss scale: 1.0 | grad norm: 8.967 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      605/   14369 | consumed samples:       619520 | elapsed time per iteration (ms): 112342.8 | rate (tokens/sec): 18667.44 | learning rate: 9.075E-05 | global batch size:  1024 | lm loss: 2.028672E+00 | loss scale: 1.0 | grad norm: 4.356 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      606/   14369 | consumed samples:       620544 | elapsed time per iteration (ms): 112040.0 | rate (tokens/sec): 18717.88 | learning rate: 9.090E-05 | global batch size:  1024 | lm loss: 2.012585E+00 | loss scale: 1.0 | grad norm: 3.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      607/   14369 | consumed samples:       621568 | elapsed time per iteration (ms): 111804.8 | rate (tokens/sec): 18757.27 | learning rate: 9.105E-05 | global batch size:  1024 | lm loss: 2.001368E+00 | loss scale: 1.0 | grad norm: 3.213 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      608/   14369 | consumed samples:       622592 | elapsed time per iteration (ms): 111440.0 | rate (tokens/sec): 18818.66 | learning rate: 9.120E-05 | global batch size:  1024 | lm loss: 1.954608E+00 | loss scale: 1.0 | grad norm: 2.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      609/   14369 | consumed samples:       623616 | elapsed time per iteration (ms): 112599.9 | rate (tokens/sec): 18624.81 | learning rate: 9.135E-05 | global batch size:  1024 | lm loss: 1.975949E+00 | loss scale: 1.0 | grad norm: 2.609 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      610/   14369 | consumed samples:       624640 | elapsed time per iteration (ms): 111681.9 | rate (tokens/sec): 18777.91 | learning rate: 9.150E-05 | global batch size:  1024 | lm loss: 1.980355E+00 | loss scale: 1.0 | grad norm: 2.605 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      611/   14369 | consumed samples:       625664 | elapsed time per iteration (ms): 112171.6 | rate (tokens/sec): 18695.93 | learning rate: 9.165E-05 | global batch size:  1024 | lm loss: 1.970846E+00 | loss scale: 1.0 | grad norm: 2.424 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      612/   14369 | consumed samples:       626688 | elapsed time per iteration (ms): 112742.7 | rate (tokens/sec): 18601.22 | learning rate: 9.180E-05 | global batch size:  1024 | lm loss: 1.961198E+00 | loss scale: 1.0 | grad norm: 1.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      613/   14369 | consumed samples:       627712 | elapsed time per iteration (ms): 113120.0 | rate (tokens/sec): 18539.18 | learning rate: 9.195E-05 | global batch size:  1024 | lm loss: 1.977734E+00 | loss scale: 1.0 | grad norm: 2.450 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      614/   14369 | consumed samples:       628736 | elapsed time per iteration (ms): 112201.9 | rate (tokens/sec): 18690.88 | learning rate: 9.210E-05 | global batch size:  1024 | lm loss: 1.954155E+00 | loss scale: 1.0 | grad norm: 2.246 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      615/   14369 | consumed samples:       629760 | elapsed time per iteration (ms): 111765.2 | rate (tokens/sec): 18763.91 | learning rate: 9.225E-05 | global batch size:  1024 | lm loss: 1.969690E+00 | loss scale: 1.0 | grad norm: 1.654 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      616/   14369 | consumed samples:       630784 | elapsed time per iteration (ms): 111781.7 | rate (tokens/sec): 18761.13 | learning rate: 9.240E-05 | global batch size:  1024 | lm loss: 1.957303E+00 | loss scale: 1.0 | grad norm: 1.939 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      617/   14369 | consumed samples:       631808 | elapsed time per iteration (ms): 111554.0 | rate (tokens/sec): 18799.43 | learning rate: 9.255E-05 | global batch size:  1024 | lm loss: 1.954231E+00 | loss scale: 1.0 | grad norm: 2.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      618/   14369 | consumed samples:       632832 | elapsed time per iteration (ms): 112141.9 | rate (tokens/sec): 18700.87 | learning rate: 9.270E-05 | global batch size:  1024 | lm loss: 1.945017E+00 | loss scale: 1.0 | grad norm: 1.766 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      619/   14369 | consumed samples:       633856 | elapsed time per iteration (ms): 111501.9 | rate (tokens/sec): 18808.22 | learning rate: 9.285E-05 | global batch size:  1024 | lm loss: 1.938766E+00 | loss scale: 1.0 | grad norm: 2.167 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      620/   14369 | consumed samples:       634880 | elapsed time per iteration (ms): 111079.8 | rate (tokens/sec): 18879.70 | learning rate: 9.300E-05 | global batch size:  1024 | lm loss: 1.939422E+00 | loss scale: 1.0 | grad norm: 2.103 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      621/   14369 | consumed samples:       635904 | elapsed time per iteration (ms): 112780.0 | rate (tokens/sec): 18595.06 | learning rate: 9.315E-05 | global batch size:  1024 | lm loss: 1.969035E+00 | loss scale: 1.0 | grad norm: 2.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      622/   14369 | consumed samples:       636928 | elapsed time per iteration (ms): 111338.1 | rate (tokens/sec): 18835.88 | learning rate: 9.330E-05 | global batch size:  1024 | lm loss: 1.950008E+00 | loss scale: 1.0 | grad norm: 1.531 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      623/   14369 | consumed samples:       637952 | elapsed time per iteration (ms): 111886.0 | rate (tokens/sec): 18743.65 | learning rate: 9.345E-05 | global batch size:  1024 | lm loss: 1.939582E+00 | loss scale: 1.0 | grad norm: 2.215 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      624/   14369 | consumed samples:       638976 | elapsed time per iteration (ms): 111180.0 | rate (tokens/sec): 18862.68 | learning rate: 9.360E-05 | global batch size:  1024 | lm loss: 1.970098E+00 | loss scale: 1.0 | grad norm: 2.582 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      625/   14369 | consumed samples:       640000 | elapsed time per iteration (ms): 111654.9 | rate (tokens/sec): 18782.44 | learning rate: 9.375E-05 | global batch size:  1024 | lm loss: 1.943149E+00 | loss scale: 1.0 | grad norm: 2.073 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      626/   14369 | consumed samples:       641024 | elapsed time per iteration (ms): 112004.8 | rate (tokens/sec): 18723.76 | learning rate: 9.390E-05 | global batch size:  1024 | lm loss: 1.952745E+00 | loss scale: 1.0 | grad norm: 3.479 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      627/   14369 | consumed samples:       642048 | elapsed time per iteration (ms): 111333.0 | rate (tokens/sec): 18836.75 | learning rate: 9.405E-05 | global batch size:  1024 | lm loss: 1.934814E+00 | loss scale: 1.0 | grad norm: 1.886 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      628/   14369 | consumed samples:       643072 | elapsed time per iteration (ms): 112326.9 | rate (tokens/sec): 18670.08 | learning rate: 9.420E-05 | global batch size:  1024 | lm loss: 1.940173E+00 | loss scale: 1.0 | grad norm: 2.253 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      629/   14369 | consumed samples:       644096 | elapsed time per iteration (ms): 112554.8 | rate (tokens/sec): 18632.27 | learning rate: 9.435E-05 | global batch size:  1024 | lm loss: 1.945662E+00 | loss scale: 1.0 | grad norm: 2.255 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      630/   14369 | consumed samples:       645120 | elapsed time per iteration (ms): 112146.0 | rate (tokens/sec): 18700.20 | learning rate: 9.450E-05 | global batch size:  1024 | lm loss: 1.936493E+00 | loss scale: 1.0 | grad norm: 2.799 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      631/   14369 | consumed samples:       646144 | elapsed time per iteration (ms): 111942.8 | rate (tokens/sec): 18734.14 | learning rate: 9.465E-05 | global batch size:  1024 | lm loss: 1.941620E+00 | loss scale: 1.0 | grad norm: 1.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      632/   14369 | consumed samples:       647168 | elapsed time per iteration (ms): 111053.6 | rate (tokens/sec): 18884.14 | learning rate: 9.480E-05 | global batch size:  1024 | lm loss: 1.914336E+00 | loss scale: 1.0 | grad norm: 2.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      633/   14369 | consumed samples:       648192 | elapsed time per iteration (ms): 111847.8 | rate (tokens/sec): 18750.06 | learning rate: 9.495E-05 | global batch size:  1024 | lm loss: 1.938149E+00 | loss scale: 1.0 | grad norm: 2.322 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      634/   14369 | consumed samples:       649216 | elapsed time per iteration (ms): 111735.0 | rate (tokens/sec): 18768.98 | learning rate: 9.510E-05 | global batch size:  1024 | lm loss: 1.946185E+00 | loss scale: 1.0 | grad norm: 2.481 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      635/   14369 | consumed samples:       650240 | elapsed time per iteration (ms): 120925.8 | rate (tokens/sec): 17342.46 | learning rate: 9.525E-05 | global batch size:  1024 | lm loss: 1.943768E+00 | loss scale: 1.0 | grad norm: 2.384 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      636/   14369 | consumed samples:       651264 | elapsed time per iteration (ms): 117916.6 | rate (tokens/sec): 17785.05 | learning rate: 9.540E-05 | global batch size:  1024 | lm loss: 1.908937E+00 | loss scale: 1.0 | grad norm: 2.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      637/   14369 | consumed samples:       652288 | elapsed time per iteration (ms): 114115.0 | rate (tokens/sec): 18377.53 | learning rate: 9.555E-05 | global batch size:  1024 | lm loss: 1.934477E+00 | loss scale: 1.0 | grad norm: 2.334 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      638/   14369 | consumed samples:       653312 | elapsed time per iteration (ms): 114106.9 | rate (tokens/sec): 18378.83 | learning rate: 9.570E-05 | global batch size:  1024 | lm loss: 1.940214E+00 | loss scale: 1.0 | grad norm: 2.603 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      639/   14369 | consumed samples:       654336 | elapsed time per iteration (ms): 114720.0 | rate (tokens/sec): 18280.61 | learning rate: 9.585E-05 | global batch size:  1024 | lm loss: 1.934037E+00 | loss scale: 1.0 | grad norm: 2.145 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      640/   14369 | consumed samples:       655360 | elapsed time per iteration (ms): 113681.7 | rate (tokens/sec): 18447.57 | learning rate: 9.600E-05 | global batch size:  1024 | lm loss: 1.913344E+00 | loss scale: 1.0 | grad norm: 2.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      641/   14369 | consumed samples:       656384 | elapsed time per iteration (ms): 113556.8 | rate (tokens/sec): 18467.87 | learning rate: 9.615E-05 | global batch size:  1024 | lm loss: 1.927081E+00 | loss scale: 1.0 | grad norm: 2.089 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      642/   14369 | consumed samples:       657408 | elapsed time per iteration (ms): 112819.9 | rate (tokens/sec): 18588.50 | learning rate: 9.630E-05 | global batch size:  1024 | lm loss: 1.930594E+00 | loss scale: 1.0 | grad norm: 2.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      643/   14369 | consumed samples:       658432 | elapsed time per iteration (ms): 112581.8 | rate (tokens/sec): 18627.80 | learning rate: 9.645E-05 | global batch size:  1024 | lm loss: 1.934951E+00 | loss scale: 1.0 | grad norm: 2.420 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      644/   14369 | consumed samples:       659456 | elapsed time per iteration (ms): 114819.9 | rate (tokens/sec): 18264.71 | learning rate: 9.660E-05 | global batch size:  1024 | lm loss: 1.944318E+00 | loss scale: 1.0 | grad norm: 2.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      645/   14369 | consumed samples:       660480 | elapsed time per iteration (ms): 120080.9 | rate (tokens/sec): 17464.49 | learning rate: 9.675E-05 | global batch size:  1024 | lm loss: 1.936267E+00 | loss scale: 1.0 | grad norm: 2.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      646/   14369 | consumed samples:       661504 | elapsed time per iteration (ms): 113415.8 | rate (tokens/sec): 18490.82 | learning rate: 9.690E-05 | global batch size:  1024 | lm loss: 1.893630E+00 | loss scale: 1.0 | grad norm: 2.051 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      647/   14369 | consumed samples:       662528 | elapsed time per iteration (ms): 113299.9 | rate (tokens/sec): 18509.74 | learning rate: 9.705E-05 | global batch size:  1024 | lm loss: 1.930449E+00 | loss scale: 1.0 | grad norm: 2.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      648/   14369 | consumed samples:       663552 | elapsed time per iteration (ms): 112953.2 | rate (tokens/sec): 18566.56 | learning rate: 9.720E-05 | global batch size:  1024 | lm loss: 1.893738E+00 | loss scale: 1.0 | grad norm: 1.933 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      649/   14369 | consumed samples:       664576 | elapsed time per iteration (ms): 111879.9 | rate (tokens/sec): 18744.67 | learning rate: 9.735E-05 | global batch size:  1024 | lm loss: 1.944738E+00 | loss scale: 1.0 | grad norm: 2.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      650/   14369 | consumed samples:       665600 | elapsed time per iteration (ms): 113135.2 | rate (tokens/sec): 18536.69 | learning rate: 9.750E-05 | global batch size:  1024 | lm loss: 1.923522E+00 | loss scale: 1.0 | grad norm: 2.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      651/   14369 | consumed samples:       666624 | elapsed time per iteration (ms): 118474.7 | rate (tokens/sec): 17701.27 | learning rate: 9.765E-05 | global batch size:  1024 | lm loss: 1.911680E+00 | loss scale: 1.0 | grad norm: 2.837 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      652/   14369 | consumed samples:       667648 | elapsed time per iteration (ms): 121490.4 | rate (tokens/sec): 17261.87 | learning rate: 9.780E-05 | global batch size:  1024 | lm loss: 1.923026E+00 | loss scale: 1.0 | grad norm: 1.997 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      653/   14369 | consumed samples:       668672 | elapsed time per iteration (ms): 120307.0 | rate (tokens/sec): 17431.68 | learning rate: 9.795E-05 | global batch size:  1024 | lm loss: 1.919769E+00 | loss scale: 1.0 | grad norm: 2.785 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      654/   14369 | consumed samples:       669696 | elapsed time per iteration (ms): 118789.8 | rate (tokens/sec): 17654.31 | learning rate: 9.810E-05 | global batch size:  1024 | lm loss: 1.916473E+00 | loss scale: 1.0 | grad norm: 2.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      655/   14369 | consumed samples:       670720 | elapsed time per iteration (ms): 112581.6 | rate (tokens/sec): 18627.83 | learning rate: 9.825E-05 | global batch size:  1024 | lm loss: 1.916220E+00 | loss scale: 1.0 | grad norm: 2.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      656/   14369 | consumed samples:       671744 | elapsed time per iteration (ms): 113343.2 | rate (tokens/sec): 18502.67 | learning rate: 9.840E-05 | global batch size:  1024 | lm loss: 1.927447E+00 | loss scale: 1.0 | grad norm: 2.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      657/   14369 | consumed samples:       672768 | elapsed time per iteration (ms): 112419.3 | rate (tokens/sec): 18654.73 | learning rate: 9.855E-05 | global batch size:  1024 | lm loss: 1.906985E+00 | loss scale: 1.0 | grad norm: 2.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      658/   14369 | consumed samples:       673792 | elapsed time per iteration (ms): 112700.0 | rate (tokens/sec): 18608.27 | learning rate: 9.870E-05 | global batch size:  1024 | lm loss: 1.925189E+00 | loss scale: 1.0 | grad norm: 2.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      659/   14369 | consumed samples:       674816 | elapsed time per iteration (ms): 112780.6 | rate (tokens/sec): 18594.97 | learning rate: 9.885E-05 | global batch size:  1024 | lm loss: 1.903028E+00 | loss scale: 1.0 | grad norm: 1.950 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      660/   14369 | consumed samples:       675840 | elapsed time per iteration (ms): 112101.8 | rate (tokens/sec): 18707.56 | learning rate: 9.900E-05 | global batch size:  1024 | lm loss: 1.916463E+00 | loss scale: 1.0 | grad norm: 2.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      661/   14369 | consumed samples:       676864 | elapsed time per iteration (ms): 113901.1 | rate (tokens/sec): 18412.04 | learning rate: 9.915E-05 | global batch size:  1024 | lm loss: 1.886220E+00 | loss scale: 1.0 | grad norm: 1.946 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      662/   14369 | consumed samples:       677888 | elapsed time per iteration (ms): 113140.0 | rate (tokens/sec): 18535.90 | learning rate: 9.930E-05 | global batch size:  1024 | lm loss: 1.906657E+00 | loss scale: 1.0 | grad norm: 2.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      663/   14369 | consumed samples:       678912 | elapsed time per iteration (ms): 113683.9 | rate (tokens/sec): 18447.23 | learning rate: 9.945E-05 | global batch size:  1024 | lm loss: 1.929760E+00 | loss scale: 1.0 | grad norm: 1.875 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      664/   14369 | consumed samples:       679936 | elapsed time per iteration (ms): 111164.2 | rate (tokens/sec): 18865.35 | learning rate: 9.960E-05 | global batch size:  1024 | lm loss: 1.925900E+00 | loss scale: 1.0 | grad norm: 2.920 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      665/   14369 | consumed samples:       680960 | elapsed time per iteration (ms): 111693.0 | rate (tokens/sec): 18776.03 | learning rate: 9.975E-05 | global batch size:  1024 | lm loss: 1.933149E+00 | loss scale: 1.0 | grad norm: 2.172 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      666/   14369 | consumed samples:       681984 | elapsed time per iteration (ms): 111739.9 | rate (tokens/sec): 18768.16 | learning rate: 9.990E-05 | global batch size:  1024 | lm loss: 1.907081E+00 | loss scale: 1.0 | grad norm: 2.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      667/   14369 | consumed samples:       683008 | elapsed time per iteration (ms): 111924.7 | rate (tokens/sec): 18737.16 | learning rate: 1.000E-04 | global batch size:  1024 | lm loss: 1.904682E+00 | loss scale: 1.0 | grad norm: 2.461 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      668/   14369 | consumed samples:       684032 | elapsed time per iteration (ms): 112145.9 | rate (tokens/sec): 18700.22 | learning rate: 1.002E-04 | global batch size:  1024 | lm loss: 1.920300E+00 | loss scale: 1.0 | grad norm: 2.131 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      669/   14369 | consumed samples:       685056 | elapsed time per iteration (ms): 114879.9 | rate (tokens/sec): 18255.17 | learning rate: 1.003E-04 | global batch size:  1024 | lm loss: 1.914485E+00 | loss scale: 1.0 | grad norm: 2.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      670/   14369 | consumed samples:       686080 | elapsed time per iteration (ms): 111684.9 | rate (tokens/sec): 18777.40 | learning rate: 1.005E-04 | global batch size:  1024 | lm loss: 1.920320E+00 | loss scale: 1.0 | grad norm: 2.673 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      671/   14369 | consumed samples:       687104 | elapsed time per iteration (ms): 113289.9 | rate (tokens/sec): 18511.37 | learning rate: 1.006E-04 | global batch size:  1024 | lm loss: 1.912729E+00 | loss scale: 1.0 | grad norm: 2.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      672/   14369 | consumed samples:       688128 | elapsed time per iteration (ms): 112406.2 | rate (tokens/sec): 18656.92 | learning rate: 1.008E-04 | global batch size:  1024 | lm loss: 1.920253E+00 | loss scale: 1.0 | grad norm: 2.386 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      673/   14369 | consumed samples:       689152 | elapsed time per iteration (ms): 112939.0 | rate (tokens/sec): 18568.89 | learning rate: 1.010E-04 | global batch size:  1024 | lm loss: 1.912059E+00 | loss scale: 1.0 | grad norm: 1.959 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      674/   14369 | consumed samples:       690176 | elapsed time per iteration (ms): 112041.7 | rate (tokens/sec): 18717.60 | learning rate: 1.011E-04 | global batch size:  1024 | lm loss: 1.876823E+00 | loss scale: 1.0 | grad norm: 2.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      675/   14369 | consumed samples:       691200 | elapsed time per iteration (ms): 111917.0 | rate (tokens/sec): 18738.46 | learning rate: 1.012E-04 | global batch size:  1024 | lm loss: 1.876296E+00 | loss scale: 1.0 | grad norm: 2.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      676/   14369 | consumed samples:       692224 | elapsed time per iteration (ms): 112695.9 | rate (tokens/sec): 18608.95 | learning rate: 1.014E-04 | global batch size:  1024 | lm loss: 1.893450E+00 | loss scale: 1.0 | grad norm: 1.855 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      677/   14369 | consumed samples:       693248 | elapsed time per iteration (ms): 112495.7 | rate (tokens/sec): 18642.06 | learning rate: 1.015E-04 | global batch size:  1024 | lm loss: 1.913719E+00 | loss scale: 1.0 | grad norm: 2.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      678/   14369 | consumed samples:       694272 | elapsed time per iteration (ms): 111880.0 | rate (tokens/sec): 18744.65 | learning rate: 1.017E-04 | global batch size:  1024 | lm loss: 1.900969E+00 | loss scale: 1.0 | grad norm: 1.761 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      679/   14369 | consumed samples:       695296 | elapsed time per iteration (ms): 111541.7 | rate (tokens/sec): 18801.50 | learning rate: 1.018E-04 | global batch size:  1024 | lm loss: 1.907911E+00 | loss scale: 1.0 | grad norm: 2.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      680/   14369 | consumed samples:       696320 | elapsed time per iteration (ms): 112876.9 | rate (tokens/sec): 18579.11 | learning rate: 1.020E-04 | global batch size:  1024 | lm loss: 1.906109E+00 | loss scale: 1.0 | grad norm: 1.981 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      681/   14369 | consumed samples:       697344 | elapsed time per iteration (ms): 113402.1 | rate (tokens/sec): 18493.07 | learning rate: 1.021E-04 | global batch size:  1024 | lm loss: 1.910111E+00 | loss scale: 1.0 | grad norm: 2.193 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      682/   14369 | consumed samples:       698368 | elapsed time per iteration (ms): 113581.7 | rate (tokens/sec): 18463.82 | learning rate: 1.023E-04 | global batch size:  1024 | lm loss: 1.896628E+00 | loss scale: 1.0 | grad norm: 2.573 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      683/   14369 | consumed samples:       699392 | elapsed time per iteration (ms): 111645.6 | rate (tokens/sec): 18784.00 | learning rate: 1.024E-04 | global batch size:  1024 | lm loss: 1.901060E+00 | loss scale: 1.0 | grad norm: 1.793 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      684/   14369 | consumed samples:       700416 | elapsed time per iteration (ms): 111699.9 | rate (tokens/sec): 18774.88 | learning rate: 1.026E-04 | global batch size:  1024 | lm loss: 1.899813E+00 | loss scale: 1.0 | grad norm: 2.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      685/   14369 | consumed samples:       701440 | elapsed time per iteration (ms): 111326.9 | rate (tokens/sec): 18837.78 | learning rate: 1.027E-04 | global batch size:  1024 | lm loss: 1.917699E+00 | loss scale: 1.0 | grad norm: 2.367 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      686/   14369 | consumed samples:       702464 | elapsed time per iteration (ms): 112414.8 | rate (tokens/sec): 18655.48 | learning rate: 1.029E-04 | global batch size:  1024 | lm loss: 1.895707E+00 | loss scale: 1.0 | grad norm: 2.210 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      687/   14369 | consumed samples:       703488 | elapsed time per iteration (ms): 111365.0 | rate (tokens/sec): 18831.34 | learning rate: 1.030E-04 | global batch size:  1024 | lm loss: 1.893648E+00 | loss scale: 1.0 | grad norm: 2.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      688/   14369 | consumed samples:       704512 | elapsed time per iteration (ms): 111546.4 | rate (tokens/sec): 18800.72 | learning rate: 1.032E-04 | global batch size:  1024 | lm loss: 1.888808E+00 | loss scale: 1.0 | grad norm: 2.265 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      689/   14369 | consumed samples:       705536 | elapsed time per iteration (ms): 112802.9 | rate (tokens/sec): 18591.30 | learning rate: 1.033E-04 | global batch size:  1024 | lm loss: 1.895651E+00 | loss scale: 1.0 | grad norm: 2.901 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      690/   14369 | consumed samples:       706560 | elapsed time per iteration (ms): 112517.7 | rate (tokens/sec): 18638.42 | learning rate: 1.035E-04 | global batch size:  1024 | lm loss: 1.884925E+00 | loss scale: 1.0 | grad norm: 1.917 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      691/   14369 | consumed samples:       707584 | elapsed time per iteration (ms): 111506.9 | rate (tokens/sec): 18807.37 | learning rate: 1.036E-04 | global batch size:  1024 | lm loss: 1.905890E+00 | loss scale: 1.0 | grad norm: 2.237 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      692/   14369 | consumed samples:       708608 | elapsed time per iteration (ms): 111986.8 | rate (tokens/sec): 18726.79 | learning rate: 1.038E-04 | global batch size:  1024 | lm loss: 1.863946E+00 | loss scale: 1.0 | grad norm: 1.954 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      693/   14369 | consumed samples:       709632 | elapsed time per iteration (ms): 111987.0 | rate (tokens/sec): 18726.74 | learning rate: 1.039E-04 | global batch size:  1024 | lm loss: 1.892233E+00 | loss scale: 1.0 | grad norm: 3.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      694/   14369 | consumed samples:       710656 | elapsed time per iteration (ms): 112020.0 | rate (tokens/sec): 18721.22 | learning rate: 1.041E-04 | global batch size:  1024 | lm loss: 1.873477E+00 | loss scale: 1.0 | grad norm: 1.645 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      695/   14369 | consumed samples:       711680 | elapsed time per iteration (ms): 112964.0 | rate (tokens/sec): 18564.78 | learning rate: 1.042E-04 | global batch size:  1024 | lm loss: 1.886871E+00 | loss scale: 1.0 | grad norm: 2.142 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      696/   14369 | consumed samples:       712704 | elapsed time per iteration (ms): 112425.2 | rate (tokens/sec): 18653.75 | learning rate: 1.044E-04 | global batch size:  1024 | lm loss: 1.908822E+00 | loss scale: 1.0 | grad norm: 1.915 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      697/   14369 | consumed samples:       713728 | elapsed time per iteration (ms): 113402.6 | rate (tokens/sec): 18492.99 | learning rate: 1.045E-04 | global batch size:  1024 | lm loss: 1.890295E+00 | loss scale: 1.0 | grad norm: 2.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      698/   14369 | consumed samples:       714752 | elapsed time per iteration (ms): 111357.9 | rate (tokens/sec): 18832.54 | learning rate: 1.047E-04 | global batch size:  1024 | lm loss: 1.915759E+00 | loss scale: 1.0 | grad norm: 2.158 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      699/   14369 | consumed samples:       715776 | elapsed time per iteration (ms): 111501.7 | rate (tokens/sec): 18808.25 | learning rate: 1.048E-04 | global batch size:  1024 | lm loss: 1.884532E+00 | loss scale: 1.0 | grad norm: 2.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      700/   14369 | consumed samples:       716800 | elapsed time per iteration (ms): 111232.7 | rate (tokens/sec): 18853.74 | learning rate: 1.050E-04 | global batch size:  1024 | lm loss: 1.896976E+00 | loss scale: 1.0 | grad norm: 2.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      701/   14369 | consumed samples:       717824 | elapsed time per iteration (ms): 111519.9 | rate (tokens/sec): 18805.18 | learning rate: 1.051E-04 | global batch size:  1024 | lm loss: 1.874554E+00 | loss scale: 1.0 | grad norm: 1.993 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      702/   14369 | consumed samples:       718848 | elapsed time per iteration (ms): 110841.7 | rate (tokens/sec): 18920.25 | learning rate: 1.053E-04 | global batch size:  1024 | lm loss: 1.897026E+00 | loss scale: 1.0 | grad norm: 2.442 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      703/   14369 | consumed samples:       719872 | elapsed time per iteration (ms): 112203.7 | rate (tokens/sec): 18690.58 | learning rate: 1.054E-04 | global batch size:  1024 | lm loss: 1.892112E+00 | loss scale: 1.0 | grad norm: 2.121 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      704/   14369 | consumed samples:       720896 | elapsed time per iteration (ms): 111519.9 | rate (tokens/sec): 18805.18 | learning rate: 1.056E-04 | global batch size:  1024 | lm loss: 1.890957E+00 | loss scale: 1.0 | grad norm: 2.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      705/   14369 | consumed samples:       721920 | elapsed time per iteration (ms): 111881.7 | rate (tokens/sec): 18744.36 | learning rate: 1.057E-04 | global batch size:  1024 | lm loss: 1.857182E+00 | loss scale: 1.0 | grad norm: 1.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      706/   14369 | consumed samples:       722944 | elapsed time per iteration (ms): 111522.9 | rate (tokens/sec): 18804.68 | learning rate: 1.059E-04 | global batch size:  1024 | lm loss: 1.886076E+00 | loss scale: 1.0 | grad norm: 2.096 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      707/   14369 | consumed samples:       723968 | elapsed time per iteration (ms): 111306.0 | rate (tokens/sec): 18841.32 | learning rate: 1.060E-04 | global batch size:  1024 | lm loss: 1.890590E+00 | loss scale: 1.0 | grad norm: 2.858 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      708/   14369 | consumed samples:       724992 | elapsed time per iteration (ms): 112294.5 | rate (tokens/sec): 18675.47 | learning rate: 1.062E-04 | global batch size:  1024 | lm loss: 1.874155E+00 | loss scale: 1.0 | grad norm: 2.103 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      709/   14369 | consumed samples:       726016 | elapsed time per iteration (ms): 112623.8 | rate (tokens/sec): 18620.86 | learning rate: 1.063E-04 | global batch size:  1024 | lm loss: 1.902907E+00 | loss scale: 1.0 | grad norm: 2.782 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      710/   14369 | consumed samples:       727040 | elapsed time per iteration (ms): 111898.7 | rate (tokens/sec): 18741.52 | learning rate: 1.065E-04 | global batch size:  1024 | lm loss: 1.892346E+00 | loss scale: 1.0 | grad norm: 2.174 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      711/   14369 | consumed samples:       728064 | elapsed time per iteration (ms): 112381.7 | rate (tokens/sec): 18660.97 | learning rate: 1.066E-04 | global batch size:  1024 | lm loss: 1.901353E+00 | loss scale: 1.0 | grad norm: 2.721 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      712/   14369 | consumed samples:       729088 | elapsed time per iteration (ms): 112421.7 | rate (tokens/sec): 18654.33 | learning rate: 1.068E-04 | global batch size:  1024 | lm loss: 1.872371E+00 | loss scale: 1.0 | grad norm: 1.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      713/   14369 | consumed samples:       730112 | elapsed time per iteration (ms): 112184.3 | rate (tokens/sec): 18693.81 | learning rate: 1.069E-04 | global batch size:  1024 | lm loss: 1.879459E+00 | loss scale: 1.0 | grad norm: 2.594 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      714/   14369 | consumed samples:       731136 | elapsed time per iteration (ms): 112761.9 | rate (tokens/sec): 18598.05 | learning rate: 1.071E-04 | global batch size:  1024 | lm loss: 1.886045E+00 | loss scale: 1.0 | grad norm: 1.953 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      715/   14369 | consumed samples:       732160 | elapsed time per iteration (ms): 111098.8 | rate (tokens/sec): 18876.45 | learning rate: 1.072E-04 | global batch size:  1024 | lm loss: 1.872988E+00 | loss scale: 1.0 | grad norm: 2.610 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      716/   14369 | consumed samples:       733184 | elapsed time per iteration (ms): 111381.3 | rate (tokens/sec): 18828.58 | learning rate: 1.074E-04 | global batch size:  1024 | lm loss: 1.862380E+00 | loss scale: 1.0 | grad norm: 2.043 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      717/   14369 | consumed samples:       734208 | elapsed time per iteration (ms): 111687.7 | rate (tokens/sec): 18776.93 | learning rate: 1.075E-04 | global batch size:  1024 | lm loss: 1.910863E+00 | loss scale: 1.0 | grad norm: 2.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      718/   14369 | consumed samples:       735232 | elapsed time per iteration (ms): 111440.7 | rate (tokens/sec): 18818.55 | learning rate: 1.077E-04 | global batch size:  1024 | lm loss: 1.873228E+00 | loss scale: 1.0 | grad norm: 2.403 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      719/   14369 | consumed samples:       736256 | elapsed time per iteration (ms): 110261.8 | rate (tokens/sec): 19019.76 | learning rate: 1.078E-04 | global batch size:  1024 | lm loss: 1.860889E+00 | loss scale: 1.0 | grad norm: 1.834 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      720/   14369 | consumed samples:       737280 | elapsed time per iteration (ms): 111211.9 | rate (tokens/sec): 18857.27 | learning rate: 1.080E-04 | global batch size:  1024 | lm loss: 1.851517E+00 | loss scale: 1.0 | grad norm: 2.217 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      721/   14369 | consumed samples:       738304 | elapsed time per iteration (ms): 111842.9 | rate (tokens/sec): 18750.87 | learning rate: 1.081E-04 | global batch size:  1024 | lm loss: 1.868044E+00 | loss scale: 1.0 | grad norm: 2.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      722/   14369 | consumed samples:       739328 | elapsed time per iteration (ms): 111059.2 | rate (tokens/sec): 18883.20 | learning rate: 1.083E-04 | global batch size:  1024 | lm loss: 1.872962E+00 | loss scale: 1.0 | grad norm: 2.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      723/   14369 | consumed samples:       740352 | elapsed time per iteration (ms): 113739.3 | rate (tokens/sec): 18438.24 | learning rate: 1.084E-04 | global batch size:  1024 | lm loss: 1.887075E+00 | loss scale: 1.0 | grad norm: 2.261 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      724/   14369 | consumed samples:       741376 | elapsed time per iteration (ms): 112264.1 | rate (tokens/sec): 18680.53 | learning rate: 1.086E-04 | global batch size:  1024 | lm loss: 1.896307E+00 | loss scale: 1.0 | grad norm: 2.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      725/   14369 | consumed samples:       742400 | elapsed time per iteration (ms): 111168.7 | rate (tokens/sec): 18864.59 | learning rate: 1.087E-04 | global batch size:  1024 | lm loss: 1.900420E+00 | loss scale: 1.0 | grad norm: 2.287 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      726/   14369 | consumed samples:       743424 | elapsed time per iteration (ms): 112740.0 | rate (tokens/sec): 18601.66 | learning rate: 1.089E-04 | global batch size:  1024 | lm loss: 1.896691E+00 | loss scale: 1.0 | grad norm: 1.829 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      727/   14369 | consumed samples:       744448 | elapsed time per iteration (ms): 112359.9 | rate (tokens/sec): 18664.60 | learning rate: 1.090E-04 | global batch size:  1024 | lm loss: 1.859457E+00 | loss scale: 1.0 | grad norm: 2.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      728/   14369 | consumed samples:       745472 | elapsed time per iteration (ms): 112099.9 | rate (tokens/sec): 18707.89 | learning rate: 1.092E-04 | global batch size:  1024 | lm loss: 1.904668E+00 | loss scale: 1.0 | grad norm: 2.254 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      729/   14369 | consumed samples:       746496 | elapsed time per iteration (ms): 112321.7 | rate (tokens/sec): 18670.94 | learning rate: 1.093E-04 | global batch size:  1024 | lm loss: 1.877098E+00 | loss scale: 1.0 | grad norm: 2.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      730/   14369 | consumed samples:       747520 | elapsed time per iteration (ms): 111666.3 | rate (tokens/sec): 18780.52 | learning rate: 1.095E-04 | global batch size:  1024 | lm loss: 1.860109E+00 | loss scale: 1.0 | grad norm: 2.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      731/   14369 | consumed samples:       748544 | elapsed time per iteration (ms): 113310.4 | rate (tokens/sec): 18508.03 | learning rate: 1.096E-04 | global batch size:  1024 | lm loss: 1.879047E+00 | loss scale: 1.0 | grad norm: 2.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      732/   14369 | consumed samples:       749568 | elapsed time per iteration (ms): 111931.9 | rate (tokens/sec): 18735.96 | learning rate: 1.098E-04 | global batch size:  1024 | lm loss: 1.866526E+00 | loss scale: 1.0 | grad norm: 2.558 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      733/   14369 | consumed samples:       750592 | elapsed time per iteration (ms): 110866.9 | rate (tokens/sec): 18915.94 | learning rate: 1.099E-04 | global batch size:  1024 | lm loss: 1.880480E+00 | loss scale: 1.0 | grad norm: 1.695 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      734/   14369 | consumed samples:       751616 | elapsed time per iteration (ms): 110729.0 | rate (tokens/sec): 18939.50 | learning rate: 1.101E-04 | global batch size:  1024 | lm loss: 1.872520E+00 | loss scale: 1.0 | grad norm: 2.617 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      735/   14369 | consumed samples:       752640 | elapsed time per iteration (ms): 111400.3 | rate (tokens/sec): 18825.37 | learning rate: 1.102E-04 | global batch size:  1024 | lm loss: 1.855929E+00 | loss scale: 1.0 | grad norm: 2.156 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      736/   14369 | consumed samples:       753664 | elapsed time per iteration (ms): 111879.9 | rate (tokens/sec): 18744.67 | learning rate: 1.104E-04 | global batch size:  1024 | lm loss: 1.868710E+00 | loss scale: 1.0 | grad norm: 2.051 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      737/   14369 | consumed samples:       754688 | elapsed time per iteration (ms): 111347.8 | rate (tokens/sec): 18834.25 | learning rate: 1.105E-04 | global batch size:  1024 | lm loss: 1.854215E+00 | loss scale: 1.0 | grad norm: 1.816 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      738/   14369 | consumed samples:       755712 | elapsed time per iteration (ms): 111795.0 | rate (tokens/sec): 18758.90 | learning rate: 1.107E-04 | global batch size:  1024 | lm loss: 1.894783E+00 | loss scale: 1.0 | grad norm: 3.100 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      739/   14369 | consumed samples:       756736 | elapsed time per iteration (ms): 111608.3 | rate (tokens/sec): 18790.29 | learning rate: 1.108E-04 | global batch size:  1024 | lm loss: 1.877997E+00 | loss scale: 1.0 | grad norm: 1.938 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      740/   14369 | consumed samples:       757760 | elapsed time per iteration (ms): 111327.0 | rate (tokens/sec): 18837.76 | learning rate: 1.110E-04 | global batch size:  1024 | lm loss: 1.858646E+00 | loss scale: 1.0 | grad norm: 2.288 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      741/   14369 | consumed samples:       758784 | elapsed time per iteration (ms): 111544.4 | rate (tokens/sec): 18801.06 | learning rate: 1.111E-04 | global batch size:  1024 | lm loss: 1.878759E+00 | loss scale: 1.0 | grad norm: 2.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      742/   14369 | consumed samples:       759808 | elapsed time per iteration (ms): 112241.7 | rate (tokens/sec): 18684.24 | learning rate: 1.113E-04 | global batch size:  1024 | lm loss: 1.864351E+00 | loss scale: 1.0 | grad norm: 2.233 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      743/   14369 | consumed samples:       760832 | elapsed time per iteration (ms): 111636.7 | rate (tokens/sec): 18785.51 | learning rate: 1.114E-04 | global batch size:  1024 | lm loss: 1.856639E+00 | loss scale: 1.0 | grad norm: 2.415 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      744/   14369 | consumed samples:       761856 | elapsed time per iteration (ms): 111261.9 | rate (tokens/sec): 18848.79 | learning rate: 1.116E-04 | global batch size:  1024 | lm loss: 1.865553E+00 | loss scale: 1.0 | grad norm: 1.915 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      745/   14369 | consumed samples:       762880 | elapsed time per iteration (ms): 112104.8 | rate (tokens/sec): 18707.08 | learning rate: 1.117E-04 | global batch size:  1024 | lm loss: 1.867728E+00 | loss scale: 1.0 | grad norm: 2.619 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      746/   14369 | consumed samples:       763904 | elapsed time per iteration (ms): 112421.7 | rate (tokens/sec): 18654.33 | learning rate: 1.119E-04 | global batch size:  1024 | lm loss: 1.890711E+00 | loss scale: 1.0 | grad norm: 2.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      747/   14369 | consumed samples:       764928 | elapsed time per iteration (ms): 111640.0 | rate (tokens/sec): 18784.95 | learning rate: 1.120E-04 | global batch size:  1024 | lm loss: 1.877310E+00 | loss scale: 1.0 | grad norm: 2.254 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      748/   14369 | consumed samples:       765952 | elapsed time per iteration (ms): 110952.9 | rate (tokens/sec): 18901.27 | learning rate: 1.122E-04 | global batch size:  1024 | lm loss: 1.874358E+00 | loss scale: 1.0 | grad norm: 2.421 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      749/   14369 | consumed samples:       766976 | elapsed time per iteration (ms): 112274.8 | rate (tokens/sec): 18678.74 | learning rate: 1.123E-04 | global batch size:  1024 | lm loss: 1.854611E+00 | loss scale: 1.0 | grad norm: 1.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      750/   14369 | consumed samples:       768000 | elapsed time per iteration (ms): 112057.9 | rate (tokens/sec): 18714.90 | learning rate: 1.125E-04 | global batch size:  1024 | lm loss: 1.875080E+00 | loss scale: 1.0 | grad norm: 2.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
 validation loss at iteration 750 | lm loss value: 2.067671E+00 | lm loss PPL: 7.906390E+00 | 
-----------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
 test loss at iteration 750 | lm loss value: 3.448144E+00 | lm loss PPL: 3.144199E+01 | 
-----------------------------------------------------------------------------------------
saving checkpoint at iteration     750 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
  successfully saved checkpoint at iteration     750 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
(min, max) time across ranks (ms):
    save-checkpoint ................................: (115473.88, 115484.95)
 iteration      751/   14369 | consumed samples:       769024 | elapsed time per iteration (ms): 1199006.9 | rate (tokens/sec): 1749.07 | learning rate: 1.126E-04 | global batch size:  1024 | lm loss: 1.847920E+00 | loss scale: 1.0 | grad norm: 2.312 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      752/   14369 | consumed samples:       770048 | elapsed time per iteration (ms): 110114.8 | rate (tokens/sec): 19045.13 | learning rate: 1.128E-04 | global batch size:  1024 | lm loss: 1.861608E+00 | loss scale: 1.0 | grad norm: 2.671 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      753/   14369 | consumed samples:       771072 | elapsed time per iteration (ms): 111552.6 | rate (tokens/sec): 18799.67 | learning rate: 1.129E-04 | global batch size:  1024 | lm loss: 1.841501E+00 | loss scale: 1.0 | grad norm: 1.470 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      754/   14369 | consumed samples:       772096 | elapsed time per iteration (ms): 110320.0 | rate (tokens/sec): 19009.71 | learning rate: 1.131E-04 | global batch size:  1024 | lm loss: 1.882143E+00 | loss scale: 1.0 | grad norm: 2.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      755/   14369 | consumed samples:       773120 | elapsed time per iteration (ms): 114359.7 | rate (tokens/sec): 18338.20 | learning rate: 1.132E-04 | global batch size:  1024 | lm loss: 1.864462E+00 | loss scale: 1.0 | grad norm: 2.562 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      756/   14369 | consumed samples:       774144 | elapsed time per iteration (ms): 115721.6 | rate (tokens/sec): 18122.40 | learning rate: 1.134E-04 | global batch size:  1024 | lm loss: 1.856299E+00 | loss scale: 1.0 | grad norm: 2.247 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      757/   14369 | consumed samples:       775168 | elapsed time per iteration (ms): 114103.8 | rate (tokens/sec): 18379.33 | learning rate: 1.135E-04 | global batch size:  1024 | lm loss: 1.856930E+00 | loss scale: 1.0 | grad norm: 2.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      758/   14369 | consumed samples:       776192 | elapsed time per iteration (ms): 112061.8 | rate (tokens/sec): 18714.24 | learning rate: 1.137E-04 | global batch size:  1024 | lm loss: 1.852989E+00 | loss scale: 1.0 | grad norm: 1.751 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      759/   14369 | consumed samples:       777216 | elapsed time per iteration (ms): 112763.2 | rate (tokens/sec): 18597.84 | learning rate: 1.138E-04 | global batch size:  1024 | lm loss: 1.848494E+00 | loss scale: 1.0 | grad norm: 2.187 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      760/   14369 | consumed samples:       778240 | elapsed time per iteration (ms): 112894.4 | rate (tokens/sec): 18576.22 | learning rate: 1.140E-04 | global batch size:  1024 | lm loss: 1.857415E+00 | loss scale: 1.0 | grad norm: 2.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      761/   14369 | consumed samples:       779264 | elapsed time per iteration (ms): 114020.7 | rate (tokens/sec): 18392.74 | learning rate: 1.141E-04 | global batch size:  1024 | lm loss: 1.874996E+00 | loss scale: 1.0 | grad norm: 2.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      762/   14369 | consumed samples:       780288 | elapsed time per iteration (ms): 111249.2 | rate (tokens/sec): 18850.94 | learning rate: 1.143E-04 | global batch size:  1024 | lm loss: 1.855792E+00 | loss scale: 1.0 | grad norm: 1.963 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      763/   14369 | consumed samples:       781312 | elapsed time per iteration (ms): 111978.9 | rate (tokens/sec): 18728.10 | learning rate: 1.144E-04 | global batch size:  1024 | lm loss: 1.872733E+00 | loss scale: 1.0 | grad norm: 2.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      764/   14369 | consumed samples:       782336 | elapsed time per iteration (ms): 111977.7 | rate (tokens/sec): 18728.30 | learning rate: 1.146E-04 | global batch size:  1024 | lm loss: 1.861359E+00 | loss scale: 1.0 | grad norm: 1.733 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      765/   14369 | consumed samples:       783360 | elapsed time per iteration (ms): 112486.3 | rate (tokens/sec): 18643.63 | learning rate: 1.147E-04 | global batch size:  1024 | lm loss: 1.862714E+00 | loss scale: 1.0 | grad norm: 2.656 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      766/   14369 | consumed samples:       784384 | elapsed time per iteration (ms): 111261.8 | rate (tokens/sec): 18848.80 | learning rate: 1.149E-04 | global batch size:  1024 | lm loss: 1.847475E+00 | loss scale: 1.0 | grad norm: 1.626 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      767/   14369 | consumed samples:       785408 | elapsed time per iteration (ms): 111715.0 | rate (tokens/sec): 18772.34 | learning rate: 1.150E-04 | global batch size:  1024 | lm loss: 1.874173E+00 | loss scale: 1.0 | grad norm: 2.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      768/   14369 | consumed samples:       786432 | elapsed time per iteration (ms): 112221.7 | rate (tokens/sec): 18687.58 | learning rate: 1.152E-04 | global batch size:  1024 | lm loss: 1.861165E+00 | loss scale: 1.0 | grad norm: 1.777 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      769/   14369 | consumed samples:       787456 | elapsed time per iteration (ms): 112554.8 | rate (tokens/sec): 18632.28 | learning rate: 1.153E-04 | global batch size:  1024 | lm loss: 1.854667E+00 | loss scale: 1.0 | grad norm: 2.032 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      770/   14369 | consumed samples:       788480 | elapsed time per iteration (ms): 111908.7 | rate (tokens/sec): 18739.85 | learning rate: 1.155E-04 | global batch size:  1024 | lm loss: 1.870323E+00 | loss scale: 1.0 | grad norm: 2.197 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      771/   14369 | consumed samples:       789504 | elapsed time per iteration (ms): 111925.8 | rate (tokens/sec): 18736.98 | learning rate: 1.156E-04 | global batch size:  1024 | lm loss: 1.892215E+00 | loss scale: 1.0 | grad norm: 2.080 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      772/   14369 | consumed samples:       790528 | elapsed time per iteration (ms): 111961.6 | rate (tokens/sec): 18730.99 | learning rate: 1.158E-04 | global batch size:  1024 | lm loss: 1.862211E+00 | loss scale: 1.0 | grad norm: 2.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      773/   14369 | consumed samples:       791552 | elapsed time per iteration (ms): 113519.0 | rate (tokens/sec): 18474.01 | learning rate: 1.159E-04 | global batch size:  1024 | lm loss: 1.850092E+00 | loss scale: 1.0 | grad norm: 2.091 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      774/   14369 | consumed samples:       792576 | elapsed time per iteration (ms): 112227.2 | rate (tokens/sec): 18686.67 | learning rate: 1.161E-04 | global batch size:  1024 | lm loss: 1.858130E+00 | loss scale: 1.0 | grad norm: 2.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      775/   14369 | consumed samples:       793600 | elapsed time per iteration (ms): 112819.9 | rate (tokens/sec): 18588.49 | learning rate: 1.162E-04 | global batch size:  1024 | lm loss: 1.870670E+00 | loss scale: 1.0 | grad norm: 2.206 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      776/   14369 | consumed samples:       794624 | elapsed time per iteration (ms): 111512.9 | rate (tokens/sec): 18806.36 | learning rate: 1.164E-04 | global batch size:  1024 | lm loss: 1.845119E+00 | loss scale: 1.0 | grad norm: 2.536 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      777/   14369 | consumed samples:       795648 | elapsed time per iteration (ms): 111680.8 | rate (tokens/sec): 18778.08 | learning rate: 1.165E-04 | global batch size:  1024 | lm loss: 1.845269E+00 | loss scale: 1.0 | grad norm: 2.059 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      778/   14369 | consumed samples:       796672 | elapsed time per iteration (ms): 112040.7 | rate (tokens/sec): 18717.76 | learning rate: 1.167E-04 | global batch size:  1024 | lm loss: 1.843458E+00 | loss scale: 1.0 | grad norm: 2.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      779/   14369 | consumed samples:       797696 | elapsed time per iteration (ms): 111841.8 | rate (tokens/sec): 18751.05 | learning rate: 1.168E-04 | global batch size:  1024 | lm loss: 1.862991E+00 | loss scale: 1.0 | grad norm: 1.707 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      780/   14369 | consumed samples:       798720 | elapsed time per iteration (ms): 112621.5 | rate (tokens/sec): 18621.23 | learning rate: 1.170E-04 | global batch size:  1024 | lm loss: 1.840657E+00 | loss scale: 1.0 | grad norm: 2.561 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      781/   14369 | consumed samples:       799744 | elapsed time per iteration (ms): 111694.3 | rate (tokens/sec): 18775.82 | learning rate: 1.171E-04 | global batch size:  1024 | lm loss: 1.852200E+00 | loss scale: 1.0 | grad norm: 1.998 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      782/   14369 | consumed samples:       800768 | elapsed time per iteration (ms): 111781.7 | rate (tokens/sec): 18761.14 | learning rate: 1.173E-04 | global batch size:  1024 | lm loss: 1.845897E+00 | loss scale: 1.0 | grad norm: 2.402 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      783/   14369 | consumed samples:       801792 | elapsed time per iteration (ms): 111080.0 | rate (tokens/sec): 18879.65 | learning rate: 1.174E-04 | global batch size:  1024 | lm loss: 1.864460E+00 | loss scale: 1.0 | grad norm: 1.964 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      784/   14369 | consumed samples:       802816 | elapsed time per iteration (ms): 111179.9 | rate (tokens/sec): 18862.69 | learning rate: 1.176E-04 | global batch size:  1024 | lm loss: 1.851338E+00 | loss scale: 1.0 | grad norm: 1.908 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      785/   14369 | consumed samples:       803840 | elapsed time per iteration (ms): 112195.4 | rate (tokens/sec): 18691.95 | learning rate: 1.177E-04 | global batch size:  1024 | lm loss: 1.861120E+00 | loss scale: 1.0 | grad norm: 2.522 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      786/   14369 | consumed samples:       804864 | elapsed time per iteration (ms): 113079.9 | rate (tokens/sec): 18545.75 | learning rate: 1.179E-04 | global batch size:  1024 | lm loss: 1.874848E+00 | loss scale: 1.0 | grad norm: 1.610 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      787/   14369 | consumed samples:       805888 | elapsed time per iteration (ms): 111381.8 | rate (tokens/sec): 18828.50 | learning rate: 1.180E-04 | global batch size:  1024 | lm loss: 1.865177E+00 | loss scale: 1.0 | grad norm: 2.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      788/   14369 | consumed samples:       806912 | elapsed time per iteration (ms): 111186.9 | rate (tokens/sec): 18861.51 | learning rate: 1.182E-04 | global batch size:  1024 | lm loss: 1.849850E+00 | loss scale: 1.0 | grad norm: 1.748 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      789/   14369 | consumed samples:       807936 | elapsed time per iteration (ms): 111731.1 | rate (tokens/sec): 18769.64 | learning rate: 1.183E-04 | global batch size:  1024 | lm loss: 1.855498E+00 | loss scale: 1.0 | grad norm: 2.849 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      790/   14369 | consumed samples:       808960 | elapsed time per iteration (ms): 111471.1 | rate (tokens/sec): 18813.42 | learning rate: 1.185E-04 | global batch size:  1024 | lm loss: 1.856036E+00 | loss scale: 1.0 | grad norm: 1.386 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      791/   14369 | consumed samples:       809984 | elapsed time per iteration (ms): 110847.1 | rate (tokens/sec): 18919.31 | learning rate: 1.186E-04 | global batch size:  1024 | lm loss: 1.845382E+00 | loss scale: 1.0 | grad norm: 2.600 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      792/   14369 | consumed samples:       811008 | elapsed time per iteration (ms): 111864.7 | rate (tokens/sec): 18747.22 | learning rate: 1.188E-04 | global batch size:  1024 | lm loss: 1.864366E+00 | loss scale: 1.0 | grad norm: 1.946 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      793/   14369 | consumed samples:       812032 | elapsed time per iteration (ms): 113206.9 | rate (tokens/sec): 18524.95 | learning rate: 1.189E-04 | global batch size:  1024 | lm loss: 1.854386E+00 | loss scale: 1.0 | grad norm: 2.568 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      794/   14369 | consumed samples:       813056 | elapsed time per iteration (ms): 111567.0 | rate (tokens/sec): 18797.24 | learning rate: 1.191E-04 | global batch size:  1024 | lm loss: 1.869259E+00 | loss scale: 1.0 | grad norm: 1.784 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      795/   14369 | consumed samples:       814080 | elapsed time per iteration (ms): 112006.5 | rate (tokens/sec): 18723.48 | learning rate: 1.192E-04 | global batch size:  1024 | lm loss: 1.855292E+00 | loss scale: 1.0 | grad norm: 2.208 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      796/   14369 | consumed samples:       815104 | elapsed time per iteration (ms): 111507.0 | rate (tokens/sec): 18807.35 | learning rate: 1.194E-04 | global batch size:  1024 | lm loss: 1.860957E+00 | loss scale: 1.0 | grad norm: 2.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      797/   14369 | consumed samples:       816128 | elapsed time per iteration (ms): 111521.8 | rate (tokens/sec): 18804.85 | learning rate: 1.195E-04 | global batch size:  1024 | lm loss: 1.855064E+00 | loss scale: 1.0 | grad norm: 2.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      798/   14369 | consumed samples:       817152 | elapsed time per iteration (ms): 111328.9 | rate (tokens/sec): 18837.45 | learning rate: 1.197E-04 | global batch size:  1024 | lm loss: 1.854455E+00 | loss scale: 1.0 | grad norm: 1.703 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      799/   14369 | consumed samples:       818176 | elapsed time per iteration (ms): 111283.0 | rate (tokens/sec): 18845.21 | learning rate: 1.198E-04 | global batch size:  1024 | lm loss: 1.865783E+00 | loss scale: 1.0 | grad norm: 2.108 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      800/   14369 | consumed samples:       819200 | elapsed time per iteration (ms): 111704.8 | rate (tokens/sec): 18774.05 | learning rate: 1.200E-04 | global batch size:  1024 | lm loss: 1.865003E+00 | loss scale: 1.0 | grad norm: 2.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      801/   14369 | consumed samples:       820224 | elapsed time per iteration (ms): 111113.0 | rate (tokens/sec): 18874.05 | learning rate: 1.201E-04 | global batch size:  1024 | lm loss: 1.862900E+00 | loss scale: 1.0 | grad norm: 2.084 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      802/   14369 | consumed samples:       821248 | elapsed time per iteration (ms): 110819.9 | rate (tokens/sec): 18923.97 | learning rate: 1.203E-04 | global batch size:  1024 | lm loss: 1.806857E+00 | loss scale: 1.0 | grad norm: 2.212 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      803/   14369 | consumed samples:       822272 | elapsed time per iteration (ms): 110351.8 | rate (tokens/sec): 19004.24 | learning rate: 1.204E-04 | global batch size:  1024 | lm loss: 1.814375E+00 | loss scale: 1.0 | grad norm: 1.992 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      804/   14369 | consumed samples:       823296 | elapsed time per iteration (ms): 111813.0 | rate (tokens/sec): 18755.88 | learning rate: 1.206E-04 | global batch size:  1024 | lm loss: 1.846178E+00 | loss scale: 1.0 | grad norm: 2.350 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      805/   14369 | consumed samples:       824320 | elapsed time per iteration (ms): 111746.9 | rate (tokens/sec): 18766.98 | learning rate: 1.207E-04 | global batch size:  1024 | lm loss: 1.856831E+00 | loss scale: 1.0 | grad norm: 1.754 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      806/   14369 | consumed samples:       825344 | elapsed time per iteration (ms): 111574.7 | rate (tokens/sec): 18795.95 | learning rate: 1.209E-04 | global batch size:  1024 | lm loss: 1.834311E+00 | loss scale: 1.0 | grad norm: 2.090 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      807/   14369 | consumed samples:       826368 | elapsed time per iteration (ms): 112600.0 | rate (tokens/sec): 18624.79 | learning rate: 1.210E-04 | global batch size:  1024 | lm loss: 1.861110E+00 | loss scale: 1.0 | grad norm: 2.060 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      808/   14369 | consumed samples:       827392 | elapsed time per iteration (ms): 112955.9 | rate (tokens/sec): 18566.11 | learning rate: 1.212E-04 | global batch size:  1024 | lm loss: 1.850563E+00 | loss scale: 1.0 | grad norm: 1.952 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      809/   14369 | consumed samples:       828416 | elapsed time per iteration (ms): 113378.9 | rate (tokens/sec): 18496.85 | learning rate: 1.213E-04 | global batch size:  1024 | lm loss: 1.853927E+00 | loss scale: 1.0 | grad norm: 2.143 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      810/   14369 | consumed samples:       829440 | elapsed time per iteration (ms): 110782.9 | rate (tokens/sec): 18930.28 | learning rate: 1.215E-04 | global batch size:  1024 | lm loss: 1.828221E+00 | loss scale: 1.0 | grad norm: 1.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      811/   14369 | consumed samples:       830464 | elapsed time per iteration (ms): 111141.7 | rate (tokens/sec): 18869.18 | learning rate: 1.216E-04 | global batch size:  1024 | lm loss: 1.836618E+00 | loss scale: 1.0 | grad norm: 2.324 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      812/   14369 | consumed samples:       831488 | elapsed time per iteration (ms): 110014.8 | rate (tokens/sec): 19062.45 | learning rate: 1.218E-04 | global batch size:  1024 | lm loss: 1.835477E+00 | loss scale: 1.0 | grad norm: 1.769 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      813/   14369 | consumed samples:       832512 | elapsed time per iteration (ms): 110825.9 | rate (tokens/sec): 18922.94 | learning rate: 1.219E-04 | global batch size:  1024 | lm loss: 1.821418E+00 | loss scale: 1.0 | grad norm: 2.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      814/   14369 | consumed samples:       833536 | elapsed time per iteration (ms): 112334.8 | rate (tokens/sec): 18668.76 | learning rate: 1.221E-04 | global batch size:  1024 | lm loss: 1.816925E+00 | loss scale: 1.0 | grad norm: 1.899 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      815/   14369 | consumed samples:       834560 | elapsed time per iteration (ms): 111686.9 | rate (tokens/sec): 18777.06 | learning rate: 1.222E-04 | global batch size:  1024 | lm loss: 1.859578E+00 | loss scale: 1.0 | grad norm: 2.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      816/   14369 | consumed samples:       835584 | elapsed time per iteration (ms): 112010.7 | rate (tokens/sec): 18722.79 | learning rate: 1.224E-04 | global batch size:  1024 | lm loss: 1.814651E+00 | loss scale: 1.0 | grad norm: 1.725 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      817/   14369 | consumed samples:       836608 | elapsed time per iteration (ms): 111774.6 | rate (tokens/sec): 18762.33 | learning rate: 1.225E-04 | global batch size:  1024 | lm loss: 1.839242E+00 | loss scale: 1.0 | grad norm: 2.453 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      818/   14369 | consumed samples:       837632 | elapsed time per iteration (ms): 110481.7 | rate (tokens/sec): 18981.89 | learning rate: 1.227E-04 | global batch size:  1024 | lm loss: 1.822998E+00 | loss scale: 1.0 | grad norm: 1.844 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      819/   14369 | consumed samples:       838656 | elapsed time per iteration (ms): 114317.7 | rate (tokens/sec): 18344.94 | learning rate: 1.228E-04 | global batch size:  1024 | lm loss: 1.835759E+00 | loss scale: 1.0 | grad norm: 2.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      820/   14369 | consumed samples:       839680 | elapsed time per iteration (ms): 112981.9 | rate (tokens/sec): 18561.84 | learning rate: 1.230E-04 | global batch size:  1024 | lm loss: 1.832488E+00 | loss scale: 1.0 | grad norm: 1.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      821/   14369 | consumed samples:       840704 | elapsed time per iteration (ms): 111681.9 | rate (tokens/sec): 18777.90 | learning rate: 1.231E-04 | global batch size:  1024 | lm loss: 1.833628E+00 | loss scale: 1.0 | grad norm: 2.064 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      822/   14369 | consumed samples:       841728 | elapsed time per iteration (ms): 111631.5 | rate (tokens/sec): 18786.38 | learning rate: 1.233E-04 | global batch size:  1024 | lm loss: 1.842131E+00 | loss scale: 1.0 | grad norm: 2.142 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      823/   14369 | consumed samples:       842752 | elapsed time per iteration (ms): 112500.0 | rate (tokens/sec): 18641.35 | learning rate: 1.234E-04 | global batch size:  1024 | lm loss: 1.804957E+00 | loss scale: 1.0 | grad norm: 1.772 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      824/   14369 | consumed samples:       843776 | elapsed time per iteration (ms): 113721.7 | rate (tokens/sec): 18441.08 | learning rate: 1.236E-04 | global batch size:  1024 | lm loss: 1.833142E+00 | loss scale: 1.0 | grad norm: 2.669 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      825/   14369 | consumed samples:       844800 | elapsed time per iteration (ms): 112343.0 | rate (tokens/sec): 18667.40 | learning rate: 1.237E-04 | global batch size:  1024 | lm loss: 1.809695E+00 | loss scale: 1.0 | grad norm: 1.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      826/   14369 | consumed samples:       845824 | elapsed time per iteration (ms): 111441.9 | rate (tokens/sec): 18818.35 | learning rate: 1.239E-04 | global batch size:  1024 | lm loss: 1.855332E+00 | loss scale: 1.0 | grad norm: 2.892 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      827/   14369 | consumed samples:       846848 | elapsed time per iteration (ms): 112681.9 | rate (tokens/sec): 18611.26 | learning rate: 1.241E-04 | global batch size:  1024 | lm loss: 1.851431E+00 | loss scale: 1.0 | grad norm: 2.087 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      828/   14369 | consumed samples:       847872 | elapsed time per iteration (ms): 112257.7 | rate (tokens/sec): 18681.58 | learning rate: 1.242E-04 | global batch size:  1024 | lm loss: 1.853221E+00 | loss scale: 1.0 | grad norm: 2.148 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      829/   14369 | consumed samples:       848896 | elapsed time per iteration (ms): 111686.9 | rate (tokens/sec): 18777.06 | learning rate: 1.243E-04 | global batch size:  1024 | lm loss: 1.827985E+00 | loss scale: 1.0 | grad norm: 2.143 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      830/   14369 | consumed samples:       849920 | elapsed time per iteration (ms): 111489.4 | rate (tokens/sec): 18810.33 | learning rate: 1.245E-04 | global batch size:  1024 | lm loss: 1.850124E+00 | loss scale: 1.0 | grad norm: 2.136 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      831/   14369 | consumed samples:       850944 | elapsed time per iteration (ms): 112078.9 | rate (tokens/sec): 18711.39 | learning rate: 1.246E-04 | global batch size:  1024 | lm loss: 1.832603E+00 | loss scale: 1.0 | grad norm: 2.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      832/   14369 | consumed samples:       851968 | elapsed time per iteration (ms): 112425.0 | rate (tokens/sec): 18653.78 | learning rate: 1.248E-04 | global batch size:  1024 | lm loss: 1.861077E+00 | loss scale: 1.0 | grad norm: 1.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      833/   14369 | consumed samples:       852992 | elapsed time per iteration (ms): 112414.8 | rate (tokens/sec): 18655.47 | learning rate: 1.250E-04 | global batch size:  1024 | lm loss: 1.811758E+00 | loss scale: 1.0 | grad norm: 2.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      834/   14369 | consumed samples:       854016 | elapsed time per iteration (ms): 112180.0 | rate (tokens/sec): 18694.52 | learning rate: 1.251E-04 | global batch size:  1024 | lm loss: 1.820873E+00 | loss scale: 1.0 | grad norm: 1.819 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      835/   14369 | consumed samples:       855040 | elapsed time per iteration (ms): 112241.7 | rate (tokens/sec): 18684.24 | learning rate: 1.253E-04 | global batch size:  1024 | lm loss: 1.822044E+00 | loss scale: 1.0 | grad norm: 1.998 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      836/   14369 | consumed samples:       856064 | elapsed time per iteration (ms): 111725.0 | rate (tokens/sec): 18770.66 | learning rate: 1.254E-04 | global batch size:  1024 | lm loss: 1.817640E+00 | loss scale: 1.0 | grad norm: 2.037 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      837/   14369 | consumed samples:       857088 | elapsed time per iteration (ms): 112103.9 | rate (tokens/sec): 18707.22 | learning rate: 1.255E-04 | global batch size:  1024 | lm loss: 1.813284E+00 | loss scale: 1.0 | grad norm: 2.113 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      838/   14369 | consumed samples:       858112 | elapsed time per iteration (ms): 112796.9 | rate (tokens/sec): 18592.29 | learning rate: 1.257E-04 | global batch size:  1024 | lm loss: 1.836434E+00 | loss scale: 1.0 | grad norm: 2.322 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      839/   14369 | consumed samples:       859136 | elapsed time per iteration (ms): 111423.7 | rate (tokens/sec): 18821.42 | learning rate: 1.258E-04 | global batch size:  1024 | lm loss: 1.832791E+00 | loss scale: 1.0 | grad norm: 1.956 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      840/   14369 | consumed samples:       860160 | elapsed time per iteration (ms): 111728.8 | rate (tokens/sec): 18770.02 | learning rate: 1.260E-04 | global batch size:  1024 | lm loss: 1.820364E+00 | loss scale: 1.0 | grad norm: 2.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      841/   14369 | consumed samples:       861184 | elapsed time per iteration (ms): 112173.0 | rate (tokens/sec): 18695.69 | learning rate: 1.261E-04 | global batch size:  1024 | lm loss: 1.827168E+00 | loss scale: 1.0 | grad norm: 2.130 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      842/   14369 | consumed samples:       862208 | elapsed time per iteration (ms): 112746.9 | rate (tokens/sec): 18600.53 | learning rate: 1.263E-04 | global batch size:  1024 | lm loss: 1.816097E+00 | loss scale: 1.0 | grad norm: 2.051 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      843/   14369 | consumed samples:       863232 | elapsed time per iteration (ms): 112136.4 | rate (tokens/sec): 18701.80 | learning rate: 1.264E-04 | global batch size:  1024 | lm loss: 1.829004E+00 | loss scale: 1.0 | grad norm: 1.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      844/   14369 | consumed samples:       864256 | elapsed time per iteration (ms): 111899.9 | rate (tokens/sec): 18741.32 | learning rate: 1.266E-04 | global batch size:  1024 | lm loss: 1.810902E+00 | loss scale: 1.0 | grad norm: 2.174 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      845/   14369 | consumed samples:       865280 | elapsed time per iteration (ms): 111384.9 | rate (tokens/sec): 18827.97 | learning rate: 1.267E-04 | global batch size:  1024 | lm loss: 1.809490E+00 | loss scale: 1.0 | grad norm: 1.791 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      846/   14369 | consumed samples:       866304 | elapsed time per iteration (ms): 110895.5 | rate (tokens/sec): 18911.07 | learning rate: 1.269E-04 | global batch size:  1024 | lm loss: 1.811128E+00 | loss scale: 1.0 | grad norm: 2.155 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      847/   14369 | consumed samples:       867328 | elapsed time per iteration (ms): 111475.9 | rate (tokens/sec): 18812.61 | learning rate: 1.270E-04 | global batch size:  1024 | lm loss: 1.837692E+00 | loss scale: 1.0 | grad norm: 2.194 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      848/   14369 | consumed samples:       868352 | elapsed time per iteration (ms): 111279.7 | rate (tokens/sec): 18845.77 | learning rate: 1.272E-04 | global batch size:  1024 | lm loss: 1.820072E+00 | loss scale: 1.0 | grad norm: 1.881 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      849/   14369 | consumed samples:       869376 | elapsed time per iteration (ms): 111065.8 | rate (tokens/sec): 18882.06 | learning rate: 1.273E-04 | global batch size:  1024 | lm loss: 1.825833E+00 | loss scale: 1.0 | grad norm: 2.272 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      850/   14369 | consumed samples:       870400 | elapsed time per iteration (ms): 112263.1 | rate (tokens/sec): 18680.69 | learning rate: 1.275E-04 | global batch size:  1024 | lm loss: 1.840907E+00 | loss scale: 1.0 | grad norm: 1.729 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      851/   14369 | consumed samples:       871424 | elapsed time per iteration (ms): 111561.7 | rate (tokens/sec): 18798.13 | learning rate: 1.276E-04 | global batch size:  1024 | lm loss: 1.832107E+00 | loss scale: 1.0 | grad norm: 2.318 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      852/   14369 | consumed samples:       872448 | elapsed time per iteration (ms): 111424.7 | rate (tokens/sec): 18821.24 | learning rate: 1.278E-04 | global batch size:  1024 | lm loss: 1.811173E+00 | loss scale: 1.0 | grad norm: 1.922 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      853/   14369 | consumed samples:       873472 | elapsed time per iteration (ms): 110903.8 | rate (tokens/sec): 18909.64 | learning rate: 1.279E-04 | global batch size:  1024 | lm loss: 1.828108E+00 | loss scale: 1.0 | grad norm: 2.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      854/   14369 | consumed samples:       874496 | elapsed time per iteration (ms): 113167.2 | rate (tokens/sec): 18531.46 | learning rate: 1.281E-04 | global batch size:  1024 | lm loss: 1.837719E+00 | loss scale: 1.0 | grad norm: 1.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      855/   14369 | consumed samples:       875520 | elapsed time per iteration (ms): 113047.0 | rate (tokens/sec): 18551.15 | learning rate: 1.282E-04 | global batch size:  1024 | lm loss: 1.815867E+00 | loss scale: 1.0 | grad norm: 2.267 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      856/   14369 | consumed samples:       876544 | elapsed time per iteration (ms): 111714.3 | rate (tokens/sec): 18772.45 | learning rate: 1.284E-04 | global batch size:  1024 | lm loss: 1.833227E+00 | loss scale: 1.0 | grad norm: 2.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      857/   14369 | consumed samples:       877568 | elapsed time per iteration (ms): 112241.8 | rate (tokens/sec): 18684.23 | learning rate: 1.286E-04 | global batch size:  1024 | lm loss: 1.849485E+00 | loss scale: 1.0 | grad norm: 2.057 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      858/   14369 | consumed samples:       878592 | elapsed time per iteration (ms): 111646.8 | rate (tokens/sec): 18783.81 | learning rate: 1.287E-04 | global batch size:  1024 | lm loss: 1.807119E+00 | loss scale: 1.0 | grad norm: 1.976 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      859/   14369 | consumed samples:       879616 | elapsed time per iteration (ms): 110898.0 | rate (tokens/sec): 18910.64 | learning rate: 1.288E-04 | global batch size:  1024 | lm loss: 1.825508E+00 | loss scale: 1.0 | grad norm: 1.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      860/   14369 | consumed samples:       880640 | elapsed time per iteration (ms): 111092.8 | rate (tokens/sec): 18877.48 | learning rate: 1.290E-04 | global batch size:  1024 | lm loss: 1.807578E+00 | loss scale: 1.0 | grad norm: 1.754 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      861/   14369 | consumed samples:       881664 | elapsed time per iteration (ms): 111794.2 | rate (tokens/sec): 18759.04 | learning rate: 1.291E-04 | global batch size:  1024 | lm loss: 1.824632E+00 | loss scale: 1.0 | grad norm: 2.738 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      862/   14369 | consumed samples:       882688 | elapsed time per iteration (ms): 112021.7 | rate (tokens/sec): 18720.94 | learning rate: 1.293E-04 | global batch size:  1024 | lm loss: 1.787805E+00 | loss scale: 1.0 | grad norm: 1.525 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      863/   14369 | consumed samples:       883712 | elapsed time per iteration (ms): 110918.0 | rate (tokens/sec): 18907.23 | learning rate: 1.294E-04 | global batch size:  1024 | lm loss: 1.854292E+00 | loss scale: 1.0 | grad norm: 2.912 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      864/   14369 | consumed samples:       884736 | elapsed time per iteration (ms): 110999.9 | rate (tokens/sec): 18893.28 | learning rate: 1.296E-04 | global batch size:  1024 | lm loss: 1.837019E+00 | loss scale: 1.0 | grad norm: 2.238 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      865/   14369 | consumed samples:       885760 | elapsed time per iteration (ms): 111739.9 | rate (tokens/sec): 18768.16 | learning rate: 1.297E-04 | global batch size:  1024 | lm loss: 1.842902E+00 | loss scale: 1.0 | grad norm: 1.733 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      866/   14369 | consumed samples:       886784 | elapsed time per iteration (ms): 111786.9 | rate (tokens/sec): 18760.27 | learning rate: 1.299E-04 | global batch size:  1024 | lm loss: 1.824094E+00 | loss scale: 1.0 | grad norm: 1.849 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      867/   14369 | consumed samples:       887808 | elapsed time per iteration (ms): 112200.0 | rate (tokens/sec): 18691.19 | learning rate: 1.300E-04 | global batch size:  1024 | lm loss: 1.823014E+00 | loss scale: 1.0 | grad norm: 1.944 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      868/   14369 | consumed samples:       888832 | elapsed time per iteration (ms): 112104.9 | rate (tokens/sec): 18707.05 | learning rate: 1.302E-04 | global batch size:  1024 | lm loss: 1.843338E+00 | loss scale: 1.0 | grad norm: 2.469 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      869/   14369 | consumed samples:       889856 | elapsed time per iteration (ms): 111436.6 | rate (tokens/sec): 18819.24 | learning rate: 1.303E-04 | global batch size:  1024 | lm loss: 1.819096E+00 | loss scale: 1.0 | grad norm: 1.834 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      870/   14369 | consumed samples:       890880 | elapsed time per iteration (ms): 110934.4 | rate (tokens/sec): 18904.43 | learning rate: 1.305E-04 | global batch size:  1024 | lm loss: 1.827031E+00 | loss scale: 1.0 | grad norm: 2.844 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      871/   14369 | consumed samples:       891904 | elapsed time per iteration (ms): 111104.9 | rate (tokens/sec): 18875.43 | learning rate: 1.306E-04 | global batch size:  1024 | lm loss: 1.821046E+00 | loss scale: 1.0 | grad norm: 1.742 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      872/   14369 | consumed samples:       892928 | elapsed time per iteration (ms): 111503.9 | rate (tokens/sec): 18807.88 | learning rate: 1.308E-04 | global batch size:  1024 | lm loss: 1.828986E+00 | loss scale: 1.0 | grad norm: 2.106 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      873/   14369 | consumed samples:       893952 | elapsed time per iteration (ms): 110941.7 | rate (tokens/sec): 18903.19 | learning rate: 1.309E-04 | global batch size:  1024 | lm loss: 1.836081E+00 | loss scale: 1.0 | grad norm: 1.958 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      874/   14369 | consumed samples:       894976 | elapsed time per iteration (ms): 110584.5 | rate (tokens/sec): 18964.24 | learning rate: 1.311E-04 | global batch size:  1024 | lm loss: 1.839620E+00 | loss scale: 1.0 | grad norm: 2.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      875/   14369 | consumed samples:       896000 | elapsed time per iteration (ms): 111092.0 | rate (tokens/sec): 18877.62 | learning rate: 1.312E-04 | global batch size:  1024 | lm loss: 1.837088E+00 | loss scale: 1.0 | grad norm: 2.144 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      876/   14369 | consumed samples:       897024 | elapsed time per iteration (ms): 111941.9 | rate (tokens/sec): 18734.30 | learning rate: 1.314E-04 | global batch size:  1024 | lm loss: 1.821737E+00 | loss scale: 1.0 | grad norm: 1.732 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      877/   14369 | consumed samples:       898048 | elapsed time per iteration (ms): 110364.5 | rate (tokens/sec): 19002.05 | learning rate: 1.315E-04 | global batch size:  1024 | lm loss: 1.815852E+00 | loss scale: 1.0 | grad norm: 2.240 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      878/   14369 | consumed samples:       899072 | elapsed time per iteration (ms): 111646.9 | rate (tokens/sec): 18783.79 | learning rate: 1.317E-04 | global batch size:  1024 | lm loss: 1.835946E+00 | loss scale: 1.0 | grad norm: 1.868 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      879/   14369 | consumed samples:       900096 | elapsed time per iteration (ms): 111481.4 | rate (tokens/sec): 18811.67 | learning rate: 1.319E-04 | global batch size:  1024 | lm loss: 1.813251E+00 | loss scale: 1.0 | grad norm: 1.971 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      880/   14369 | consumed samples:       901120 | elapsed time per iteration (ms): 112066.9 | rate (tokens/sec): 18713.39 | learning rate: 1.320E-04 | global batch size:  1024 | lm loss: 1.815402E+00 | loss scale: 1.0 | grad norm: 2.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      881/   14369 | consumed samples:       902144 | elapsed time per iteration (ms): 111326.3 | rate (tokens/sec): 18837.89 | learning rate: 1.321E-04 | global batch size:  1024 | lm loss: 1.815064E+00 | loss scale: 1.0 | grad norm: 2.042 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      882/   14369 | consumed samples:       903168 | elapsed time per iteration (ms): 110379.9 | rate (tokens/sec): 18999.40 | learning rate: 1.323E-04 | global batch size:  1024 | lm loss: 1.821857E+00 | loss scale: 1.0 | grad norm: 1.817 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      883/   14369 | consumed samples:       904192 | elapsed time per iteration (ms): 111259.9 | rate (tokens/sec): 18849.13 | learning rate: 1.324E-04 | global batch size:  1024 | lm loss: 1.825090E+00 | loss scale: 1.0 | grad norm: 2.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      884/   14369 | consumed samples:       905216 | elapsed time per iteration (ms): 111144.9 | rate (tokens/sec): 18868.63 | learning rate: 1.326E-04 | global batch size:  1024 | lm loss: 1.825657E+00 | loss scale: 1.0 | grad norm: 1.702 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      885/   14369 | consumed samples:       906240 | elapsed time per iteration (ms): 111157.3 | rate (tokens/sec): 18866.52 | learning rate: 1.327E-04 | global batch size:  1024 | lm loss: 1.823672E+00 | loss scale: 1.0 | grad norm: 2.200 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      886/   14369 | consumed samples:       907264 | elapsed time per iteration (ms): 111972.2 | rate (tokens/sec): 18729.21 | learning rate: 1.329E-04 | global batch size:  1024 | lm loss: 1.810697E+00 | loss scale: 1.0 | grad norm: 1.756 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      887/   14369 | consumed samples:       908288 | elapsed time per iteration (ms): 112037.0 | rate (tokens/sec): 18718.38 | learning rate: 1.331E-04 | global batch size:  1024 | lm loss: 1.827252E+00 | loss scale: 1.0 | grad norm: 2.061 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      888/   14369 | consumed samples:       909312 | elapsed time per iteration (ms): 111547.0 | rate (tokens/sec): 18800.61 | learning rate: 1.332E-04 | global batch size:  1024 | lm loss: 1.807356E+00 | loss scale: 1.0 | grad norm: 1.781 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      889/   14369 | consumed samples:       910336 | elapsed time per iteration (ms): 111820.8 | rate (tokens/sec): 18754.58 | learning rate: 1.333E-04 | global batch size:  1024 | lm loss: 1.812116E+00 | loss scale: 1.0 | grad norm: 2.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      890/   14369 | consumed samples:       911360 | elapsed time per iteration (ms): 114932.6 | rate (tokens/sec): 18246.80 | learning rate: 1.335E-04 | global batch size:  1024 | lm loss: 1.819233E+00 | loss scale: 1.0 | grad norm: 1.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      891/   14369 | consumed samples:       912384 | elapsed time per iteration (ms): 111516.7 | rate (tokens/sec): 18805.73 | learning rate: 1.336E-04 | global batch size:  1024 | lm loss: 1.844913E+00 | loss scale: 1.0 | grad norm: 2.671 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      892/   14369 | consumed samples:       913408 | elapsed time per iteration (ms): 111286.9 | rate (tokens/sec): 18844.55 | learning rate: 1.338E-04 | global batch size:  1024 | lm loss: 1.804818E+00 | loss scale: 1.0 | grad norm: 1.646 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      893/   14369 | consumed samples:       914432 | elapsed time per iteration (ms): 111320.0 | rate (tokens/sec): 18838.95 | learning rate: 1.339E-04 | global batch size:  1024 | lm loss: 1.788516E+00 | loss scale: 1.0 | grad norm: 2.177 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      894/   14369 | consumed samples:       915456 | elapsed time per iteration (ms): 112404.4 | rate (tokens/sec): 18657.20 | learning rate: 1.341E-04 | global batch size:  1024 | lm loss: 1.825998E+00 | loss scale: 1.0 | grad norm: 1.580 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      895/   14369 | consumed samples:       916480 | elapsed time per iteration (ms): 111481.8 | rate (tokens/sec): 18811.62 | learning rate: 1.342E-04 | global batch size:  1024 | lm loss: 1.806422E+00 | loss scale: 1.0 | grad norm: 2.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      896/   14369 | consumed samples:       917504 | elapsed time per iteration (ms): 111114.0 | rate (tokens/sec): 18873.87 | learning rate: 1.344E-04 | global batch size:  1024 | lm loss: 1.820850E+00 | loss scale: 1.0 | grad norm: 2.612 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      897/   14369 | consumed samples:       918528 | elapsed time per iteration (ms): 112860.7 | rate (tokens/sec): 18581.77 | learning rate: 1.345E-04 | global batch size:  1024 | lm loss: 1.793906E+00 | loss scale: 1.0 | grad norm: 1.482 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      898/   14369 | consumed samples:       919552 | elapsed time per iteration (ms): 112962.5 | rate (tokens/sec): 18565.02 | learning rate: 1.347E-04 | global batch size:  1024 | lm loss: 1.823998E+00 | loss scale: 1.0 | grad norm: 2.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      899/   14369 | consumed samples:       920576 | elapsed time per iteration (ms): 112249.9 | rate (tokens/sec): 18682.88 | learning rate: 1.348E-04 | global batch size:  1024 | lm loss: 1.849810E+00 | loss scale: 1.0 | grad norm: 1.726 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      900/   14369 | consumed samples:       921600 | elapsed time per iteration (ms): 111961.9 | rate (tokens/sec): 18730.95 | learning rate: 1.350E-04 | global batch size:  1024 | lm loss: 1.800290E+00 | loss scale: 1.0 | grad norm: 1.947 | number of skipped iterations:   0 | number of nan iterations:   0 |
-----------------------------------------------------------------------------------------------
 validation loss at iteration 900 | lm loss value: 1.998799E+00 | lm loss PPL: 7.380185E+00 | 
-----------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
 test loss at iteration 900 | lm loss value: 3.288159E+00 | lm loss PPL: 2.679349E+01 | 
-----------------------------------------------------------------------------------------
saving checkpoint at iteration     900 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
  successfully saved checkpoint at iteration     900 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
(min, max) time across ranks (ms):
    save-checkpoint ................................: (114210.09, 114227.80)
 iteration      901/   14369 | consumed samples:       922624 | elapsed time per iteration (ms): 1196463.6 | rate (tokens/sec): 1752.79 | learning rate: 1.352E-04 | global batch size:  1024 | lm loss: 1.831864E+00 | loss scale: 1.0 | grad norm: 1.879 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      902/   14369 | consumed samples:       923648 | elapsed time per iteration (ms): 111722.0 | rate (tokens/sec): 18771.16 | learning rate: 1.353E-04 | global batch size:  1024 | lm loss: 1.839750E+00 | loss scale: 1.0 | grad norm: 2.279 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      903/   14369 | consumed samples:       924672 | elapsed time per iteration (ms): 112647.7 | rate (tokens/sec): 18616.91 | learning rate: 1.354E-04 | global batch size:  1024 | lm loss: 1.818307E+00 | loss scale: 1.0 | grad norm: 2.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      904/   14369 | consumed samples:       925696 | elapsed time per iteration (ms): 111555.0 | rate (tokens/sec): 18799.26 | learning rate: 1.356E-04 | global batch size:  1024 | lm loss: 1.800416E+00 | loss scale: 1.0 | grad norm: 1.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      905/   14369 | consumed samples:       926720 | elapsed time per iteration (ms): 111544.9 | rate (tokens/sec): 18800.97 | learning rate: 1.357E-04 | global batch size:  1024 | lm loss: 1.815902E+00 | loss scale: 1.0 | grad norm: 2.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      906/   14369 | consumed samples:       927744 | elapsed time per iteration (ms): 112996.7 | rate (tokens/sec): 18559.41 | learning rate: 1.359E-04 | global batch size:  1024 | lm loss: 1.805233E+00 | loss scale: 1.0 | grad norm: 1.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      907/   14369 | consumed samples:       928768 | elapsed time per iteration (ms): 111036.8 | rate (tokens/sec): 18887.00 | learning rate: 1.360E-04 | global batch size:  1024 | lm loss: 1.786897E+00 | loss scale: 1.0 | grad norm: 2.080 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      908/   14369 | consumed samples:       929792 | elapsed time per iteration (ms): 111598.0 | rate (tokens/sec): 18792.03 | learning rate: 1.362E-04 | global batch size:  1024 | lm loss: 1.812234E+00 | loss scale: 1.0 | grad norm: 1.842 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      909/   14369 | consumed samples:       930816 | elapsed time per iteration (ms): 111306.7 | rate (tokens/sec): 18841.20 | learning rate: 1.364E-04 | global batch size:  1024 | lm loss: 1.824181E+00 | loss scale: 1.0 | grad norm: 1.980 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      910/   14369 | consumed samples:       931840 | elapsed time per iteration (ms): 111656.1 | rate (tokens/sec): 18782.24 | learning rate: 1.365E-04 | global batch size:  1024 | lm loss: 1.812615E+00 | loss scale: 1.0 | grad norm: 2.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      911/   14369 | consumed samples:       932864 | elapsed time per iteration (ms): 111745.2 | rate (tokens/sec): 18767.27 | learning rate: 1.366E-04 | global batch size:  1024 | lm loss: 1.813898E+00 | loss scale: 1.0 | grad norm: 1.532 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      912/   14369 | consumed samples:       933888 | elapsed time per iteration (ms): 111241.7 | rate (tokens/sec): 18852.21 | learning rate: 1.368E-04 | global batch size:  1024 | lm loss: 1.811869E+00 | loss scale: 1.0 | grad norm: 1.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      913/   14369 | consumed samples:       934912 | elapsed time per iteration (ms): 110608.5 | rate (tokens/sec): 18960.13 | learning rate: 1.369E-04 | global batch size:  1024 | lm loss: 1.792173E+00 | loss scale: 1.0 | grad norm: 1.903 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      914/   14369 | consumed samples:       935936 | elapsed time per iteration (ms): 110721.7 | rate (tokens/sec): 18940.74 | learning rate: 1.371E-04 | global batch size:  1024 | lm loss: 1.829167E+00 | loss scale: 1.0 | grad norm: 3.074 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      915/   14369 | consumed samples:       936960 | elapsed time per iteration (ms): 112016.8 | rate (tokens/sec): 18721.76 | learning rate: 1.372E-04 | global batch size:  1024 | lm loss: 1.818638E+00 | loss scale: 1.0 | grad norm: 1.351 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      916/   14369 | consumed samples:       937984 | elapsed time per iteration (ms): 112599.9 | rate (tokens/sec): 18624.82 | learning rate: 1.374E-04 | global batch size:  1024 | lm loss: 1.819246E+00 | loss scale: 1.0 | grad norm: 3.212 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      917/   14369 | consumed samples:       939008 | elapsed time per iteration (ms): 111658.9 | rate (tokens/sec): 18781.77 | learning rate: 1.375E-04 | global batch size:  1024 | lm loss: 1.845638E+00 | loss scale: 1.0 | grad norm: 2.464 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      918/   14369 | consumed samples:       940032 | elapsed time per iteration (ms): 113181.7 | rate (tokens/sec): 18529.07 | learning rate: 1.377E-04 | global batch size:  1024 | lm loss: 1.817186E+00 | loss scale: 1.0 | grad norm: 1.951 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      919/   14369 | consumed samples:       941056 | elapsed time per iteration (ms): 112559.9 | rate (tokens/sec): 18631.43 | learning rate: 1.378E-04 | global batch size:  1024 | lm loss: 1.816690E+00 | loss scale: 1.0 | grad norm: 2.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      920/   14369 | consumed samples:       942080 | elapsed time per iteration (ms): 112007.8 | rate (tokens/sec): 18723.27 | learning rate: 1.380E-04 | global batch size:  1024 | lm loss: 1.817175E+00 | loss scale: 1.0 | grad norm: 1.994 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      921/   14369 | consumed samples:       943104 | elapsed time per iteration (ms): 111728.7 | rate (tokens/sec): 18770.03 | learning rate: 1.381E-04 | global batch size:  1024 | lm loss: 1.804685E+00 | loss scale: 1.0 | grad norm: 1.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      922/   14369 | consumed samples:       944128 | elapsed time per iteration (ms): 111783.9 | rate (tokens/sec): 18760.78 | learning rate: 1.383E-04 | global batch size:  1024 | lm loss: 1.819452E+00 | loss scale: 1.0 | grad norm: 1.669 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      923/   14369 | consumed samples:       945152 | elapsed time per iteration (ms): 111060.0 | rate (tokens/sec): 18883.05 | learning rate: 1.384E-04 | global batch size:  1024 | lm loss: 1.821012E+00 | loss scale: 1.0 | grad norm: 2.252 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      924/   14369 | consumed samples:       946176 | elapsed time per iteration (ms): 111961.7 | rate (tokens/sec): 18730.97 | learning rate: 1.386E-04 | global batch size:  1024 | lm loss: 1.811153E+00 | loss scale: 1.0 | grad norm: 1.893 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      925/   14369 | consumed samples:       947200 | elapsed time per iteration (ms): 112319.0 | rate (tokens/sec): 18671.39 | learning rate: 1.387E-04 | global batch size:  1024 | lm loss: 1.801810E+00 | loss scale: 1.0 | grad norm: 2.228 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      926/   14369 | consumed samples:       948224 | elapsed time per iteration (ms): 112346.9 | rate (tokens/sec): 18666.75 | learning rate: 1.389E-04 | global batch size:  1024 | lm loss: 1.787835E+00 | loss scale: 1.0 | grad norm: 2.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      927/   14369 | consumed samples:       949248 | elapsed time per iteration (ms): 111820.9 | rate (tokens/sec): 18754.57 | learning rate: 1.390E-04 | global batch size:  1024 | lm loss: 1.809817E+00 | loss scale: 1.0 | grad norm: 1.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      928/   14369 | consumed samples:       950272 | elapsed time per iteration (ms): 111802.7 | rate (tokens/sec): 18757.61 | learning rate: 1.392E-04 | global batch size:  1024 | lm loss: 1.805183E+00 | loss scale: 1.0 | grad norm: 2.193 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      929/   14369 | consumed samples:       951296 | elapsed time per iteration (ms): 111100.0 | rate (tokens/sec): 18876.25 | learning rate: 1.393E-04 | global batch size:  1024 | lm loss: 1.786877E+00 | loss scale: 1.0 | grad norm: 1.948 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      930/   14369 | consumed samples:       952320 | elapsed time per iteration (ms): 115600.7 | rate (tokens/sec): 18141.34 | learning rate: 1.395E-04 | global batch size:  1024 | lm loss: 1.803423E+00 | loss scale: 1.0 | grad norm: 2.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      931/   14369 | consumed samples:       953344 | elapsed time per iteration (ms): 117036.0 | rate (tokens/sec): 17918.86 | learning rate: 1.397E-04 | global batch size:  1024 | lm loss: 1.774561E+00 | loss scale: 1.0 | grad norm: 1.823 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      932/   14369 | consumed samples:       954368 | elapsed time per iteration (ms): 114443.8 | rate (tokens/sec): 18324.73 | learning rate: 1.398E-04 | global batch size:  1024 | lm loss: 1.780626E+00 | loss scale: 1.0 | grad norm: 2.647 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      933/   14369 | consumed samples:       955392 | elapsed time per iteration (ms): 113963.3 | rate (tokens/sec): 18402.00 | learning rate: 1.399E-04 | global batch size:  1024 | lm loss: 1.807774E+00 | loss scale: 1.0 | grad norm: 1.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      934/   14369 | consumed samples:       956416 | elapsed time per iteration (ms): 115579.9 | rate (tokens/sec): 18144.61 | learning rate: 1.401E-04 | global batch size:  1024 | lm loss: 1.810263E+00 | loss scale: 1.0 | grad norm: 2.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      935/   14369 | consumed samples:       957440 | elapsed time per iteration (ms): 115040.9 | rate (tokens/sec): 18229.63 | learning rate: 1.402E-04 | global batch size:  1024 | lm loss: 1.802249E+00 | loss scale: 1.0 | grad norm: 1.773 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      936/   14369 | consumed samples:       958464 | elapsed time per iteration (ms): 120521.7 | rate (tokens/sec): 17400.61 | learning rate: 1.404E-04 | global batch size:  1024 | lm loss: 1.808022E+00 | loss scale: 1.0 | grad norm: 2.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      937/   14369 | consumed samples:       959488 | elapsed time per iteration (ms): 117981.4 | rate (tokens/sec): 17775.27 | learning rate: 1.405E-04 | global batch size:  1024 | lm loss: 1.791684E+00 | loss scale: 1.0 | grad norm: 2.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      938/   14369 | consumed samples:       960512 | elapsed time per iteration (ms): 114807.9 | rate (tokens/sec): 18266.62 | learning rate: 1.407E-04 | global batch size:  1024 | lm loss: 1.827696E+00 | loss scale: 1.0 | grad norm: 1.844 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      939/   14369 | consumed samples:       961536 | elapsed time per iteration (ms): 115594.7 | rate (tokens/sec): 18142.28 | learning rate: 1.408E-04 | global batch size:  1024 | lm loss: 1.805356E+00 | loss scale: 1.0 | grad norm: 2.058 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      940/   14369 | consumed samples:       962560 | elapsed time per iteration (ms): 113146.9 | rate (tokens/sec): 18534.78 | learning rate: 1.410E-04 | global batch size:  1024 | lm loss: 1.773878E+00 | loss scale: 1.0 | grad norm: 1.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      941/   14369 | consumed samples:       963584 | elapsed time per iteration (ms): 113502.1 | rate (tokens/sec): 18476.76 | learning rate: 1.411E-04 | global batch size:  1024 | lm loss: 1.811254E+00 | loss scale: 1.0 | grad norm: 2.194 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      942/   14369 | consumed samples:       964608 | elapsed time per iteration (ms): 113686.6 | rate (tokens/sec): 18446.78 | learning rate: 1.413E-04 | global batch size:  1024 | lm loss: 1.782885E+00 | loss scale: 1.0 | grad norm: 1.760 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      943/   14369 | consumed samples:       965632 | elapsed time per iteration (ms): 112654.3 | rate (tokens/sec): 18615.82 | learning rate: 1.414E-04 | global batch size:  1024 | lm loss: 1.791719E+00 | loss scale: 1.0 | grad norm: 1.980 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      944/   14369 | consumed samples:       966656 | elapsed time per iteration (ms): 112622.9 | rate (tokens/sec): 18621.01 | learning rate: 1.416E-04 | global batch size:  1024 | lm loss: 1.806784E+00 | loss scale: 1.0 | grad norm: 2.058 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      945/   14369 | consumed samples:       967680 | elapsed time per iteration (ms): 113358.9 | rate (tokens/sec): 18500.11 | learning rate: 1.417E-04 | global batch size:  1024 | lm loss: 1.816505E+00 | loss scale: 1.0 | grad norm: 2.030 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      946/   14369 | consumed samples:       968704 | elapsed time per iteration (ms): 112335.0 | rate (tokens/sec): 18668.73 | learning rate: 1.419E-04 | global batch size:  1024 | lm loss: 1.808809E+00 | loss scale: 1.0 | grad norm: 1.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      947/   14369 | consumed samples:       969728 | elapsed time per iteration (ms): 112381.9 | rate (tokens/sec): 18660.94 | learning rate: 1.420E-04 | global batch size:  1024 | lm loss: 1.795818E+00 | loss scale: 1.0 | grad norm: 2.099 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      948/   14369 | consumed samples:       970752 | elapsed time per iteration (ms): 111512.9 | rate (tokens/sec): 18806.36 | learning rate: 1.422E-04 | global batch size:  1024 | lm loss: 1.797861E+00 | loss scale: 1.0 | grad norm: 1.844 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      949/   14369 | consumed samples:       971776 | elapsed time per iteration (ms): 112741.4 | rate (tokens/sec): 18601.44 | learning rate: 1.423E-04 | global batch size:  1024 | lm loss: 1.790001E+00 | loss scale: 1.0 | grad norm: 1.857 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      950/   14369 | consumed samples:       972800 | elapsed time per iteration (ms): 113644.9 | rate (tokens/sec): 18453.55 | learning rate: 1.425E-04 | global batch size:  1024 | lm loss: 1.764532E+00 | loss scale: 1.0 | grad norm: 1.843 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      951/   14369 | consumed samples:       973824 | elapsed time per iteration (ms): 113126.6 | rate (tokens/sec): 18538.10 | learning rate: 1.426E-04 | global batch size:  1024 | lm loss: 1.790513E+00 | loss scale: 1.0 | grad norm: 1.831 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      952/   14369 | consumed samples:       974848 | elapsed time per iteration (ms): 114361.8 | rate (tokens/sec): 18337.88 | learning rate: 1.428E-04 | global batch size:  1024 | lm loss: 1.782776E+00 | loss scale: 1.0 | grad norm: 2.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      953/   14369 | consumed samples:       975872 | elapsed time per iteration (ms): 113060.0 | rate (tokens/sec): 18549.02 | learning rate: 1.430E-04 | global batch size:  1024 | lm loss: 1.775728E+00 | loss scale: 1.0 | grad norm: 1.908 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      954/   14369 | consumed samples:       976896 | elapsed time per iteration (ms): 113061.7 | rate (tokens/sec): 18548.74 | learning rate: 1.431E-04 | global batch size:  1024 | lm loss: 1.775094E+00 | loss scale: 1.0 | grad norm: 1.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      955/   14369 | consumed samples:       977920 | elapsed time per iteration (ms): 113861.7 | rate (tokens/sec): 18418.41 | learning rate: 1.432E-04 | global batch size:  1024 | lm loss: 1.782878E+00 | loss scale: 1.0 | grad norm: 1.877 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      956/   14369 | consumed samples:       978944 | elapsed time per iteration (ms): 112047.1 | rate (tokens/sec): 18716.70 | learning rate: 1.434E-04 | global batch size:  1024 | lm loss: 1.816668E+00 | loss scale: 1.0 | grad norm: 1.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      957/   14369 | consumed samples:       979968 | elapsed time per iteration (ms): 112887.0 | rate (tokens/sec): 18577.44 | learning rate: 1.435E-04 | global batch size:  1024 | lm loss: 1.792803E+00 | loss scale: 1.0 | grad norm: 1.566 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      958/   14369 | consumed samples:       980992 | elapsed time per iteration (ms): 112333.5 | rate (tokens/sec): 18668.98 | learning rate: 1.437E-04 | global batch size:  1024 | lm loss: 1.818247E+00 | loss scale: 1.0 | grad norm: 2.418 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      959/   14369 | consumed samples:       982016 | elapsed time per iteration (ms): 112979.9 | rate (tokens/sec): 18562.17 | learning rate: 1.438E-04 | global batch size:  1024 | lm loss: 1.791196E+00 | loss scale: 1.0 | grad norm: 1.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      960/   14369 | consumed samples:       983040 | elapsed time per iteration (ms): 112598.9 | rate (tokens/sec): 18624.98 | learning rate: 1.440E-04 | global batch size:  1024 | lm loss: 1.799028E+00 | loss scale: 1.0 | grad norm: 2.440 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      961/   14369 | consumed samples:       984064 | elapsed time per iteration (ms): 112121.7 | rate (tokens/sec): 18704.24 | learning rate: 1.442E-04 | global batch size:  1024 | lm loss: 1.787734E+00 | loss scale: 1.0 | grad norm: 1.448 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      962/   14369 | consumed samples:       985088 | elapsed time per iteration (ms): 111598.9 | rate (tokens/sec): 18791.87 | learning rate: 1.443E-04 | global batch size:  1024 | lm loss: 1.809748E+00 | loss scale: 1.0 | grad norm: 2.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      963/   14369 | consumed samples:       986112 | elapsed time per iteration (ms): 111961.1 | rate (tokens/sec): 18731.08 | learning rate: 1.444E-04 | global batch size:  1024 | lm loss: 1.806210E+00 | loss scale: 1.0 | grad norm: 1.917 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      964/   14369 | consumed samples:       987136 | elapsed time per iteration (ms): 112341.8 | rate (tokens/sec): 18667.60 | learning rate: 1.446E-04 | global batch size:  1024 | lm loss: 1.795068E+00 | loss scale: 1.0 | grad norm: 2.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      965/   14369 | consumed samples:       988160 | elapsed time per iteration (ms): 112358.9 | rate (tokens/sec): 18664.76 | learning rate: 1.447E-04 | global batch size:  1024 | lm loss: 1.790191E+00 | loss scale: 1.0 | grad norm: 1.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      966/   14369 | consumed samples:       989184 | elapsed time per iteration (ms): 111626.9 | rate (tokens/sec): 18787.16 | learning rate: 1.449E-04 | global batch size:  1024 | lm loss: 1.801341E+00 | loss scale: 1.0 | grad norm: 2.116 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      967/   14369 | consumed samples:       990208 | elapsed time per iteration (ms): 111704.8 | rate (tokens/sec): 18774.05 | learning rate: 1.450E-04 | global batch size:  1024 | lm loss: 1.778933E+00 | loss scale: 1.0 | grad norm: 1.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      968/   14369 | consumed samples:       991232 | elapsed time per iteration (ms): 111720.0 | rate (tokens/sec): 18771.50 | learning rate: 1.452E-04 | global batch size:  1024 | lm loss: 1.795798E+00 | loss scale: 1.0 | grad norm: 2.144 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      969/   14369 | consumed samples:       992256 | elapsed time per iteration (ms): 116140.1 | rate (tokens/sec): 18057.09 | learning rate: 1.453E-04 | global batch size:  1024 | lm loss: 1.783192E+00 | loss scale: 1.0 | grad norm: 1.759 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      970/   14369 | consumed samples:       993280 | elapsed time per iteration (ms): 112107.0 | rate (tokens/sec): 18706.70 | learning rate: 1.455E-04 | global batch size:  1024 | lm loss: 1.789370E+00 | loss scale: 1.0 | grad norm: 1.801 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      971/   14369 | consumed samples:       994304 | elapsed time per iteration (ms): 111488.4 | rate (tokens/sec): 18810.49 | learning rate: 1.456E-04 | global batch size:  1024 | lm loss: 1.797227E+00 | loss scale: 1.0 | grad norm: 2.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      972/   14369 | consumed samples:       995328 | elapsed time per iteration (ms): 112532.9 | rate (tokens/sec): 18635.90 | learning rate: 1.458E-04 | global batch size:  1024 | lm loss: 1.793059E+00 | loss scale: 1.0 | grad norm: 1.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      973/   14369 | consumed samples:       996352 | elapsed time per iteration (ms): 112019.9 | rate (tokens/sec): 18721.25 | learning rate: 1.459E-04 | global batch size:  1024 | lm loss: 1.796098E+00 | loss scale: 1.0 | grad norm: 1.985 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      974/   14369 | consumed samples:       997376 | elapsed time per iteration (ms): 112141.7 | rate (tokens/sec): 18700.91 | learning rate: 1.461E-04 | global batch size:  1024 | lm loss: 1.779534E+00 | loss scale: 1.0 | grad norm: 2.226 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      975/   14369 | consumed samples:       998400 | elapsed time per iteration (ms): 111623.9 | rate (tokens/sec): 18787.66 | learning rate: 1.463E-04 | global batch size:  1024 | lm loss: 1.784544E+00 | loss scale: 1.0 | grad norm: 1.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      976/   14369 | consumed samples:       999424 | elapsed time per iteration (ms): 112773.0 | rate (tokens/sec): 18596.22 | learning rate: 1.464E-04 | global batch size:  1024 | lm loss: 1.792602E+00 | loss scale: 1.0 | grad norm: 2.125 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      977/   14369 | consumed samples:      1000448 | elapsed time per iteration (ms): 112641.7 | rate (tokens/sec): 18617.90 | learning rate: 1.465E-04 | global batch size:  1024 | lm loss: 1.776824E+00 | loss scale: 1.0 | grad norm: 1.905 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      978/   14369 | consumed samples:      1001472 | elapsed time per iteration (ms): 112180.0 | rate (tokens/sec): 18694.52 | learning rate: 1.467E-04 | global batch size:  1024 | lm loss: 1.792618E+00 | loss scale: 1.0 | grad norm: 1.874 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      979/   14369 | consumed samples:      1002496 | elapsed time per iteration (ms): 111943.7 | rate (tokens/sec): 18733.99 | learning rate: 1.468E-04 | global batch size:  1024 | lm loss: 1.792487E+00 | loss scale: 1.0 | grad norm: 1.767 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      980/   14369 | consumed samples:      1003520 | elapsed time per iteration (ms): 113608.2 | rate (tokens/sec): 18459.51 | learning rate: 1.470E-04 | global batch size:  1024 | lm loss: 1.773872E+00 | loss scale: 1.0 | grad norm: 2.136 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      981/   14369 | consumed samples:      1004544 | elapsed time per iteration (ms): 112193.0 | rate (tokens/sec): 18692.36 | learning rate: 1.471E-04 | global batch size:  1024 | lm loss: 1.772362E+00 | loss scale: 1.0 | grad norm: 1.719 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      982/   14369 | consumed samples:      1005568 | elapsed time per iteration (ms): 111941.7 | rate (tokens/sec): 18734.33 | learning rate: 1.473E-04 | global batch size:  1024 | lm loss: 1.781122E+00 | loss scale: 1.0 | grad norm: 2.077 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      983/   14369 | consumed samples:      1006592 | elapsed time per iteration (ms): 112187.0 | rate (tokens/sec): 18693.36 | learning rate: 1.475E-04 | global batch size:  1024 | lm loss: 1.790788E+00 | loss scale: 1.0 | grad norm: 1.791 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      984/   14369 | consumed samples:      1007616 | elapsed time per iteration (ms): 111680.0 | rate (tokens/sec): 18778.22 | learning rate: 1.476E-04 | global batch size:  1024 | lm loss: 1.791302E+00 | loss scale: 1.0 | grad norm: 1.749 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      985/   14369 | consumed samples:      1008640 | elapsed time per iteration (ms): 111306.9 | rate (tokens/sec): 18841.17 | learning rate: 1.477E-04 | global batch size:  1024 | lm loss: 1.766447E+00 | loss scale: 1.0 | grad norm: 1.736 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      986/   14369 | consumed samples:      1009664 | elapsed time per iteration (ms): 112833.0 | rate (tokens/sec): 18586.33 | learning rate: 1.479E-04 | global batch size:  1024 | lm loss: 1.789110E+00 | loss scale: 1.0 | grad norm: 2.550 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      987/   14369 | consumed samples:      1010688 | elapsed time per iteration (ms): 111360.0 | rate (tokens/sec): 18832.18 | learning rate: 1.480E-04 | global batch size:  1024 | lm loss: 1.787419E+00 | loss scale: 1.0 | grad norm: 1.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      988/   14369 | consumed samples:      1011712 | elapsed time per iteration (ms): 111367.0 | rate (tokens/sec): 18830.99 | learning rate: 1.482E-04 | global batch size:  1024 | lm loss: 1.766690E+00 | loss scale: 1.0 | grad norm: 1.788 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      989/   14369 | consumed samples:      1012736 | elapsed time per iteration (ms): 111646.8 | rate (tokens/sec): 18783.81 | learning rate: 1.483E-04 | global batch size:  1024 | lm loss: 1.806308E+00 | loss scale: 1.0 | grad norm: 2.164 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      990/   14369 | consumed samples:      1013760 | elapsed time per iteration (ms): 111434.9 | rate (tokens/sec): 18819.53 | learning rate: 1.485E-04 | global batch size:  1024 | lm loss: 1.802841E+00 | loss scale: 1.0 | grad norm: 1.816 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      991/   14369 | consumed samples:      1014784 | elapsed time per iteration (ms): 111601.7 | rate (tokens/sec): 18791.40 | learning rate: 1.486E-04 | global batch size:  1024 | lm loss: 1.757849E+00 | loss scale: 1.0 | grad norm: 1.952 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      992/   14369 | consumed samples:      1015808 | elapsed time per iteration (ms): 111967.4 | rate (tokens/sec): 18730.03 | learning rate: 1.488E-04 | global batch size:  1024 | lm loss: 1.778793E+00 | loss scale: 1.0 | grad norm: 2.034 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      993/   14369 | consumed samples:      1016832 | elapsed time per iteration (ms): 111573.0 | rate (tokens/sec): 18796.23 | learning rate: 1.489E-04 | global batch size:  1024 | lm loss: 1.778886E+00 | loss scale: 1.0 | grad norm: 1.537 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      994/   14369 | consumed samples:      1017856 | elapsed time per iteration (ms): 111441.7 | rate (tokens/sec): 18818.37 | learning rate: 1.491E-04 | global batch size:  1024 | lm loss: 1.782862E+00 | loss scale: 1.0 | grad norm: 2.142 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      995/   14369 | consumed samples:      1018880 | elapsed time per iteration (ms): 111357.9 | rate (tokens/sec): 18832.54 | learning rate: 1.492E-04 | global batch size:  1024 | lm loss: 1.788380E+00 | loss scale: 1.0 | grad norm: 1.686 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      996/   14369 | consumed samples:      1019904 | elapsed time per iteration (ms): 111226.9 | rate (tokens/sec): 18854.72 | learning rate: 1.494E-04 | global batch size:  1024 | lm loss: 1.786586E+00 | loss scale: 1.0 | grad norm: 2.194 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      997/   14369 | consumed samples:      1020928 | elapsed time per iteration (ms): 111344.4 | rate (tokens/sec): 18834.82 | learning rate: 1.495E-04 | global batch size:  1024 | lm loss: 1.787654E+00 | loss scale: 1.0 | grad norm: 1.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      998/   14369 | consumed samples:      1021952 | elapsed time per iteration (ms): 111545.1 | rate (tokens/sec): 18800.93 | learning rate: 1.497E-04 | global batch size:  1024 | lm loss: 1.783984E+00 | loss scale: 1.0 | grad norm: 2.050 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      999/   14369 | consumed samples:      1022976 | elapsed time per iteration (ms): 111001.7 | rate (tokens/sec): 18892.97 | learning rate: 1.498E-04 | global batch size:  1024 | lm loss: 1.776679E+00 | loss scale: 1.0 | grad norm: 1.721 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1000/   14369 | consumed samples:      1024000 | elapsed time per iteration (ms): 111561.7 | rate (tokens/sec): 18798.14 | learning rate: 1.500E-04 | global batch size:  1024 | lm loss: 1.756913E+00 | loss scale: 1.0 | grad norm: 1.750 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1001/   14369 | consumed samples:      1025024 | elapsed time per iteration (ms): 110633.0 | rate (tokens/sec): 18955.93 | learning rate: 1.501E-04 | global batch size:  1024 | lm loss: 1.787257E+00 | loss scale: 1.0 | grad norm: 2.414 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1002/   14369 | consumed samples:      1026048 | elapsed time per iteration (ms): 111678.5 | rate (tokens/sec): 18778.48 | learning rate: 1.503E-04 | global batch size:  1024 | lm loss: 1.788268E+00 | loss scale: 1.0 | grad norm: 1.793 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1003/   14369 | consumed samples:      1027072 | elapsed time per iteration (ms): 112608.3 | rate (tokens/sec): 18623.42 | learning rate: 1.504E-04 | global batch size:  1024 | lm loss: 1.777322E+00 | loss scale: 1.0 | grad norm: 1.536 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1004/   14369 | consumed samples:      1028096 | elapsed time per iteration (ms): 110874.3 | rate (tokens/sec): 18914.68 | learning rate: 1.506E-04 | global batch size:  1024 | lm loss: 1.761836E+00 | loss scale: 1.0 | grad norm: 1.634 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1005/   14369 | consumed samples:      1029120 | elapsed time per iteration (ms): 112294.0 | rate (tokens/sec): 18675.54 | learning rate: 1.508E-04 | global batch size:  1024 | lm loss: 1.774292E+00 | loss scale: 1.0 | grad norm: 1.793 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1006/   14369 | consumed samples:      1030144 | elapsed time per iteration (ms): 112099.9 | rate (tokens/sec): 18707.88 | learning rate: 1.509E-04 | global batch size:  1024 | lm loss: 1.762047E+00 | loss scale: 1.0 | grad norm: 1.713 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1007/   14369 | consumed samples:      1031168 | elapsed time per iteration (ms): 112261.7 | rate (tokens/sec): 18680.91 | learning rate: 1.510E-04 | global batch size:  1024 | lm loss: 1.797654E+00 | loss scale: 1.0 | grad norm: 1.892 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1008/   14369 | consumed samples:      1032192 | elapsed time per iteration (ms): 112327.0 | rate (tokens/sec): 18670.06 | learning rate: 1.512E-04 | global batch size:  1024 | lm loss: 1.778726E+00 | loss scale: 1.0 | grad norm: 1.936 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1009/   14369 | consumed samples:      1033216 | elapsed time per iteration (ms): 110690.3 | rate (tokens/sec): 18946.13 | learning rate: 1.513E-04 | global batch size:  1024 | lm loss: 1.760337E+00 | loss scale: 1.0 | grad norm: 1.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1010/   14369 | consumed samples:      1034240 | elapsed time per iteration (ms): 111121.8 | rate (tokens/sec): 18872.55 | learning rate: 1.515E-04 | global batch size:  1024 | lm loss: 1.782950E+00 | loss scale: 1.0 | grad norm: 1.952 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1011/   14369 | consumed samples:      1035264 | elapsed time per iteration (ms): 111299.9 | rate (tokens/sec): 18842.36 | learning rate: 1.516E-04 | global batch size:  1024 | lm loss: 1.785067E+00 | loss scale: 1.0 | grad norm: 2.081 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1012/   14369 | consumed samples:      1036288 | elapsed time per iteration (ms): 112868.9 | rate (tokens/sec): 18580.43 | learning rate: 1.518E-04 | global batch size:  1024 | lm loss: 1.763090E+00 | loss scale: 1.0 | grad norm: 1.555 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1013/   14369 | consumed samples:      1037312 | elapsed time per iteration (ms): 111946.0 | rate (tokens/sec): 18733.60 | learning rate: 1.519E-04 | global batch size:  1024 | lm loss: 1.784745E+00 | loss scale: 1.0 | grad norm: 2.090 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1014/   14369 | consumed samples:      1038336 | elapsed time per iteration (ms): 113281.7 | rate (tokens/sec): 18512.72 | learning rate: 1.521E-04 | global batch size:  1024 | lm loss: 1.786968E+00 | loss scale: 1.0 | grad norm: 1.707 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1015/   14369 | consumed samples:      1039360 | elapsed time per iteration (ms): 112844.2 | rate (tokens/sec): 18584.49 | learning rate: 1.522E-04 | global batch size:  1024 | lm loss: 1.776073E+00 | loss scale: 1.0 | grad norm: 2.384 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1016/   14369 | consumed samples:      1040384 | elapsed time per iteration (ms): 112166.9 | rate (tokens/sec): 18696.71 | learning rate: 1.524E-04 | global batch size:  1024 | lm loss: 1.759398E+00 | loss scale: 1.0 | grad norm: 1.166 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1017/   14369 | consumed samples:      1041408 | elapsed time per iteration (ms): 112265.1 | rate (tokens/sec): 18680.35 | learning rate: 1.525E-04 | global batch size:  1024 | lm loss: 1.814987E+00 | loss scale: 1.0 | grad norm: 2.856 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1018/   14369 | consumed samples:      1042432 | elapsed time per iteration (ms): 112260.7 | rate (tokens/sec): 18681.09 | learning rate: 1.527E-04 | global batch size:  1024 | lm loss: 1.794757E+00 | loss scale: 1.0 | grad norm: 1.562 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1019/   14369 | consumed samples:      1043456 | elapsed time per iteration (ms): 112307.0 | rate (tokens/sec): 18673.38 | learning rate: 1.528E-04 | global batch size:  1024 | lm loss: 1.776603E+00 | loss scale: 1.0 | grad norm: 1.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1020/   14369 | consumed samples:      1044480 | elapsed time per iteration (ms): 112149.5 | rate (tokens/sec): 18699.61 | learning rate: 1.530E-04 | global batch size:  1024 | lm loss: 1.783012E+00 | loss scale: 1.0 | grad norm: 1.761 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1021/   14369 | consumed samples:      1045504 | elapsed time per iteration (ms): 111646.9 | rate (tokens/sec): 18783.79 | learning rate: 1.531E-04 | global batch size:  1024 | lm loss: 1.776772E+00 | loss scale: 1.0 | grad norm: 1.547 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1022/   14369 | consumed samples:      1046528 | elapsed time per iteration (ms): 111706.0 | rate (tokens/sec): 18773.85 | learning rate: 1.533E-04 | global batch size:  1024 | lm loss: 1.792732E+00 | loss scale: 1.0 | grad norm: 1.934 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1023/   14369 | consumed samples:      1047552 | elapsed time per iteration (ms): 111720.0 | rate (tokens/sec): 18771.50 | learning rate: 1.534E-04 | global batch size:  1024 | lm loss: 1.772745E+00 | loss scale: 1.0 | grad norm: 1.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1024/   14369 | consumed samples:      1048576 | elapsed time per iteration (ms): 111094.8 | rate (tokens/sec): 18877.13 | learning rate: 1.536E-04 | global batch size:  1024 | lm loss: 1.775051E+00 | loss scale: 1.0 | grad norm: 1.607 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1025/   14369 | consumed samples:      1049600 | elapsed time per iteration (ms): 110906.0 | rate (tokens/sec): 18909.27 | learning rate: 1.537E-04 | global batch size:  1024 | lm loss: 1.774334E+00 | loss scale: 1.0 | grad norm: 2.017 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1026/   14369 | consumed samples:      1050624 | elapsed time per iteration (ms): 111540.5 | rate (tokens/sec): 18801.71 | learning rate: 1.539E-04 | global batch size:  1024 | lm loss: 1.773965E+00 | loss scale: 1.0 | grad norm: 2.259 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1027/   14369 | consumed samples:      1051648 | elapsed time per iteration (ms): 112029.0 | rate (tokens/sec): 18719.72 | learning rate: 1.541E-04 | global batch size:  1024 | lm loss: 1.790541E+00 | loss scale: 1.0 | grad norm: 1.933 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1028/   14369 | consumed samples:      1052672 | elapsed time per iteration (ms): 111947.0 | rate (tokens/sec): 18733.43 | learning rate: 1.542E-04 | global batch size:  1024 | lm loss: 1.788327E+00 | loss scale: 1.0 | grad norm: 2.497 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1029/   14369 | consumed samples:      1053696 | elapsed time per iteration (ms): 111702.7 | rate (tokens/sec): 18774.40 | learning rate: 1.543E-04 | global batch size:  1024 | lm loss: 1.773832E+00 | loss scale: 1.0 | grad norm: 1.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1030/   14369 | consumed samples:      1054720 | elapsed time per iteration (ms): 112216.3 | rate (tokens/sec): 18688.48 | learning rate: 1.545E-04 | global batch size:  1024 | lm loss: 1.799813E+00 | loss scale: 1.0 | grad norm: 2.151 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1031/   14369 | consumed samples:      1055744 | elapsed time per iteration (ms): 112019.0 | rate (tokens/sec): 18721.39 | learning rate: 1.546E-04 | global batch size:  1024 | lm loss: 1.783904E+00 | loss scale: 1.0 | grad norm: 1.601 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1032/   14369 | consumed samples:      1056768 | elapsed time per iteration (ms): 111214.0 | rate (tokens/sec): 18856.90 | learning rate: 1.548E-04 | global batch size:  1024 | lm loss: 1.793582E+00 | loss scale: 1.0 | grad norm: 1.977 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1033/   14369 | consumed samples:      1057792 | elapsed time per iteration (ms): 111108.2 | rate (tokens/sec): 18874.87 | learning rate: 1.549E-04 | global batch size:  1024 | lm loss: 1.787373E+00 | loss scale: 1.0 | grad norm: 1.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1034/   14369 | consumed samples:      1058816 | elapsed time per iteration (ms): 112900.0 | rate (tokens/sec): 18575.30 | learning rate: 1.551E-04 | global batch size:  1024 | lm loss: 1.768180E+00 | loss scale: 1.0 | grad norm: 2.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1035/   14369 | consumed samples:      1059840 | elapsed time per iteration (ms): 111240.0 | rate (tokens/sec): 18852.50 | learning rate: 1.553E-04 | global batch size:  1024 | lm loss: 1.796733E+00 | loss scale: 1.0 | grad norm: 1.669 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1036/   14369 | consumed samples:      1060864 | elapsed time per iteration (ms): 111167.3 | rate (tokens/sec): 18864.82 | learning rate: 1.554E-04 | global batch size:  1024 | lm loss: 1.780318E+00 | loss scale: 1.0 | grad norm: 2.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1037/   14369 | consumed samples:      1061888 | elapsed time per iteration (ms): 112091.0 | rate (tokens/sec): 18709.37 | learning rate: 1.555E-04 | global batch size:  1024 | lm loss: 1.759969E+00 | loss scale: 1.0 | grad norm: 1.922 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1038/   14369 | consumed samples:      1062912 | elapsed time per iteration (ms): 111930.8 | rate (tokens/sec): 18736.14 | learning rate: 1.557E-04 | global batch size:  1024 | lm loss: 1.759587E+00 | loss scale: 1.0 | grad norm: 1.723 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1039/   14369 | consumed samples:      1063936 | elapsed time per iteration (ms): 111469.8 | rate (tokens/sec): 18813.63 | learning rate: 1.558E-04 | global batch size:  1024 | lm loss: 1.775173E+00 | loss scale: 1.0 | grad norm: 2.213 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1040/   14369 | consumed samples:      1064960 | elapsed time per iteration (ms): 111981.7 | rate (tokens/sec): 18727.63 | learning rate: 1.560E-04 | global batch size:  1024 | lm loss: 1.768021E+00 | loss scale: 1.0 | grad norm: 1.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1041/   14369 | consumed samples:      1065984 | elapsed time per iteration (ms): 112383.5 | rate (tokens/sec): 18660.68 | learning rate: 1.561E-04 | global batch size:  1024 | lm loss: 1.779782E+00 | loss scale: 1.0 | grad norm: 2.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1042/   14369 | consumed samples:      1067008 | elapsed time per iteration (ms): 111967.0 | rate (tokens/sec): 18730.09 | learning rate: 1.563E-04 | global batch size:  1024 | lm loss: 1.761456E+00 | loss scale: 1.0 | grad norm: 1.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1043/   14369 | consumed samples:      1068032 | elapsed time per iteration (ms): 111422.8 | rate (tokens/sec): 18821.57 | learning rate: 1.564E-04 | global batch size:  1024 | lm loss: 1.774393E+00 | loss scale: 1.0 | grad norm: 1.792 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1044/   14369 | consumed samples:      1069056 | elapsed time per iteration (ms): 116560.0 | rate (tokens/sec): 17992.05 | learning rate: 1.566E-04 | global batch size:  1024 | lm loss: 1.756952E+00 | loss scale: 1.0 | grad norm: 1.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1045/   14369 | consumed samples:      1070080 | elapsed time per iteration (ms): 110129.8 | rate (tokens/sec): 19042.55 | learning rate: 1.567E-04 | global batch size:  1024 | lm loss: 1.776605E+00 | loss scale: 1.0 | grad norm: 2.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1046/   14369 | consumed samples:      1071104 | elapsed time per iteration (ms): 111113.9 | rate (tokens/sec): 18873.89 | learning rate: 1.569E-04 | global batch size:  1024 | lm loss: 1.759429E+00 | loss scale: 1.0 | grad norm: 1.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1047/   14369 | consumed samples:      1072128 | elapsed time per iteration (ms): 111113.7 | rate (tokens/sec): 18873.92 | learning rate: 1.570E-04 | global batch size:  1024 | lm loss: 1.749531E+00 | loss scale: 1.0 | grad norm: 2.243 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1048/   14369 | consumed samples:      1073152 | elapsed time per iteration (ms): 111700.9 | rate (tokens/sec): 18774.70 | learning rate: 1.572E-04 | global batch size:  1024 | lm loss: 1.793478E+00 | loss scale: 1.0 | grad norm: 1.956 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1049/   14369 | consumed samples:      1074176 | elapsed time per iteration (ms): 110664.9 | rate (tokens/sec): 18950.47 | learning rate: 1.574E-04 | global batch size:  1024 | lm loss: 1.779143E+00 | loss scale: 1.0 | grad norm: 1.948 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1050/   14369 | consumed samples:      1075200 | elapsed time per iteration (ms): 111267.5 | rate (tokens/sec): 18847.84 | learning rate: 1.575E-04 | global batch size:  1024 | lm loss: 1.782080E+00 | loss scale: 1.0 | grad norm: 2.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
------------------------------------------------------------------------------------------------
 validation loss at iteration 1050 | lm loss value: 1.938339E+00 | lm loss PPL: 6.947204E+00 | 
------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------
 test loss at iteration 1050 | lm loss value: 3.169384E+00 | lm loss PPL: 2.379281E+01 | 
------------------------------------------------------------------------------------------
saving checkpoint at iteration    1050 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
  successfully saved checkpoint at iteration    1050 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
(min, max) time across ranks (ms):
    save-checkpoint ................................: (114155.46, 114155.95)
 iteration     1051/   14369 | consumed samples:      1076224 | elapsed time per iteration (ms): 1176082.8 | rate (tokens/sec): 1783.17 | learning rate: 1.576E-04 | global batch size:  1024 | lm loss: 1.757989E+00 | loss scale: 1.0 | grad norm: 2.201 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1052/   14369 | consumed samples:      1077248 | elapsed time per iteration (ms): 109161.9 | rate (tokens/sec): 19211.40 | learning rate: 1.578E-04 | global batch size:  1024 | lm loss: 1.764925E+00 | loss scale: 1.0 | grad norm: 1.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1053/   14369 | consumed samples:      1078272 | elapsed time per iteration (ms): 110482.6 | rate (tokens/sec): 18981.73 | learning rate: 1.579E-04 | global batch size:  1024 | lm loss: 1.786006E+00 | loss scale: 1.0 | grad norm: 2.548 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1054/   14369 | consumed samples:      1079296 | elapsed time per iteration (ms): 110838.3 | rate (tokens/sec): 18920.82 | learning rate: 1.581E-04 | global batch size:  1024 | lm loss: 1.769967E+00 | loss scale: 1.0 | grad norm: 1.658 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1055/   14369 | consumed samples:      1080320 | elapsed time per iteration (ms): 111084.9 | rate (tokens/sec): 18878.82 | learning rate: 1.582E-04 | global batch size:  1024 | lm loss: 1.769161E+00 | loss scale: 1.0 | grad norm: 1.957 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1056/   14369 | consumed samples:      1081344 | elapsed time per iteration (ms): 110511.1 | rate (tokens/sec): 18976.85 | learning rate: 1.584E-04 | global batch size:  1024 | lm loss: 1.779467E+00 | loss scale: 1.0 | grad norm: 1.609 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1057/   14369 | consumed samples:      1082368 | elapsed time per iteration (ms): 111629.0 | rate (tokens/sec): 18786.80 | learning rate: 1.586E-04 | global batch size:  1024 | lm loss: 1.776791E+00 | loss scale: 1.0 | grad norm: 2.107 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1058/   14369 | consumed samples:      1083392 | elapsed time per iteration (ms): 111241.9 | rate (tokens/sec): 18852.18 | learning rate: 1.587E-04 | global batch size:  1024 | lm loss: 1.786409E+00 | loss scale: 1.0 | grad norm: 2.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1059/   14369 | consumed samples:      1084416 | elapsed time per iteration (ms): 111528.2 | rate (tokens/sec): 18803.78 | learning rate: 1.588E-04 | global batch size:  1024 | lm loss: 1.763128E+00 | loss scale: 1.0 | grad norm: 1.696 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1060/   14369 | consumed samples:      1085440 | elapsed time per iteration (ms): 111367.3 | rate (tokens/sec): 18830.95 | learning rate: 1.590E-04 | global batch size:  1024 | lm loss: 1.769880E+00 | loss scale: 1.0 | grad norm: 2.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1061/   14369 | consumed samples:      1086464 | elapsed time per iteration (ms): 111636.1 | rate (tokens/sec): 18785.62 | learning rate: 1.591E-04 | global batch size:  1024 | lm loss: 1.788679E+00 | loss scale: 1.0 | grad norm: 1.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1062/   14369 | consumed samples:      1087488 | elapsed time per iteration (ms): 112886.9 | rate (tokens/sec): 18577.46 | learning rate: 1.593E-04 | global batch size:  1024 | lm loss: 1.764540E+00 | loss scale: 1.0 | grad norm: 1.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1063/   14369 | consumed samples:      1088512 | elapsed time per iteration (ms): 111981.8 | rate (tokens/sec): 18727.61 | learning rate: 1.594E-04 | global batch size:  1024 | lm loss: 1.773731E+00 | loss scale: 1.0 | grad norm: 1.826 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1064/   14369 | consumed samples:      1089536 | elapsed time per iteration (ms): 112736.1 | rate (tokens/sec): 18602.31 | learning rate: 1.596E-04 | global batch size:  1024 | lm loss: 1.763612E+00 | loss scale: 1.0 | grad norm: 1.784 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1065/   14369 | consumed samples:      1090560 | elapsed time per iteration (ms): 113054.9 | rate (tokens/sec): 18549.85 | learning rate: 1.597E-04 | global batch size:  1024 | lm loss: 1.778242E+00 | loss scale: 1.0 | grad norm: 2.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1066/   14369 | consumed samples:      1091584 | elapsed time per iteration (ms): 111659.9 | rate (tokens/sec): 18781.60 | learning rate: 1.599E-04 | global batch size:  1024 | lm loss: 1.777609E+00 | loss scale: 1.0 | grad norm: 1.873 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1067/   14369 | consumed samples:      1092608 | elapsed time per iteration (ms): 111079.6 | rate (tokens/sec): 18879.72 | learning rate: 1.600E-04 | global batch size:  1024 | lm loss: 1.750329E+00 | loss scale: 1.0 | grad norm: 1.801 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1068/   14369 | consumed samples:      1093632 | elapsed time per iteration (ms): 116076.6 | rate (tokens/sec): 18066.97 | learning rate: 1.602E-04 | global batch size:  1024 | lm loss: 1.770129E+00 | loss scale: 1.0 | grad norm: 1.813 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1069/   14369 | consumed samples:      1094656 | elapsed time per iteration (ms): 112319.8 | rate (tokens/sec): 18671.26 | learning rate: 1.603E-04 | global batch size:  1024 | lm loss: 1.786468E+00 | loss scale: 1.0 | grad norm: 1.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1070/   14369 | consumed samples:      1095680 | elapsed time per iteration (ms): 113366.9 | rate (tokens/sec): 18498.80 | learning rate: 1.605E-04 | global batch size:  1024 | lm loss: 1.780898E+00 | loss scale: 1.0 | grad norm: 1.883 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1071/   14369 | consumed samples:      1096704 | elapsed time per iteration (ms): 111402.0 | rate (tokens/sec): 18825.09 | learning rate: 1.606E-04 | global batch size:  1024 | lm loss: 1.758023E+00 | loss scale: 1.0 | grad norm: 1.636 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1072/   14369 | consumed samples:      1097728 | elapsed time per iteration (ms): 112514.0 | rate (tokens/sec): 18639.03 | learning rate: 1.608E-04 | global batch size:  1024 | lm loss: 1.739861E+00 | loss scale: 1.0 | grad norm: 1.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1073/   14369 | consumed samples:      1098752 | elapsed time per iteration (ms): 111047.6 | rate (tokens/sec): 18885.16 | learning rate: 1.609E-04 | global batch size:  1024 | lm loss: 1.776602E+00 | loss scale: 1.0 | grad norm: 1.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1074/   14369 | consumed samples:      1099776 | elapsed time per iteration (ms): 111412.0 | rate (tokens/sec): 18823.39 | learning rate: 1.611E-04 | global batch size:  1024 | lm loss: 1.765029E+00 | loss scale: 1.0 | grad norm: 1.637 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1075/   14369 | consumed samples:      1100800 | elapsed time per iteration (ms): 110913.3 | rate (tokens/sec): 18908.03 | learning rate: 1.612E-04 | global batch size:  1024 | lm loss: 1.755743E+00 | loss scale: 1.0 | grad norm: 2.030 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1076/   14369 | consumed samples:      1101824 | elapsed time per iteration (ms): 112221.0 | rate (tokens/sec): 18687.69 | learning rate: 1.614E-04 | global batch size:  1024 | lm loss: 1.737553E+00 | loss scale: 1.0 | grad norm: 1.703 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1077/   14369 | consumed samples:      1102848 | elapsed time per iteration (ms): 111656.6 | rate (tokens/sec): 18782.16 | learning rate: 1.615E-04 | global batch size:  1024 | lm loss: 1.761584E+00 | loss scale: 1.0 | grad norm: 2.177 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1078/   14369 | consumed samples:      1103872 | elapsed time per iteration (ms): 111479.9 | rate (tokens/sec): 18811.93 | learning rate: 1.617E-04 | global batch size:  1024 | lm loss: 1.760534E+00 | loss scale: 1.0 | grad norm: 1.591 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1079/   14369 | consumed samples:      1104896 | elapsed time per iteration (ms): 111601.7 | rate (tokens/sec): 18791.40 | learning rate: 1.619E-04 | global batch size:  1024 | lm loss: 1.771318E+00 | loss scale: 1.0 | grad norm: 1.964 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1080/   14369 | consumed samples:      1105920 | elapsed time per iteration (ms): 111246.9 | rate (tokens/sec): 18851.33 | learning rate: 1.620E-04 | global batch size:  1024 | lm loss: 1.751612E+00 | loss scale: 1.0 | grad norm: 1.889 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1081/   14369 | consumed samples:      1106944 | elapsed time per iteration (ms): 111836.9 | rate (tokens/sec): 18751.88 | learning rate: 1.621E-04 | global batch size:  1024 | lm loss: 1.737021E+00 | loss scale: 1.0 | grad norm: 1.775 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1082/   14369 | consumed samples:      1107968 | elapsed time per iteration (ms): 112057.9 | rate (tokens/sec): 18714.90 | learning rate: 1.623E-04 | global batch size:  1024 | lm loss: 1.751975E+00 | loss scale: 1.0 | grad norm: 1.810 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1083/   14369 | consumed samples:      1108992 | elapsed time per iteration (ms): 111868.8 | rate (tokens/sec): 18746.53 | learning rate: 1.624E-04 | global batch size:  1024 | lm loss: 1.773690E+00 | loss scale: 1.0 | grad norm: 1.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1084/   14369 | consumed samples:      1110016 | elapsed time per iteration (ms): 112277.0 | rate (tokens/sec): 18678.37 | learning rate: 1.626E-04 | global batch size:  1024 | lm loss: 1.755438E+00 | loss scale: 1.0 | grad norm: 1.517 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1085/   14369 | consumed samples:      1111040 | elapsed time per iteration (ms): 111090.5 | rate (tokens/sec): 18877.86 | learning rate: 1.627E-04 | global batch size:  1024 | lm loss: 1.736544E+00 | loss scale: 1.0 | grad norm: 1.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1086/   14369 | consumed samples:      1112064 | elapsed time per iteration (ms): 111099.9 | rate (tokens/sec): 18876.27 | learning rate: 1.629E-04 | global batch size:  1024 | lm loss: 1.763590E+00 | loss scale: 1.0 | grad norm: 1.820 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1087/   14369 | consumed samples:      1113088 | elapsed time per iteration (ms): 111606.9 | rate (tokens/sec): 18790.52 | learning rate: 1.630E-04 | global batch size:  1024 | lm loss: 1.756197E+00 | loss scale: 1.0 | grad norm: 1.721 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1088/   14369 | consumed samples:      1114112 | elapsed time per iteration (ms): 111538.5 | rate (tokens/sec): 18802.05 | learning rate: 1.632E-04 | global batch size:  1024 | lm loss: 1.778962E+00 | loss scale: 1.0 | grad norm: 2.067 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1089/   14369 | consumed samples:      1115136 | elapsed time per iteration (ms): 111205.8 | rate (tokens/sec): 18858.30 | learning rate: 1.633E-04 | global batch size:  1024 | lm loss: 1.751737E+00 | loss scale: 1.0 | grad norm: 1.568 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1090/   14369 | consumed samples:      1116160 | elapsed time per iteration (ms): 111230.1 | rate (tokens/sec): 18854.18 | learning rate: 1.635E-04 | global batch size:  1024 | lm loss: 1.765151E+00 | loss scale: 1.0 | grad norm: 1.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1091/   14369 | consumed samples:      1117184 | elapsed time per iteration (ms): 112642.0 | rate (tokens/sec): 18617.85 | learning rate: 1.636E-04 | global batch size:  1024 | lm loss: 1.765410E+00 | loss scale: 1.0 | grad norm: 1.827 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1092/   14369 | consumed samples:      1118208 | elapsed time per iteration (ms): 112294.8 | rate (tokens/sec): 18675.41 | learning rate: 1.638E-04 | global batch size:  1024 | lm loss: 1.742701E+00 | loss scale: 1.0 | grad norm: 2.237 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1093/   14369 | consumed samples:      1119232 | elapsed time per iteration (ms): 112059.9 | rate (tokens/sec): 18714.56 | learning rate: 1.639E-04 | global batch size:  1024 | lm loss: 1.754854E+00 | loss scale: 1.0 | grad norm: 1.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1094/   14369 | consumed samples:      1120256 | elapsed time per iteration (ms): 111984.9 | rate (tokens/sec): 18727.10 | learning rate: 1.641E-04 | global batch size:  1024 | lm loss: 1.762629E+00 | loss scale: 1.0 | grad norm: 1.846 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1095/   14369 | consumed samples:      1121280 | elapsed time per iteration (ms): 110579.9 | rate (tokens/sec): 18965.04 | learning rate: 1.642E-04 | global batch size:  1024 | lm loss: 1.752758E+00 | loss scale: 1.0 | grad norm: 1.241 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1096/   14369 | consumed samples:      1122304 | elapsed time per iteration (ms): 111559.9 | rate (tokens/sec): 18798.44 | learning rate: 1.644E-04 | global batch size:  1024 | lm loss: 1.766803E+00 | loss scale: 1.0 | grad norm: 2.178 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1097/   14369 | consumed samples:      1123328 | elapsed time per iteration (ms): 110537.7 | rate (tokens/sec): 18972.27 | learning rate: 1.645E-04 | global batch size:  1024 | lm loss: 1.773787E+00 | loss scale: 1.0 | grad norm: 1.492 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1098/   14369 | consumed samples:      1124352 | elapsed time per iteration (ms): 111059.8 | rate (tokens/sec): 18883.09 | learning rate: 1.647E-04 | global batch size:  1024 | lm loss: 1.754284E+00 | loss scale: 1.0 | grad norm: 2.096 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1099/   14369 | consumed samples:      1125376 | elapsed time per iteration (ms): 111546.8 | rate (tokens/sec): 18800.65 | learning rate: 1.648E-04 | global batch size:  1024 | lm loss: 1.754097E+00 | loss scale: 1.0 | grad norm: 1.566 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1100/   14369 | consumed samples:      1126400 | elapsed time per iteration (ms): 111972.9 | rate (tokens/sec): 18729.11 | learning rate: 1.650E-04 | global batch size:  1024 | lm loss: 1.772968E+00 | loss scale: 1.0 | grad norm: 2.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1101/   14369 | consumed samples:      1127424 | elapsed time per iteration (ms): 112304.8 | rate (tokens/sec): 18673.75 | learning rate: 1.652E-04 | global batch size:  1024 | lm loss: 1.771792E+00 | loss scale: 1.0 | grad norm: 1.611 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1102/   14369 | consumed samples:      1128448 | elapsed time per iteration (ms): 111815.0 | rate (tokens/sec): 18755.55 | learning rate: 1.653E-04 | global batch size:  1024 | lm loss: 1.772752E+00 | loss scale: 1.0 | grad norm: 2.034 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1103/   14369 | consumed samples:      1129472 | elapsed time per iteration (ms): 110766.9 | rate (tokens/sec): 18933.02 | learning rate: 1.654E-04 | global batch size:  1024 | lm loss: 1.762704E+00 | loss scale: 1.0 | grad norm: 1.896 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1104/   14369 | consumed samples:      1130496 | elapsed time per iteration (ms): 111823.9 | rate (tokens/sec): 18754.07 | learning rate: 1.656E-04 | global batch size:  1024 | lm loss: 1.757698E+00 | loss scale: 1.0 | grad norm: 2.265 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1105/   14369 | consumed samples:      1131520 | elapsed time per iteration (ms): 112801.9 | rate (tokens/sec): 18591.46 | learning rate: 1.657E-04 | global batch size:  1024 | lm loss: 1.751232E+00 | loss scale: 1.0 | grad norm: 1.159 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1106/   14369 | consumed samples:      1132544 | elapsed time per iteration (ms): 111697.9 | rate (tokens/sec): 18775.21 | learning rate: 1.659E-04 | global batch size:  1024 | lm loss: 1.762065E+00 | loss scale: 1.0 | grad norm: 2.070 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1107/   14369 | consumed samples:      1133568 | elapsed time per iteration (ms): 111585.9 | rate (tokens/sec): 18794.06 | learning rate: 1.660E-04 | global batch size:  1024 | lm loss: 1.778790E+00 | loss scale: 1.0 | grad norm: 1.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1108/   14369 | consumed samples:      1134592 | elapsed time per iteration (ms): 111514.0 | rate (tokens/sec): 18806.17 | learning rate: 1.662E-04 | global batch size:  1024 | lm loss: 1.764742E+00 | loss scale: 1.0 | grad norm: 2.044 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1109/   14369 | consumed samples:      1135616 | elapsed time per iteration (ms): 111303.7 | rate (tokens/sec): 18841.71 | learning rate: 1.664E-04 | global batch size:  1024 | lm loss: 1.781420E+00 | loss scale: 1.0 | grad norm: 1.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1110/   14369 | consumed samples:      1136640 | elapsed time per iteration (ms): 111466.3 | rate (tokens/sec): 18814.23 | learning rate: 1.665E-04 | global batch size:  1024 | lm loss: 1.759768E+00 | loss scale: 1.0 | grad norm: 1.787 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1111/   14369 | consumed samples:      1137664 | elapsed time per iteration (ms): 111759.9 | rate (tokens/sec): 18764.80 | learning rate: 1.666E-04 | global batch size:  1024 | lm loss: 1.758742E+00 | loss scale: 1.0 | grad norm: 2.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1112/   14369 | consumed samples:      1138688 | elapsed time per iteration (ms): 111581.7 | rate (tokens/sec): 18794.77 | learning rate: 1.668E-04 | global batch size:  1024 | lm loss: 1.768786E+00 | loss scale: 1.0 | grad norm: 1.719 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1113/   14369 | consumed samples:      1139712 | elapsed time per iteration (ms): 111840.9 | rate (tokens/sec): 18751.21 | learning rate: 1.669E-04 | global batch size:  1024 | lm loss: 1.761145E+00 | loss scale: 1.0 | grad norm: 1.673 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1114/   14369 | consumed samples:      1140736 | elapsed time per iteration (ms): 110641.7 | rate (tokens/sec): 18954.45 | learning rate: 1.671E-04 | global batch size:  1024 | lm loss: 1.757207E+00 | loss scale: 1.0 | grad norm: 1.556 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1115/   14369 | consumed samples:      1141760 | elapsed time per iteration (ms): 111821.9 | rate (tokens/sec): 18754.40 | learning rate: 1.672E-04 | global batch size:  1024 | lm loss: 1.771702E+00 | loss scale: 1.0 | grad norm: 2.062 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1116/   14369 | consumed samples:      1142784 | elapsed time per iteration (ms): 112991.3 | rate (tokens/sec): 18560.29 | learning rate: 1.674E-04 | global batch size:  1024 | lm loss: 1.769309E+00 | loss scale: 1.0 | grad norm: 1.445 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1117/   14369 | consumed samples:      1143808 | elapsed time per iteration (ms): 111821.7 | rate (tokens/sec): 18754.43 | learning rate: 1.675E-04 | global batch size:  1024 | lm loss: 1.767907E+00 | loss scale: 1.0 | grad norm: 1.821 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1118/   14369 | consumed samples:      1144832 | elapsed time per iteration (ms): 111026.0 | rate (tokens/sec): 18888.83 | learning rate: 1.677E-04 | global batch size:  1024 | lm loss: 1.761726E+00 | loss scale: 1.0 | grad norm: 1.895 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1119/   14369 | consumed samples:      1145856 | elapsed time per iteration (ms): 111666.5 | rate (tokens/sec): 18780.50 | learning rate: 1.678E-04 | global batch size:  1024 | lm loss: 1.751987E+00 | loss scale: 1.0 | grad norm: 1.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1120/   14369 | consumed samples:      1146880 | elapsed time per iteration (ms): 111810.9 | rate (tokens/sec): 18756.23 | learning rate: 1.680E-04 | global batch size:  1024 | lm loss: 1.762813E+00 | loss scale: 1.0 | grad norm: 1.698 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1121/   14369 | consumed samples:      1147904 | elapsed time per iteration (ms): 111827.4 | rate (tokens/sec): 18753.48 | learning rate: 1.681E-04 | global batch size:  1024 | lm loss: 1.755555E+00 | loss scale: 1.0 | grad norm: 1.929 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1122/   14369 | consumed samples:      1148928 | elapsed time per iteration (ms): 111740.0 | rate (tokens/sec): 18768.13 | learning rate: 1.683E-04 | global batch size:  1024 | lm loss: 1.745209E+00 | loss scale: 1.0 | grad norm: 1.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1123/   14369 | consumed samples:      1149952 | elapsed time per iteration (ms): 113476.0 | rate (tokens/sec): 18481.02 | learning rate: 1.685E-04 | global batch size:  1024 | lm loss: 1.721154E+00 | loss scale: 1.0 | grad norm: 1.839 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1124/   14369 | consumed samples:      1150976 | elapsed time per iteration (ms): 113276.8 | rate (tokens/sec): 18513.52 | learning rate: 1.686E-04 | global batch size:  1024 | lm loss: 1.738703E+00 | loss scale: 1.0 | grad norm: 1.451 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1125/   14369 | consumed samples:      1152000 | elapsed time per iteration (ms): 111793.0 | rate (tokens/sec): 18759.24 | learning rate: 1.687E-04 | global batch size:  1024 | lm loss: 1.752743E+00 | loss scale: 1.0 | grad norm: 2.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1126/   14369 | consumed samples:      1153024 | elapsed time per iteration (ms): 111200.3 | rate (tokens/sec): 18859.24 | learning rate: 1.689E-04 | global batch size:  1024 | lm loss: 1.720572E+00 | loss scale: 1.0 | grad norm: 1.451 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1127/   14369 | consumed samples:      1154048 | elapsed time per iteration (ms): 111521.8 | rate (tokens/sec): 18804.85 | learning rate: 1.690E-04 | global batch size:  1024 | lm loss: 1.779181E+00 | loss scale: 1.0 | grad norm: 1.959 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1128/   14369 | consumed samples:      1155072 | elapsed time per iteration (ms): 110959.0 | rate (tokens/sec): 18900.24 | learning rate: 1.692E-04 | global batch size:  1024 | lm loss: 1.761191E+00 | loss scale: 1.0 | grad norm: 1.684 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1129/   14369 | consumed samples:      1156096 | elapsed time per iteration (ms): 112382.7 | rate (tokens/sec): 18660.81 | learning rate: 1.693E-04 | global batch size:  1024 | lm loss: 1.749388E+00 | loss scale: 1.0 | grad norm: 1.772 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1130/   14369 | consumed samples:      1157120 | elapsed time per iteration (ms): 112179.8 | rate (tokens/sec): 18694.57 | learning rate: 1.695E-04 | global batch size:  1024 | lm loss: 1.752888E+00 | loss scale: 1.0 | grad norm: 1.937 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1131/   14369 | consumed samples:      1158144 | elapsed time per iteration (ms): 111521.7 | rate (tokens/sec): 18804.89 | learning rate: 1.697E-04 | global batch size:  1024 | lm loss: 1.729248E+00 | loss scale: 1.0 | grad norm: 1.573 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1132/   14369 | consumed samples:      1159168 | elapsed time per iteration (ms): 111118.9 | rate (tokens/sec): 18873.05 | learning rate: 1.698E-04 | global batch size:  1024 | lm loss: 1.723593E+00 | loss scale: 1.0 | grad norm: 1.702 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1133/   14369 | consumed samples:      1160192 | elapsed time per iteration (ms): 112201.7 | rate (tokens/sec): 18690.91 | learning rate: 1.699E-04 | global batch size:  1024 | lm loss: 1.743586E+00 | loss scale: 1.0 | grad norm: 1.654 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1134/   14369 | consumed samples:      1161216 | elapsed time per iteration (ms): 111489.1 | rate (tokens/sec): 18810.38 | learning rate: 1.701E-04 | global batch size:  1024 | lm loss: 1.757750E+00 | loss scale: 1.0 | grad norm: 1.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1135/   14369 | consumed samples:      1162240 | elapsed time per iteration (ms): 111382.7 | rate (tokens/sec): 18828.34 | learning rate: 1.702E-04 | global batch size:  1024 | lm loss: 1.738051E+00 | loss scale: 1.0 | grad norm: 1.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1136/   14369 | consumed samples:      1163264 | elapsed time per iteration (ms): 111420.0 | rate (tokens/sec): 18822.04 | learning rate: 1.704E-04 | global batch size:  1024 | lm loss: 1.762967E+00 | loss scale: 1.0 | grad norm: 1.998 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1137/   14369 | consumed samples:      1164288 | elapsed time per iteration (ms): 111820.0 | rate (tokens/sec): 18754.72 | learning rate: 1.705E-04 | global batch size:  1024 | lm loss: 1.749192E+00 | loss scale: 1.0 | grad norm: 1.559 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1138/   14369 | consumed samples:      1165312 | elapsed time per iteration (ms): 111417.9 | rate (tokens/sec): 18822.40 | learning rate: 1.707E-04 | global batch size:  1024 | lm loss: 1.746206E+00 | loss scale: 1.0 | grad norm: 2.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1139/   14369 | consumed samples:      1166336 | elapsed time per iteration (ms): 111201.7 | rate (tokens/sec): 18858.99 | learning rate: 1.708E-04 | global batch size:  1024 | lm loss: 1.752095E+00 | loss scale: 1.0 | grad norm: 1.336 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1140/   14369 | consumed samples:      1167360 | elapsed time per iteration (ms): 114460.1 | rate (tokens/sec): 18322.12 | learning rate: 1.710E-04 | global batch size:  1024 | lm loss: 1.752119E+00 | loss scale: 1.0 | grad norm: 2.175 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1141/   14369 | consumed samples:      1168384 | elapsed time per iteration (ms): 111607.0 | rate (tokens/sec): 18790.50 | learning rate: 1.711E-04 | global batch size:  1024 | lm loss: 1.742698E+00 | loss scale: 1.0 | grad norm: 1.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1142/   14369 | consumed samples:      1169408 | elapsed time per iteration (ms): 110865.8 | rate (tokens/sec): 18916.13 | learning rate: 1.713E-04 | global batch size:  1024 | lm loss: 1.755925E+00 | loss scale: 1.0 | grad norm: 2.053 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1143/   14369 | consumed samples:      1170432 | elapsed time per iteration (ms): 111433.0 | rate (tokens/sec): 18819.84 | learning rate: 1.714E-04 | global batch size:  1024 | lm loss: 1.741513E+00 | loss scale: 1.0 | grad norm: 1.664 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1144/   14369 | consumed samples:      1171456 | elapsed time per iteration (ms): 111481.7 | rate (tokens/sec): 18811.63 | learning rate: 1.716E-04 | global batch size:  1024 | lm loss: 1.751417E+00 | loss scale: 1.0 | grad norm: 1.594 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1145/   14369 | consumed samples:      1172480 | elapsed time per iteration (ms): 111137.9 | rate (tokens/sec): 18869.82 | learning rate: 1.717E-04 | global batch size:  1024 | lm loss: 1.768004E+00 | loss scale: 1.0 | grad norm: 1.945 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1146/   14369 | consumed samples:      1173504 | elapsed time per iteration (ms): 111539.9 | rate (tokens/sec): 18801.81 | learning rate: 1.719E-04 | global batch size:  1024 | lm loss: 1.747256E+00 | loss scale: 1.0 | grad norm: 1.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1147/   14369 | consumed samples:      1174528 | elapsed time per iteration (ms): 111384.7 | rate (tokens/sec): 18828.01 | learning rate: 1.720E-04 | global batch size:  1024 | lm loss: 1.759325E+00 | loss scale: 1.0 | grad norm: 1.862 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1148/   14369 | consumed samples:      1175552 | elapsed time per iteration (ms): 111541.9 | rate (tokens/sec): 18801.48 | learning rate: 1.722E-04 | global batch size:  1024 | lm loss: 1.742276E+00 | loss scale: 1.0 | grad norm: 1.689 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1149/   14369 | consumed samples:      1176576 | elapsed time per iteration (ms): 111428.9 | rate (tokens/sec): 18820.54 | learning rate: 1.723E-04 | global batch size:  1024 | lm loss: 1.771800E+00 | loss scale: 1.0 | grad norm: 2.375 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1150/   14369 | consumed samples:      1177600 | elapsed time per iteration (ms): 111460.0 | rate (tokens/sec): 18815.29 | learning rate: 1.725E-04 | global batch size:  1024 | lm loss: 1.739619E+00 | loss scale: 1.0 | grad norm: 1.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1151/   14369 | consumed samples:      1178624 | elapsed time per iteration (ms): 111161.7 | rate (tokens/sec): 18865.77 | learning rate: 1.726E-04 | global batch size:  1024 | lm loss: 1.769739E+00 | loss scale: 1.0 | grad norm: 2.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1152/   14369 | consumed samples:      1179648 | elapsed time per iteration (ms): 111120.0 | rate (tokens/sec): 18872.85 | learning rate: 1.728E-04 | global batch size:  1024 | lm loss: 1.740374E+00 | loss scale: 1.0 | grad norm: 1.247 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1153/   14369 | consumed samples:      1180672 | elapsed time per iteration (ms): 111756.7 | rate (tokens/sec): 18765.34 | learning rate: 1.730E-04 | global batch size:  1024 | lm loss: 1.737531E+00 | loss scale: 1.0 | grad norm: 2.247 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1154/   14369 | consumed samples:      1181696 | elapsed time per iteration (ms): 110646.9 | rate (tokens/sec): 18953.55 | learning rate: 1.731E-04 | global batch size:  1024 | lm loss: 1.753255E+00 | loss scale: 1.0 | grad norm: 1.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1155/   14369 | consumed samples:      1182720 | elapsed time per iteration (ms): 112120.0 | rate (tokens/sec): 18704.53 | learning rate: 1.732E-04 | global batch size:  1024 | lm loss: 1.758920E+00 | loss scale: 1.0 | grad norm: 2.146 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1156/   14369 | consumed samples:      1183744 | elapsed time per iteration (ms): 111037.6 | rate (tokens/sec): 18886.86 | learning rate: 1.734E-04 | global batch size:  1024 | lm loss: 1.768547E+00 | loss scale: 1.0 | grad norm: 1.684 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1157/   14369 | consumed samples:      1184768 | elapsed time per iteration (ms): 111237.0 | rate (tokens/sec): 18853.00 | learning rate: 1.735E-04 | global batch size:  1024 | lm loss: 1.765784E+00 | loss scale: 1.0 | grad norm: 1.787 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1158/   14369 | consumed samples:      1185792 | elapsed time per iteration (ms): 111772.6 | rate (tokens/sec): 18762.67 | learning rate: 1.737E-04 | global batch size:  1024 | lm loss: 1.782815E+00 | loss scale: 1.0 | grad norm: 2.096 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1159/   14369 | consumed samples:      1186816 | elapsed time per iteration (ms): 111906.9 | rate (tokens/sec): 18740.15 | learning rate: 1.738E-04 | global batch size:  1024 | lm loss: 1.779341E+00 | loss scale: 1.0 | grad norm: 1.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1160/   14369 | consumed samples:      1187840 | elapsed time per iteration (ms): 111266.1 | rate (tokens/sec): 18848.08 | learning rate: 1.740E-04 | global batch size:  1024 | lm loss: 1.749423E+00 | loss scale: 1.0 | grad norm: 1.923 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1161/   14369 | consumed samples:      1188864 | elapsed time per iteration (ms): 111410.6 | rate (tokens/sec): 18823.63 | learning rate: 1.741E-04 | global batch size:  1024 | lm loss: 1.769040E+00 | loss scale: 1.0 | grad norm: 1.826 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1162/   14369 | consumed samples:      1189888 | elapsed time per iteration (ms): 111080.3 | rate (tokens/sec): 18879.60 | learning rate: 1.743E-04 | global batch size:  1024 | lm loss: 1.740877E+00 | loss scale: 1.0 | grad norm: 1.949 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1163/   14369 | consumed samples:      1190912 | elapsed time per iteration (ms): 111400.0 | rate (tokens/sec): 18825.42 | learning rate: 1.744E-04 | global batch size:  1024 | lm loss: 1.754396E+00 | loss scale: 1.0 | grad norm: 1.430 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1164/   14369 | consumed samples:      1191936 | elapsed time per iteration (ms): 110321.7 | rate (tokens/sec): 19009.42 | learning rate: 1.746E-04 | global batch size:  1024 | lm loss: 1.754776E+00 | loss scale: 1.0 | grad norm: 1.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1165/   14369 | consumed samples:      1192960 | elapsed time per iteration (ms): 111325.2 | rate (tokens/sec): 18838.07 | learning rate: 1.747E-04 | global batch size:  1024 | lm loss: 1.772822E+00 | loss scale: 1.0 | grad norm: 1.826 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1166/   14369 | consumed samples:      1193984 | elapsed time per iteration (ms): 110714.8 | rate (tokens/sec): 18941.93 | learning rate: 1.749E-04 | global batch size:  1024 | lm loss: 1.740565E+00 | loss scale: 1.0 | grad norm: 1.713 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1167/   14369 | consumed samples:      1195008 | elapsed time per iteration (ms): 111519.7 | rate (tokens/sec): 18805.21 | learning rate: 1.750E-04 | global batch size:  1024 | lm loss: 1.738397E+00 | loss scale: 1.0 | grad norm: 1.912 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1168/   14369 | consumed samples:      1196032 | elapsed time per iteration (ms): 111656.0 | rate (tokens/sec): 18782.26 | learning rate: 1.752E-04 | global batch size:  1024 | lm loss: 1.731424E+00 | loss scale: 1.0 | grad norm: 1.590 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1169/   14369 | consumed samples:      1197056 | elapsed time per iteration (ms): 111414.8 | rate (tokens/sec): 18822.92 | learning rate: 1.753E-04 | global batch size:  1024 | lm loss: 1.721855E+00 | loss scale: 1.0 | grad norm: 1.565 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1170/   14369 | consumed samples:      1198080 | elapsed time per iteration (ms): 111252.0 | rate (tokens/sec): 18850.47 | learning rate: 1.755E-04 | global batch size:  1024 | lm loss: 1.746238E+00 | loss scale: 1.0 | grad norm: 1.852 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1171/   14369 | consumed samples:      1199104 | elapsed time per iteration (ms): 111201.7 | rate (tokens/sec): 18859.00 | learning rate: 1.756E-04 | global batch size:  1024 | lm loss: 1.741871E+00 | loss scale: 1.0 | grad norm: 1.575 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1172/   14369 | consumed samples:      1200128 | elapsed time per iteration (ms): 112141.7 | rate (tokens/sec): 18700.91 | learning rate: 1.758E-04 | global batch size:  1024 | lm loss: 1.774678E+00 | loss scale: 1.0 | grad norm: 1.990 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1173/   14369 | consumed samples:      1201152 | elapsed time per iteration (ms): 111616.9 | rate (tokens/sec): 18788.84 | learning rate: 1.759E-04 | global batch size:  1024 | lm loss: 1.743799E+00 | loss scale: 1.0 | grad norm: 1.545 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1174/   14369 | consumed samples:      1202176 | elapsed time per iteration (ms): 111227.9 | rate (tokens/sec): 18854.55 | learning rate: 1.761E-04 | global batch size:  1024 | lm loss: 1.733377E+00 | loss scale: 1.0 | grad norm: 2.161 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1175/   14369 | consumed samples:      1203200 | elapsed time per iteration (ms): 112886.0 | rate (tokens/sec): 18577.61 | learning rate: 1.763E-04 | global batch size:  1024 | lm loss: 1.740668E+00 | loss scale: 1.0 | grad norm: 1.087 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1176/   14369 | consumed samples:      1204224 | elapsed time per iteration (ms): 111914.9 | rate (tokens/sec): 18738.81 | learning rate: 1.764E-04 | global batch size:  1024 | lm loss: 1.764396E+00 | loss scale: 1.0 | grad norm: 2.039 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1177/   14369 | consumed samples:      1205248 | elapsed time per iteration (ms): 110894.6 | rate (tokens/sec): 18911.22 | learning rate: 1.765E-04 | global batch size:  1024 | lm loss: 1.765726E+00 | loss scale: 1.0 | grad norm: 1.758 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1178/   14369 | consumed samples:      1206272 | elapsed time per iteration (ms): 110802.7 | rate (tokens/sec): 18926.91 | learning rate: 1.767E-04 | global batch size:  1024 | lm loss: 1.747146E+00 | loss scale: 1.0 | grad norm: 2.039 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1179/   14369 | consumed samples:      1207296 | elapsed time per iteration (ms): 110694.9 | rate (tokens/sec): 18945.34 | learning rate: 1.768E-04 | global batch size:  1024 | lm loss: 1.749724E+00 | loss scale: 1.0 | grad norm: 1.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1180/   14369 | consumed samples:      1208320 | elapsed time per iteration (ms): 111656.6 | rate (tokens/sec): 18782.16 | learning rate: 1.770E-04 | global batch size:  1024 | lm loss: 1.735316E+00 | loss scale: 1.0 | grad norm: 1.907 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1181/   14369 | consumed samples:      1209344 | elapsed time per iteration (ms): 110690.1 | rate (tokens/sec): 18946.15 | learning rate: 1.771E-04 | global batch size:  1024 | lm loss: 1.740651E+00 | loss scale: 1.0 | grad norm: 1.479 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1182/   14369 | consumed samples:      1210368 | elapsed time per iteration (ms): 110746.9 | rate (tokens/sec): 18936.44 | learning rate: 1.773E-04 | global batch size:  1024 | lm loss: 1.731111E+00 | loss scale: 1.0 | grad norm: 1.315 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1183/   14369 | consumed samples:      1211392 | elapsed time per iteration (ms): 111953.0 | rate (tokens/sec): 18732.43 | learning rate: 1.775E-04 | global batch size:  1024 | lm loss: 1.745333E+00 | loss scale: 1.0 | grad norm: 1.608 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1184/   14369 | consumed samples:      1212416 | elapsed time per iteration (ms): 111419.9 | rate (tokens/sec): 18822.06 | learning rate: 1.776E-04 | global batch size:  1024 | lm loss: 1.725438E+00 | loss scale: 1.0 | grad norm: 1.750 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1185/   14369 | consumed samples:      1213440 | elapsed time per iteration (ms): 111001.7 | rate (tokens/sec): 18892.97 | learning rate: 1.777E-04 | global batch size:  1024 | lm loss: 1.748610E+00 | loss scale: 1.0 | grad norm: 1.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1186/   14369 | consumed samples:      1214464 | elapsed time per iteration (ms): 110861.7 | rate (tokens/sec): 18916.83 | learning rate: 1.779E-04 | global batch size:  1024 | lm loss: 1.727998E+00 | loss scale: 1.0 | grad norm: 2.210 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1187/   14369 | consumed samples:      1215488 | elapsed time per iteration (ms): 110181.0 | rate (tokens/sec): 19033.70 | learning rate: 1.780E-04 | global batch size:  1024 | lm loss: 1.746636E+00 | loss scale: 1.0 | grad norm: 1.090 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1188/   14369 | consumed samples:      1216512 | elapsed time per iteration (ms): 111681.8 | rate (tokens/sec): 18777.92 | learning rate: 1.782E-04 | global batch size:  1024 | lm loss: 1.773695E+00 | loss scale: 1.0 | grad norm: 2.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1189/   14369 | consumed samples:      1217536 | elapsed time per iteration (ms): 111639.9 | rate (tokens/sec): 18784.97 | learning rate: 1.783E-04 | global batch size:  1024 | lm loss: 1.772349E+00 | loss scale: 1.0 | grad norm: 1.447 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1190/   14369 | consumed samples:      1218560 | elapsed time per iteration (ms): 112197.9 | rate (tokens/sec): 18691.55 | learning rate: 1.785E-04 | global batch size:  1024 | lm loss: 1.758530E+00 | loss scale: 1.0 | grad norm: 2.506 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1191/   14369 | consumed samples:      1219584 | elapsed time per iteration (ms): 110841.7 | rate (tokens/sec): 18920.24 | learning rate: 1.786E-04 | global batch size:  1024 | lm loss: 1.781981E+00 | loss scale: 1.0 | grad norm: 1.716 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1192/   14369 | consumed samples:      1220608 | elapsed time per iteration (ms): 111742.0 | rate (tokens/sec): 18767.80 | learning rate: 1.788E-04 | global batch size:  1024 | lm loss: 1.778096E+00 | loss scale: 1.0 | grad norm: 1.896 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1193/   14369 | consumed samples:      1221632 | elapsed time per iteration (ms): 111061.8 | rate (tokens/sec): 18882.74 | learning rate: 1.789E-04 | global batch size:  1024 | lm loss: 1.748373E+00 | loss scale: 1.0 | grad norm: 1.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1194/   14369 | consumed samples:      1222656 | elapsed time per iteration (ms): 116000.0 | rate (tokens/sec): 18078.89 | learning rate: 1.791E-04 | global batch size:  1024 | lm loss: 1.750270E+00 | loss scale: 1.0 | grad norm: 1.781 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1195/   14369 | consumed samples:      1223680 | elapsed time per iteration (ms): 111565.0 | rate (tokens/sec): 18797.57 | learning rate: 1.792E-04 | global batch size:  1024 | lm loss: 1.767354E+00 | loss scale: 1.0 | grad norm: 1.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1196/   14369 | consumed samples:      1224704 | elapsed time per iteration (ms): 111821.7 | rate (tokens/sec): 18754.42 | learning rate: 1.794E-04 | global batch size:  1024 | lm loss: 1.754297E+00 | loss scale: 1.0 | grad norm: 1.686 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1197/   14369 | consumed samples:      1225728 | elapsed time per iteration (ms): 111906.9 | rate (tokens/sec): 18740.15 | learning rate: 1.796E-04 | global batch size:  1024 | lm loss: 1.756661E+00 | loss scale: 1.0 | grad norm: 1.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1198/   14369 | consumed samples:      1226752 | elapsed time per iteration (ms): 110593.0 | rate (tokens/sec): 18962.79 | learning rate: 1.797E-04 | global batch size:  1024 | lm loss: 1.752190E+00 | loss scale: 1.0 | grad norm: 1.603 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1199/   14369 | consumed samples:      1227776 | elapsed time per iteration (ms): 112081.8 | rate (tokens/sec): 18710.91 | learning rate: 1.798E-04 | global batch size:  1024 | lm loss: 1.753709E+00 | loss scale: 1.0 | grad norm: 1.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1200/   14369 | consumed samples:      1228800 | elapsed time per iteration (ms): 111421.8 | rate (tokens/sec): 18821.74 | learning rate: 1.800E-04 | global batch size:  1024 | lm loss: 1.755187E+00 | loss scale: 1.0 | grad norm: 1.519 | number of skipped iterations:   0 | number of nan iterations:   0 |
------------------------------------------------------------------------------------------------
 validation loss at iteration 1200 | lm loss value: 1.910739E+00 | lm loss PPL: 6.758082E+00 | 
------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------
 test loss at iteration 1200 | lm loss value: 3.068352E+00 | lm loss PPL: 2.150644E+01 | 
------------------------------------------------------------------------------------------
saving checkpoint at iteration    1200 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
  successfully saved checkpoint at iteration    1200 to /scratch/cse/btech/cs1200448/MatLlama/meditron-checkpoints/llama3
(min, max) time across ranks (ms):
    save-checkpoint ................................: (114598.24, 114598.91)
 iteration     1201/   14369 | consumed samples:      1229824 | elapsed time per iteration (ms): 1199184.0 | rate (tokens/sec): 1748.82 | learning rate: 1.801E-04 | global batch size:  1024 | lm loss: 1.743720E+00 | loss scale: 1.0 | grad norm: 2.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1202/   14369 | consumed samples:      1230848 | elapsed time per iteration (ms): 110733.0 | rate (tokens/sec): 18938.81 | learning rate: 1.803E-04 | global batch size:  1024 | lm loss: 1.740182E+00 | loss scale: 1.0 | grad norm: 1.835 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1203/   14369 | consumed samples:      1231872 | elapsed time per iteration (ms): 111739.9 | rate (tokens/sec): 18768.16 | learning rate: 1.804E-04 | global batch size:  1024 | lm loss: 1.757959E+00 | loss scale: 1.0 | grad norm: 2.301 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1204/   14369 | consumed samples:      1232896 | elapsed time per iteration (ms): 112339.9 | rate (tokens/sec): 18667.92 | learning rate: 1.806E-04 | global batch size:  1024 | lm loss: 1.728164E+00 | loss scale: 1.0 | grad norm: 1.155 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1205/   14369 | consumed samples:      1233920 | elapsed time per iteration (ms): 111637.6 | rate (tokens/sec): 18785.35 | learning rate: 1.808E-04 | global batch size:  1024 | lm loss: 1.744421E+00 | loss scale: 1.0 | grad norm: 1.837 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1206/   14369 | consumed samples:      1234944 | elapsed time per iteration (ms): 112323.7 | rate (tokens/sec): 18670.62 | learning rate: 1.809E-04 | global batch size:  1024 | lm loss: 1.749117E+00 | loss scale: 1.0 | grad norm: 1.637 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1207/   14369 | consumed samples:      1235968 | elapsed time per iteration (ms): 113748.7 | rate (tokens/sec): 18436.72 | learning rate: 1.810E-04 | global batch size:  1024 | lm loss: 1.746773E+00 | loss scale: 1.0 | grad norm: 2.134 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1208/   14369 | consumed samples:      1236992 | elapsed time per iteration (ms): 112341.7 | rate (tokens/sec): 18667.62 | learning rate: 1.812E-04 | global batch size:  1024 | lm loss: 1.743274E+00 | loss scale: 1.0 | grad norm: 1.772 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1209/   14369 | consumed samples:      1238016 | elapsed time per iteration (ms): 113181.8 | rate (tokens/sec): 18529.05 | learning rate: 1.813E-04 | global batch size:  1024 | lm loss: 1.729801E+00 | loss scale: 1.0 | grad norm: 2.067 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1210/   14369 | consumed samples:      1239040 | elapsed time per iteration (ms): 112891.8 | rate (tokens/sec): 18576.65 | learning rate: 1.815E-04 | global batch size:  1024 | lm loss: 1.739820E+00 | loss scale: 1.0 | grad norm: 1.445 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1211/   14369 | consumed samples:      1240064 | elapsed time per iteration (ms): 113100.1 | rate (tokens/sec): 18542.44 | learning rate: 1.816E-04 | global batch size:  1024 | lm loss: 1.754383E+00 | loss scale: 1.0 | grad norm: 1.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1212/   14369 | consumed samples:      1241088 | elapsed time per iteration (ms): 112938.9 | rate (tokens/sec): 18568.91 | learning rate: 1.818E-04 | global batch size:  1024 | lm loss: 1.744103E+00 | loss scale: 1.0 | grad norm: 1.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1213/   14369 | consumed samples:      1242112 | elapsed time per iteration (ms): 111099.9 | rate (tokens/sec): 18876.27 | learning rate: 1.819E-04 | global batch size:  1024 | lm loss: 1.755020E+00 | loss scale: 1.0 | grad norm: 1.745 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1214/   14369 | consumed samples:      1243136 | elapsed time per iteration (ms): 111881.7 | rate (tokens/sec): 18744.37 | learning rate: 1.821E-04 | global batch size:  1024 | lm loss: 1.723196E+00 | loss scale: 1.0 | grad norm: 1.506 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1215/   14369 | consumed samples:      1244160 | elapsed time per iteration (ms): 111838.9 | rate (tokens/sec): 18751.55 | learning rate: 1.822E-04 | global batch size:  1024 | lm loss: 1.734319E+00 | loss scale: 1.0 | grad norm: 2.043 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1216/   14369 | consumed samples:      1245184 | elapsed time per iteration (ms): 112021.7 | rate (tokens/sec): 18720.94 | learning rate: 1.824E-04 | global batch size:  1024 | lm loss: 1.743520E+00 | loss scale: 1.0 | grad norm: 1.950 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1217/   14369 | consumed samples:      1246208 | elapsed time per iteration (ms): 112463.5 | rate (tokens/sec): 18647.39 | learning rate: 1.825E-04 | global batch size:  1024 | lm loss: 1.762725E+00 | loss scale: 1.0 | grad norm: 1.560 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1218/   14369 | consumed samples:      1247232 | elapsed time per iteration (ms): 110779.9 | rate (tokens/sec): 18930.80 | learning rate: 1.827E-04 | global batch size:  1024 | lm loss: 1.735625E+00 | loss scale: 1.0 | grad norm: 2.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1219/   14369 | consumed samples:      1248256 | elapsed time per iteration (ms): 111739.9 | rate (tokens/sec): 18768.16 | learning rate: 1.828E-04 | global batch size:  1024 | lm loss: 1.738771E+00 | loss scale: 1.0 | grad norm: 1.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1220/   14369 | consumed samples:      1249280 | elapsed time per iteration (ms): 111721.1 | rate (tokens/sec): 18771.31 | learning rate: 1.830E-04 | global batch size:  1024 | lm loss: 1.728342E+00 | loss scale: 1.0 | grad norm: 1.865 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1221/   14369 | consumed samples:      1250304 | elapsed time per iteration (ms): 111214.1 | rate (tokens/sec): 18856.88 | learning rate: 1.831E-04 | global batch size:  1024 | lm loss: 1.739960E+00 | loss scale: 1.0 | grad norm: 1.450 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1222/   14369 | consumed samples:      1251328 | elapsed time per iteration (ms): 113699.9 | rate (tokens/sec): 18444.63 | learning rate: 1.833E-04 | global batch size:  1024 | lm loss: 1.717644E+00 | loss scale: 1.0 | grad norm: 1.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1223/   14369 | consumed samples:      1252352 | elapsed time per iteration (ms): 112163.9 | rate (tokens/sec): 18697.21 | learning rate: 1.834E-04 | global batch size:  1024 | lm loss: 1.729770E+00 | loss scale: 1.0 | grad norm: 1.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1224/   14369 | consumed samples:      1253376 | elapsed time per iteration (ms): 111555.8 | rate (tokens/sec): 18799.13 | learning rate: 1.836E-04 | global batch size:  1024 | lm loss: 1.729795E+00 | loss scale: 1.0 | grad norm: 1.720 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1225/   14369 | consumed samples:      1254400 | elapsed time per iteration (ms): 111907.7 | rate (tokens/sec): 18740.02 | learning rate: 1.837E-04 | global batch size:  1024 | lm loss: 1.727339E+00 | loss scale: 1.0 | grad norm: 1.687 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1226/   14369 | consumed samples:      1255424 | elapsed time per iteration (ms): 113920.0 | rate (tokens/sec): 18408.98 | learning rate: 1.839E-04 | global batch size:  1024 | lm loss: 1.747450E+00 | loss scale: 1.0 | grad norm: 1.893 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1227/   14369 | consumed samples:      1256448 | elapsed time per iteration (ms): 112840.0 | rate (tokens/sec): 18585.18 | learning rate: 1.841E-04 | global batch size:  1024 | lm loss: 1.712875E+00 | loss scale: 1.0 | grad norm: 1.199 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1228/   14369 | consumed samples:      1257472 | elapsed time per iteration (ms): 111841.7 | rate (tokens/sec): 18751.08 | learning rate: 1.842E-04 | global batch size:  1024 | lm loss: 1.744606E+00 | loss scale: 1.0 | grad norm: 2.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1229/   14369 | consumed samples:      1258496 | elapsed time per iteration (ms): 111921.8 | rate (tokens/sec): 18737.65 | learning rate: 1.843E-04 | global batch size:  1024 | lm loss: 1.751818E+00 | loss scale: 1.0 | grad norm: 1.457 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1230/   14369 | consumed samples:      1259520 | elapsed time per iteration (ms): 113741.7 | rate (tokens/sec): 18437.84 | learning rate: 1.845E-04 | global batch size:  1024 | lm loss: 1.733625E+00 | loss scale: 1.0 | grad norm: 1.658 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1231/   14369 | consumed samples:      1260544 | elapsed time per iteration (ms): 112664.0 | rate (tokens/sec): 18614.21 | learning rate: 1.846E-04 | global batch size:  1024 | lm loss: 1.730331E+00 | loss scale: 1.0 | grad norm: 1.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1232/   14369 | consumed samples:      1261568 | elapsed time per iteration (ms): 111635.9 | rate (tokens/sec): 18785.64 | learning rate: 1.848E-04 | global batch size:  1024 | lm loss: 1.722495E+00 | loss scale: 1.0 | grad norm: 1.827 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1233/   14369 | consumed samples:      1262592 | elapsed time per iteration (ms): 111828.8 | rate (tokens/sec): 18753.24 | learning rate: 1.849E-04 | global batch size:  1024 | lm loss: 1.754330E+00 | loss scale: 1.0 | grad norm: 1.695 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1234/   14369 | consumed samples:      1263616 | elapsed time per iteration (ms): 113371.0 | rate (tokens/sec): 18498.13 | learning rate: 1.851E-04 | global batch size:  1024 | lm loss: 1.734371E+00 | loss scale: 1.0 | grad norm: 1.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1235/   14369 | consumed samples:      1264640 | elapsed time per iteration (ms): 111994.9 | rate (tokens/sec): 18725.43 | learning rate: 1.852E-04 | global batch size:  1024 | lm loss: 1.740033E+00 | loss scale: 1.0 | grad norm: 1.596 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1236/   14369 | consumed samples:      1265664 | elapsed time per iteration (ms): 111959.6 | rate (tokens/sec): 18731.33 | learning rate: 1.854E-04 | global batch size:  1024 | lm loss: 1.735547E+00 | loss scale: 1.0 | grad norm: 1.951 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1237/   14369 | consumed samples:      1266688 | elapsed time per iteration (ms): 111102.0 | rate (tokens/sec): 18875.91 | learning rate: 1.855E-04 | global batch size:  1024 | lm loss: 1.755443E+00 | loss scale: 1.0 | grad norm: 1.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1238/   14369 | consumed samples:      1267712 | elapsed time per iteration (ms): 111741.5 | rate (tokens/sec): 18767.89 | learning rate: 1.857E-04 | global batch size:  1024 | lm loss: 1.727820E+00 | loss scale: 1.0 | grad norm: 1.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1239/   14369 | consumed samples:      1268736 | elapsed time per iteration (ms): 111900.9 | rate (tokens/sec): 18741.16 | learning rate: 1.858E-04 | global batch size:  1024 | lm loss: 1.750705E+00 | loss scale: 1.0 | grad norm: 1.415 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1240/   14369 | consumed samples:      1269760 | elapsed time per iteration (ms): 113591.7 | rate (tokens/sec): 18462.19 | learning rate: 1.860E-04 | global batch size:  1024 | lm loss: 1.745372E+00 | loss scale: 1.0 | grad norm: 1.614 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1241/   14369 | consumed samples:      1270784 | elapsed time per iteration (ms): 112141.9 | rate (tokens/sec): 18700.88 | learning rate: 1.861E-04 | global batch size:  1024 | lm loss: 1.751929E+00 | loss scale: 1.0 | grad norm: 1.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1242/   14369 | consumed samples:      1271808 | elapsed time per iteration (ms): 111127.0 | rate (tokens/sec): 18871.66 | learning rate: 1.863E-04 | global batch size:  1024 | lm loss: 1.732651E+00 | loss scale: 1.0 | grad norm: 1.654 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1243/   14369 | consumed samples:      1272832 | elapsed time per iteration (ms): 111737.9 | rate (tokens/sec): 18768.50 | learning rate: 1.864E-04 | global batch size:  1024 | lm loss: 1.734044E+00 | loss scale: 1.0 | grad norm: 1.732 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1244/   14369 | consumed samples:      1273856 | elapsed time per iteration (ms): 111098.9 | rate (tokens/sec): 18876.44 | learning rate: 1.866E-04 | global batch size:  1024 | lm loss: 1.725957E+00 | loss scale: 1.0 | grad norm: 1.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1245/   14369 | consumed samples:      1274880 | elapsed time per iteration (ms): 111524.9 | rate (tokens/sec): 18804.34 | learning rate: 1.867E-04 | global batch size:  1024 | lm loss: 1.739402E+00 | loss scale: 1.0 | grad norm: 1.754 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1246/   14369 | consumed samples:      1275904 | elapsed time per iteration (ms): 111833.0 | rate (tokens/sec): 18752.53 | learning rate: 1.869E-04 | global batch size:  1024 | lm loss: 1.724090E+00 | loss scale: 1.0 | grad norm: 1.524 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1247/   14369 | consumed samples:      1276928 | elapsed time per iteration (ms): 112119.9 | rate (tokens/sec): 18704.55 | learning rate: 1.870E-04 | global batch size:  1024 | lm loss: 1.731505E+00 | loss scale: 1.0 | grad norm: 1.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1248/   14369 | consumed samples:      1277952 | elapsed time per iteration (ms): 111304.1 | rate (tokens/sec): 18841.65 | learning rate: 1.872E-04 | global batch size:  1024 | lm loss: 1.729973E+00 | loss scale: 1.0 | grad norm: 1.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1249/   14369 | consumed samples:      1278976 | elapsed time per iteration (ms): 111040.7 | rate (tokens/sec): 18886.33 | learning rate: 1.874E-04 | global batch size:  1024 | lm loss: 1.736564E+00 | loss scale: 1.0 | grad norm: 1.849 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1250/   14369 | consumed samples:      1280000 | elapsed time per iteration (ms): 111440.0 | rate (tokens/sec): 18818.66 | learning rate: 1.875E-04 | global batch size:  1024 | lm loss: 1.737520E+00 | loss scale: 1.0 | grad norm: 1.674 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1251/   14369 | consumed samples:      1281024 | elapsed time per iteration (ms): 111779.7 | rate (tokens/sec): 18761.48 | learning rate: 1.876E-04 | global batch size:  1024 | lm loss: 1.730956E+00 | loss scale: 1.0 | grad norm: 1.771 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1252/   14369 | consumed samples:      1282048 | elapsed time per iteration (ms): 111279.9 | rate (tokens/sec): 18845.74 | learning rate: 1.878E-04 | global batch size:  1024 | lm loss: 1.740858E+00 | loss scale: 1.0 | grad norm: 1.709 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1253/   14369 | consumed samples:      1283072 | elapsed time per iteration (ms): 111233.9 | rate (tokens/sec): 18853.53 | learning rate: 1.879E-04 | global batch size:  1024 | lm loss: 1.716155E+00 | loss scale: 1.0 | grad norm: 1.679 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1254/   14369 | consumed samples:      1284096 | elapsed time per iteration (ms): 111386.9 | rate (tokens/sec): 18827.63 | learning rate: 1.881E-04 | global batch size:  1024 | lm loss: 1.717392E+00 | loss scale: 1.0 | grad norm: 1.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1255/   14369 | consumed samples:      1285120 | elapsed time per iteration (ms): 111809.3 | rate (tokens/sec): 18756.50 | learning rate: 1.882E-04 | global batch size:  1024 | lm loss: 1.726056E+00 | loss scale: 1.0 | grad norm: 1.718 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1256/   14369 | consumed samples:      1286144 | elapsed time per iteration (ms): 111100.0 | rate (tokens/sec): 18876.25 | learning rate: 1.884E-04 | global batch size:  1024 | lm loss: 1.728074E+00 | loss scale: 1.0 | grad norm: 1.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1257/   14369 | consumed samples:      1287168 | elapsed time per iteration (ms): 111571.8 | rate (tokens/sec): 18796.43 | learning rate: 1.886E-04 | global batch size:  1024 | lm loss: 1.724999E+00 | loss scale: 1.0 | grad norm: 1.970 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1258/   14369 | consumed samples:      1288192 | elapsed time per iteration (ms): 113065.4 | rate (tokens/sec): 18548.14 | learning rate: 1.887E-04 | global batch size:  1024 | lm loss: 1.723044E+00 | loss scale: 1.0 | grad norm: 1.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1259/   14369 | consumed samples:      1289216 | elapsed time per iteration (ms): 113341.7 | rate (tokens/sec): 18502.92 | learning rate: 1.888E-04 | global batch size:  1024 | lm loss: 1.740752E+00 | loss scale: 1.0 | grad norm: 1.716 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1260/   14369 | consumed samples:      1290240 | elapsed time per iteration (ms): 115040.1 | rate (tokens/sec): 18229.75 | learning rate: 1.890E-04 | global batch size:  1024 | lm loss: 1.738397E+00 | loss scale: 1.0 | grad norm: 1.585 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1261/   14369 | consumed samples:      1291264 | elapsed time per iteration (ms): 111401.8 | rate (tokens/sec): 18825.12 | learning rate: 1.891E-04 | global batch size:  1024 | lm loss: 1.738790E+00 | loss scale: 1.0 | grad norm: 1.915 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1262/   14369 | consumed samples:      1292288 | elapsed time per iteration (ms): 111620.0 | rate (tokens/sec): 18788.31 | learning rate: 1.893E-04 | global batch size:  1024 | lm loss: 1.730075E+00 | loss scale: 1.0 | grad norm: 1.733 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1263/   14369 | consumed samples:      1293312 | elapsed time per iteration (ms): 111426.9 | rate (tokens/sec): 18820.88 | learning rate: 1.894E-04 | global batch size:  1024 | lm loss: 1.736699E+00 | loss scale: 1.0 | grad norm: 1.878 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1264/   14369 | consumed samples:      1294336 | elapsed time per iteration (ms): 111621.8 | rate (tokens/sec): 18788.01 | learning rate: 1.896E-04 | global batch size:  1024 | lm loss: 1.727093E+00 | loss scale: 1.0 | grad norm: 1.608 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1265/   14369 | consumed samples:      1295360 | elapsed time per iteration (ms): 111911.5 | rate (tokens/sec): 18739.38 | learning rate: 1.897E-04 | global batch size:  1024 | lm loss: 1.715403E+00 | loss scale: 1.0 | grad norm: 1.535 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1266/   14369 | consumed samples:      1296384 | elapsed time per iteration (ms): 111692.9 | rate (tokens/sec): 18776.05 | learning rate: 1.899E-04 | global batch size:  1024 | lm loss: 1.727662E+00 | loss scale: 1.0 | grad norm: 1.547 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1267/   14369 | consumed samples:      1297408 | elapsed time per iteration (ms): 111461.8 | rate (tokens/sec): 18814.99 | learning rate: 1.900E-04 | global batch size:  1024 | lm loss: 1.714010E+00 | loss scale: 1.0 | grad norm: 1.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1268/   14369 | consumed samples:      1298432 | elapsed time per iteration (ms): 110968.4 | rate (tokens/sec): 18898.64 | learning rate: 1.902E-04 | global batch size:  1024 | lm loss: 1.730810E+00 | loss scale: 1.0 | grad norm: 1.517 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1269/   14369 | consumed samples:      1299456 | elapsed time per iteration (ms): 112195.8 | rate (tokens/sec): 18691.89 | learning rate: 1.903E-04 | global batch size:  1024 | lm loss: 1.738010E+00 | loss scale: 1.0 | grad norm: 2.025 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1270/   14369 | consumed samples:      1300480 | elapsed time per iteration (ms): 112306.1 | rate (tokens/sec): 18673.53 | learning rate: 1.905E-04 | global batch size:  1024 | lm loss: 1.711533E+00 | loss scale: 1.0 | grad norm: 1.355 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1271/   14369 | consumed samples:      1301504 | elapsed time per iteration (ms): 111768.0 | rate (tokens/sec): 18763.44 | learning rate: 1.906E-04 | global batch size:  1024 | lm loss: 1.720014E+00 | loss scale: 1.0 | grad norm: 1.999 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1272/   14369 | consumed samples:      1302528 | elapsed time per iteration (ms): 111279.9 | rate (tokens/sec): 18845.74 | learning rate: 1.908E-04 | global batch size:  1024 | lm loss: 1.745366E+00 | loss scale: 1.0 | grad norm: 1.377 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1273/   14369 | consumed samples:      1303552 | elapsed time per iteration (ms): 110331.2 | rate (tokens/sec): 19007.79 | learning rate: 1.909E-04 | global batch size:  1024 | lm loss: 1.719131E+00 | loss scale: 1.0 | grad norm: 1.999 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1274/   14369 | consumed samples:      1304576 | elapsed time per iteration (ms): 111421.7 | rate (tokens/sec): 18821.76 | learning rate: 1.911E-04 | global batch size:  1024 | lm loss: 1.715178E+00 | loss scale: 1.0 | grad norm: 1.797 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1275/   14369 | consumed samples:      1305600 | elapsed time per iteration (ms): 112140.0 | rate (tokens/sec): 18701.19 | learning rate: 1.912E-04 | global batch size:  1024 | lm loss: 1.723121E+00 | loss scale: 1.0 | grad norm: 1.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1276/   14369 | consumed samples:      1306624 | elapsed time per iteration (ms): 111599.9 | rate (tokens/sec): 18791.70 | learning rate: 1.914E-04 | global batch size:  1024 | lm loss: 1.722878E+00 | loss scale: 1.0 | grad norm: 1.487 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1277/   14369 | consumed samples:      1307648 | elapsed time per iteration (ms): 111499.9 | rate (tokens/sec): 18808.56 | learning rate: 1.915E-04 | global batch size:  1024 | lm loss: 1.738225E+00 | loss scale: 1.0 | grad norm: 1.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1278/   14369 | consumed samples:      1308672 | elapsed time per iteration (ms): 111319.9 | rate (tokens/sec): 18838.97 | learning rate: 1.917E-04 | global batch size:  1024 | lm loss: 1.715010E+00 | loss scale: 1.0 | grad norm: 1.564 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1279/   14369 | consumed samples:      1309696 | elapsed time per iteration (ms): 112076.2 | rate (tokens/sec): 18711.83 | learning rate: 1.919E-04 | global batch size:  1024 | lm loss: 1.735867E+00 | loss scale: 1.0 | grad norm: 2.063 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1280/   14369 | consumed samples:      1310720 | elapsed time per iteration (ms): 111923.0 | rate (tokens/sec): 18737.45 | learning rate: 1.920E-04 | global batch size:  1024 | lm loss: 1.734677E+00 | loss scale: 1.0 | grad norm: 1.233 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1281/   14369 | consumed samples:      1311744 | elapsed time per iteration (ms): 111241.7 | rate (tokens/sec): 18852.21 | learning rate: 1.921E-04 | global batch size:  1024 | lm loss: 1.735248E+00 | loss scale: 1.0 | grad norm: 2.100 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1282/   14369 | consumed samples:      1312768 | elapsed time per iteration (ms): 111504.8 | rate (tokens/sec): 18807.72 | learning rate: 1.923E-04 | global batch size:  1024 | lm loss: 1.722303E+00 | loss scale: 1.0 | grad norm: 1.490 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1283/   14369 | consumed samples:      1313792 | elapsed time per iteration (ms): 110860.7 | rate (tokens/sec): 18917.00 | learning rate: 1.924E-04 | global batch size:  1024 | lm loss: 1.747698E+00 | loss scale: 1.0 | grad norm: 1.569 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1284/   14369 | consumed samples:      1314816 | elapsed time per iteration (ms): 111123.0 | rate (tokens/sec): 18872.34 | learning rate: 1.926E-04 | global batch size:  1024 | lm loss: 1.734904E+00 | loss scale: 1.0 | grad norm: 1.605 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1285/   14369 | consumed samples:      1315840 | elapsed time per iteration (ms): 112096.9 | rate (tokens/sec): 18708.39 | learning rate: 1.927E-04 | global batch size:  1024 | lm loss: 1.751591E+00 | loss scale: 1.0 | grad norm: 1.712 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1286/   14369 | consumed samples:      1316864 | elapsed time per iteration (ms): 111370.7 | rate (tokens/sec): 18830.37 | learning rate: 1.929E-04 | global batch size:  1024 | lm loss: 1.763658E+00 | loss scale: 1.0 | grad norm: 1.590 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1287/   14369 | consumed samples:      1317888 | elapsed time per iteration (ms): 111021.7 | rate (tokens/sec): 18889.57 | learning rate: 1.930E-04 | global batch size:  1024 | lm loss: 1.720038E+00 | loss scale: 1.0 | grad norm: 1.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1288/   14369 | consumed samples:      1318912 | elapsed time per iteration (ms): 111943.0 | rate (tokens/sec): 18734.10 | learning rate: 1.932E-04 | global batch size:  1024 | lm loss: 1.718994E+00 | loss scale: 1.0 | grad norm: 1.794 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1289/   14369 | consumed samples:      1319936 | elapsed time per iteration (ms): 111299.9 | rate (tokens/sec): 18842.35 | learning rate: 1.933E-04 | global batch size:  1024 | lm loss: 1.721582E+00 | loss scale: 1.0 | grad norm: 1.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1290/   14369 | consumed samples:      1320960 | elapsed time per iteration (ms): 111154.9 | rate (tokens/sec): 18866.93 | learning rate: 1.935E-04 | global batch size:  1024 | lm loss: 1.722710E+00 | loss scale: 1.0 | grad norm: 1.904 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1291/   14369 | consumed samples:      1321984 | elapsed time per iteration (ms): 111563.9 | rate (tokens/sec): 18797.77 | learning rate: 1.936E-04 | global batch size:  1024 | lm loss: 1.710227E+00 | loss scale: 1.0 | grad norm: 1.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1292/   14369 | consumed samples:      1323008 | elapsed time per iteration (ms): 112477.5 | rate (tokens/sec): 18645.08 | learning rate: 1.938E-04 | global batch size:  1024 | lm loss: 1.711124E+00 | loss scale: 1.0 | grad norm: 1.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1293/   14369 | consumed samples:      1324032 | elapsed time per iteration (ms): 111741.9 | rate (tokens/sec): 18767.83 | learning rate: 1.939E-04 | global batch size:  1024 | lm loss: 1.728430E+00 | loss scale: 1.0 | grad norm: 1.450 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1294/   14369 | consumed samples:      1325056 | elapsed time per iteration (ms): 112936.7 | rate (tokens/sec): 18569.27 | learning rate: 1.941E-04 | global batch size:  1024 | lm loss: 1.733815E+00 | loss scale: 1.0 | grad norm: 2.049 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1295/   14369 | consumed samples:      1326080 | elapsed time per iteration (ms): 112581.8 | rate (tokens/sec): 18627.81 | learning rate: 1.942E-04 | global batch size:  1024 | lm loss: 1.739404E+00 | loss scale: 1.0 | grad norm: 1.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1296/   14369 | consumed samples:      1327104 | elapsed time per iteration (ms): 112681.9 | rate (tokens/sec): 18611.26 | learning rate: 1.944E-04 | global batch size:  1024 | lm loss: 1.734389E+00 | loss scale: 1.0 | grad norm: 1.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1297/   14369 | consumed samples:      1328128 | elapsed time per iteration (ms): 112141.7 | rate (tokens/sec): 18700.91 | learning rate: 1.945E-04 | global batch size:  1024 | lm loss: 1.722670E+00 | loss scale: 1.0 | grad norm: 1.245 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1298/   14369 | consumed samples:      1329152 | elapsed time per iteration (ms): 112281.7 | rate (tokens/sec): 18677.60 | learning rate: 1.947E-04 | global batch size:  1024 | lm loss: 1.740667E+00 | loss scale: 1.0 | grad norm: 2.044 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1299/   14369 | consumed samples:      1330176 | elapsed time per iteration (ms): 111288.0 | rate (tokens/sec): 18844.37 | learning rate: 1.948E-04 | global batch size:  1024 | lm loss: 1.725921E+00 | loss scale: 1.0 | grad norm: 1.197 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1300/   14369 | consumed samples:      1331200 | elapsed time per iteration (ms): 111456.8 | rate (tokens/sec): 18815.83 | learning rate: 1.950E-04 | global batch size:  1024 | lm loss: 1.732475E+00 | loss scale: 1.0 | grad norm: 2.588 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1301/   14369 | consumed samples:      1332224 | elapsed time per iteration (ms): 112161.8 | rate (tokens/sec): 18697.55 | learning rate: 1.952E-04 | global batch size:  1024 | lm loss: 1.738201E+00 | loss scale: 1.0 | grad norm: 1.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1302/   14369 | consumed samples:      1333248 | elapsed time per iteration (ms): 111487.0 | rate (tokens/sec): 18810.73 | learning rate: 1.953E-04 | global batch size:  1024 | lm loss: 1.729579E+00 | loss scale: 1.0 | grad norm: 1.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1303/   14369 | consumed samples:      1334272 | elapsed time per iteration (ms): 111553.0 | rate (tokens/sec): 18799.60 | learning rate: 1.954E-04 | global batch size:  1024 | lm loss: 1.731909E+00 | loss scale: 1.0 | grad norm: 2.214 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1304/   14369 | consumed samples:      1335296 | elapsed time per iteration (ms): 112081.7 | rate (tokens/sec): 18710.92 | learning rate: 1.956E-04 | global batch size:  1024 | lm loss: 1.736217E+00 | loss scale: 1.0 | grad norm: 1.505 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1305/   14369 | consumed samples:      1336320 | elapsed time per iteration (ms): 111891.6 | rate (tokens/sec): 18742.71 | learning rate: 1.957E-04 | global batch size:  1024 | lm loss: 1.730383E+00 | loss scale: 1.0 | grad norm: 1.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1306/   14369 | consumed samples:      1337344 | elapsed time per iteration (ms): 111563.9 | rate (tokens/sec): 18797.77 | learning rate: 1.959E-04 | global batch size:  1024 | lm loss: 1.735256E+00 | loss scale: 1.0 | grad norm: 1.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1307/   14369 | consumed samples:      1338368 | elapsed time per iteration (ms): 111619.9 | rate (tokens/sec): 18788.34 | learning rate: 1.960E-04 | global batch size:  1024 | lm loss: 1.719454E+00 | loss scale: 1.0 | grad norm: 1.647 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1308/   14369 | consumed samples:      1339392 | elapsed time per iteration (ms): 111560.8 | rate (tokens/sec): 18798.29 | learning rate: 1.962E-04 | global batch size:  1024 | lm loss: 1.721101E+00 | loss scale: 1.0 | grad norm: 1.824 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1309/   14369 | consumed samples:      1340416 | elapsed time per iteration (ms): 113541.8 | rate (tokens/sec): 18470.30 | learning rate: 1.963E-04 | global batch size:  1024 | lm loss: 1.733075E+00 | loss scale: 1.0 | grad norm: 1.639 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1310/   14369 | consumed samples:      1341440 | elapsed time per iteration (ms): 113781.9 | rate (tokens/sec): 18431.34 | learning rate: 1.965E-04 | global batch size:  1024 | lm loss: 1.726693E+00 | loss scale: 1.0 | grad norm: 1.781 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1311/   14369 | consumed samples:      1342464 | elapsed time per iteration (ms): 112893.5 | rate (tokens/sec): 18576.37 | learning rate: 1.966E-04 | global batch size:  1024 | lm loss: 1.724674E+00 | loss scale: 1.0 | grad norm: 1.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1312/   14369 | consumed samples:      1343488 | elapsed time per iteration (ms): 113695.1 | rate (tokens/sec): 18445.41 | learning rate: 1.968E-04 | global batch size:  1024 | lm loss: 1.720774E+00 | loss scale: 1.0 | grad norm: 2.213 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1313/   14369 | consumed samples:      1344512 | elapsed time per iteration (ms): 113376.7 | rate (tokens/sec): 18497.20 | learning rate: 1.969E-04 | global batch size:  1024 | lm loss: 1.719246E+00 | loss scale: 1.0 | grad norm: 1.251 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1314/   14369 | consumed samples:      1345536 | elapsed time per iteration (ms): 113080.0 | rate (tokens/sec): 18545.73 | learning rate: 1.971E-04 | global batch size:  1024 | lm loss: 1.714782E+00 | loss scale: 1.0 | grad norm: 1.674 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1315/   14369 | consumed samples:      1346560 | elapsed time per iteration (ms): 112481.8 | rate (tokens/sec): 18644.38 | learning rate: 1.972E-04 | global batch size:  1024 | lm loss: 1.707141E+00 | loss scale: 1.0 | grad norm: 1.678 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1316/   14369 | consumed samples:      1347584 | elapsed time per iteration (ms): 113277.7 | rate (tokens/sec): 18513.36 | learning rate: 1.974E-04 | global batch size:  1024 | lm loss: 1.733864E+00 | loss scale: 1.0 | grad norm: 1.560 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1317/   14369 | consumed samples:      1348608 | elapsed time per iteration (ms): 111359.9 | rate (tokens/sec): 18832.20 | learning rate: 1.975E-04 | global batch size:  1024 | lm loss: 1.709040E+00 | loss scale: 1.0 | grad norm: 1.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1318/   14369 | consumed samples:      1349632 | elapsed time per iteration (ms): 111807.7 | rate (tokens/sec): 18756.77 | learning rate: 1.977E-04 | global batch size:  1024 | lm loss: 1.719345E+00 | loss scale: 1.0 | grad norm: 1.947 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1319/   14369 | consumed samples:      1350656 | elapsed time per iteration (ms): 110961.8 | rate (tokens/sec): 18899.76 | learning rate: 1.978E-04 | global batch size:  1024 | lm loss: 1.711944E+00 | loss scale: 1.0 | grad norm: 1.479 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1320/   14369 | consumed samples:      1351680 | elapsed time per iteration (ms): 112061.7 | rate (tokens/sec): 18714.26 | learning rate: 1.980E-04 | global batch size:  1024 | lm loss: 1.707370E+00 | loss scale: 1.0 | grad norm: 1.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1321/   14369 | consumed samples:      1352704 | elapsed time per iteration (ms): 111973.0 | rate (tokens/sec): 18729.08 | learning rate: 1.981E-04 | global batch size:  1024 | lm loss: 1.723118E+00 | loss scale: 1.0 | grad norm: 1.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1322/   14369 | consumed samples:      1353728 | elapsed time per iteration (ms): 111894.9 | rate (tokens/sec): 18742.16 | learning rate: 1.983E-04 | global batch size:  1024 | lm loss: 1.690213E+00 | loss scale: 1.0 | grad norm: 1.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1323/   14369 | consumed samples:      1354752 | elapsed time per iteration (ms): 111959.9 | rate (tokens/sec): 18731.28 | learning rate: 1.984E-04 | global batch size:  1024 | lm loss: 1.726199E+00 | loss scale: 1.0 | grad norm: 1.582 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1324/   14369 | consumed samples:      1355776 | elapsed time per iteration (ms): 111358.6 | rate (tokens/sec): 18832.41 | learning rate: 1.986E-04 | global batch size:  1024 | lm loss: 1.722461E+00 | loss scale: 1.0 | grad norm: 1.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1325/   14369 | consumed samples:      1356800 | elapsed time per iteration (ms): 111339.9 | rate (tokens/sec): 18835.59 | learning rate: 1.987E-04 | global batch size:  1024 | lm loss: 1.714199E+00 | loss scale: 1.0 | grad norm: 1.853 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1326/   14369 | consumed samples:      1357824 | elapsed time per iteration (ms): 112181.7 | rate (tokens/sec): 18694.24 | learning rate: 1.989E-04 | global batch size:  1024 | lm loss: 1.717270E+00 | loss scale: 1.0 | grad norm: 1.393 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1327/   14369 | consumed samples:      1358848 | elapsed time per iteration (ms): 111677.0 | rate (tokens/sec): 18778.72 | learning rate: 1.990E-04 | global batch size:  1024 | lm loss: 1.734488E+00 | loss scale: 1.0 | grad norm: 1.926 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1328/   14369 | consumed samples:      1359872 | elapsed time per iteration (ms): 112223.9 | rate (tokens/sec): 18687.21 | learning rate: 1.992E-04 | global batch size:  1024 | lm loss: 1.710221E+00 | loss scale: 1.0 | grad norm: 1.143 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1329/   14369 | consumed samples:      1360896 | elapsed time per iteration (ms): 112474.8 | rate (tokens/sec): 18645.52 | learning rate: 1.993E-04 | global batch size:  1024 | lm loss: 1.718862E+00 | loss scale: 1.0 | grad norm: 2.123 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration     1330/   14369 | consumed samples:      1361920 | elapsed time per iteration (ms): 111929.3 | rate (tokens/sec): 18736.39 | learning rate: 1.995E-04 | global batch size:  1024 | lm loss: 1.719613E+00 | loss scale: 1.0 | grad norm: 1.312 | number of skipped iterations:   0 | number of nan iterations:   0 |
